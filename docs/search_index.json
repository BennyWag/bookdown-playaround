[
["index.html", "Intro to R for Forest Ecosystem Science Chapter 1 Introduction 1.1 Disclaimer", " Intro to R for Forest Ecosystem Science Anu Singh, Benjamin Wagner, and Kaitlyn Hammond Chapter 1 Introduction This is a (hopefully) continuously increasing collection of R tutorials and code collections relevant for Forest Ecosystem Sciences, curated by three R enthusiasts at the School of Ecosystem and Forest Sciences - Uni Melbourne. Nevertheless, most tutorials can be useful for anyone starting out in R or looking to broaden their skills and learn something new, that we had to learn ourselves. We currently cover: Setting up RStudio, working with projects and using code folds A brief description on data and variables types Introductions to using the tidyverse packages dplyr and ggplot2, for easy data manipulation and visualization An Introduction to using spatial data in R with both the sf and raster packages. Introduction to basic linear and generalized linear modeling in R A tutorial on functions and loops in R and the use of lapply and mapply A tutorial on using the package lidR to process pointclouds from LiDAR data and photogrammetry Code chunks for area calculations of rasters and fractions in classified rasters more is coming … watch this space! 1.1 Disclaimer If you enjoy our tutorials and code snippets, feel free to use and share this book and code compiled in it for the best purposes you can think of. Nevertheless, while doing so, please acknowledge our work and share the link to the source of this book or the github page at: https://github.com/BennyWag/bookdown-playaround. "],
["a-few-words-on-a-productive-setup-and-workflow-in-r-with-r-studio.html", "Chapter 2 A few words on a productive setup and workflow in R with R-Studio 2.1 R Projects 2.2 Setting up the R-Studio User interface 2.3 Code folds and commenting", " Chapter 2 A few words on a productive setup and workflow in R with R-Studio Before we get our hands dirty with all the great functions and packages R has to offer, there are a few things that will make your R life much easier, if you adopt them early on (or even if you’re already a weathered expert but are still using setwd() ;-)). Working with R Projects, which is made available through R-Studio is probably the most comprehensive way to set up a new project, without having to mingle around with working directories too much. It also makes your work share- and reproducible. Additionally, there are a few tweaks we can do with the user interface (UI) of R-Studio that will make it more intuitive. Last but not least, we will earn how to best set up a R script, so that you never loose track, even after thousands of lines of code! To illustrate working with projects, we will be using the koala dataset, which you will see throughout these pages again and a again as example data. You can download the data here. For now, leave it in the folder that you have downloaded it to. In my case (on a Windows PC) it would be in 'C:\\Users\\username\\Downloads' 2.1 R Projects To understand what an R project does for you and why it is advantageous over just working within the general workspace of R-Studio, you need to understand file paths. There are absolute and relative file paths: An absolute (or full) file path is a path that points to a certain location in the system on your specific computer. This means, that it might not exist on another computer! A good example is the path I mentioned above ('C:\\Users\\username\\Downloads'). The general Downloads folder on a Windows operating system is stored in the respective user folder, which will be different for each user. This absolute file path points to this location, regardless of the working directory we might have defined and includes what we call the root directory. Relative file paths on the other hand, start from a given working directory, avoiding the need to provide the full absolute path. These paths will work regardless of the root directory, as long as e.g. the folder that the relative file path refers to, is available on the computer. The relative file path is stored in an R project and thus saved even when you close R-Studio. When you give your project folder to someone else and they launch your project, all specified paths will work, as long as the project folder is the working directory and all needed files are stored within. Let’s illustrate this in a simple manner using our downloads folder, to which you have just downloaded the koala dataset. It is simple to load the data into R: koala&lt;-read.csv(&#39;C:/Users/username/Downloads/koala.csv&#39;) In case you move the file though and return to the code you have just written, R won’t be able to load the data again, because it doesn’t exist in that directory. If you have learnt R as many of us from other people’s suggestions or online tutorials, you will probably have picked up that you have to use setwd() to best point R to where all files you want to work on or load into R are stored. This would be a sort of project folder where you put all your data, as well as scripts etc. associated with the analysis you’re doing. Let’s assume we create a folder called project directly on our C drive: C:\\project . This is were we copied our koala data from the Downloads folder. What we would now do is: setwd(&#39;C:/project&#39;) koala&lt;-read.csv(&#39;koala.csv&#39;) summary(koala) ## species X Y ## Phascolarctos cinereus:242 Min. :138.6 Min. :-39.00 ## 1st Qu.:150.0 1st Qu.:-34.49 ## Median :152.0 Median :-32.67 ## Mean :150.3 Mean :-32.36 ## 3rd Qu.:152.9 3rd Qu.:-30.31 ## Max. :153.6 Max. :-21.39 ## state region sex weight ## New South Wales:181 northern:165 female:127 Min. : 5.406 ## Queensland : 16 southern: 77 male :115 1st Qu.: 6.574 ## South Australia: 14 Median : 7.277 ## Victoria : 31 Mean : 7.923 ## 3rd Qu.: 8.765 ## Max. :17.889 ## size fur tail age ## Min. :64.81 Min. :1.110 Min. :1.004 Min. : 1.00 ## 1st Qu.:68.43 1st Qu.:2.410 1st Qu.:1.272 1st Qu.: 3.00 ## Median :70.27 Median :2.797 Median :1.534 Median : 7.00 ## Mean :70.94 Mean :2.896 Mean :1.507 Mean : 6.43 ## 3rd Qu.:72.33 3rd Qu.:3.217 3rd Qu.:1.750 3rd Qu.: 9.00 ## Max. :81.91 Max. :5.876 Max. :1.981 Max. :12.00 ## color joey behav obs ## chocolate brown:21 No :185 Feeding : 48 Opportunistic:65 ## dark grey :36 Yes: 57 Just Chillin: 67 Spotlighting :94 ## grey :69 Sleeping :127 Stagwatching :83 ## grey-brown :53 ## light brown :20 ## light grey :43 Which works fine: you are actually creating a relative file path on your system, which saves you time and effort by not having to point to each file separately again. You can now also save modified files into the same folder, because R knows where to save it. koala_state&lt;-koala[4] #create a subset only containing the state column (4th column of this data) write.csv(koala_state, &#39;subset.csv&#39;, row.names = F) There are two problems with this though: If you want to share the super detailed analysis of that dataset you just did with someone else, they would need to change the code to fit their system using setwd() again to match their system. If they’re on a Mac and you’re on a PC, this becomes complicated. setwd() is a function and it needs to be executed each time you open R. Your working directory is not saved, which can lead to other problems, especially when you move files around, change computers or (god forbid) loose data somehow. The better option to avoid all this is to use R projects! If you have R-Studio open, you can easily create a new project by clicking on the File tab and hitting New Project: And you will see a window like this: You can create a completely new directory here, but I prefer to set up the folder structure first. Let’s take a step back and do that in our C:\\project directory first! In that empty folder, I like to create three base folders data outputs scripts Of course you can name them however you want, but for me this works best. You can also add other folders or subfolders if you feel you need a different structure for your project already. You can also always create more folders later, even within R using dir.create(). Your folder should look something like this now: Figure 2.1: All folders will still be empty Now we get back to the project creation window and click on Existing Directory: Here you can browse to the project folder (or enter the directory yourself). This creates a new project, R-Studio does the rest. What this mainly does is fix your working directory to that folder. There will be some additional files associated with this folder that save your user history (such as what you put in the console) and, most importantly, the working directory itself. Each time you open the project in that folder (or opening it within R-Studio), you will already be set to go! Your folder will now have this additional file after creating the project, which you can use to open it, it will also save a continuous .Rhistory file: Now we can go and fill up the project with all the data we need for our analysis! In our case. that is the koala.csv file. All data we are going to use is going into the data folder, so copy the table in there: The way we work in the project now is referring to the sub-folders of our working directory using relative file paths. To load the koala data we do: koala&lt;-read.csv(&#39;data/koala.csv&#39;) Processed data goes into the outputs folder: koala_state&lt;-koala[4] write.csv(koala_state, &#39;outputs/subset.csv&#39;, row.names = F) Once you save your script file the first time, choose the scripts folder within your project folder to store it safely. If you later on need another folder, because maybe you got a lot of output files and need to sort them, you can create a new folder and write (or read) to/from it: koala_region&lt;-koala[5] #create a subset with only the region column dir.create(&#39;outputs/region_analysis&#39;) write.csv(koala_region, &#39;outputs/region_analysis/region_states.csv&#39;, row.names = F) If you have a large dataset, that you can’t move from its location to the project directory, you can still load it using the known syntax. You have to then specify the absolute file path: #Let&#39;s assume the file is in your downloads folder superlarge_file&lt;-load(&#39;C:/Users/username/Downloads/superlargefile.file&#39;) The beauty of this way of handling your data is, that if you keep all files associated with your project within the project folder, you can move the project folder around as much as you want, give it to collaborators or even store it on a new computer if yours has exploded (given you’re backing up your files daily, which I hope you do ;)) and everything will be exactly where it should be and you can run through all your code without issues! When using projects and you re-open it, the R-Studio session is exactly where you left it when you last saved your workspace (likely when exiting the program). All script files you had open are still open, and .RData (which is stored in your project folder as well) is loaded, which means all variables in your environment are also still there, so you won’t need to re-run any of your code. The only thing you need to do still is load packages and recompile any Plots, as pane is emptied. You will destroy the order by using setwd(), so keep your hands off it. If you have some new data that you want to use, copy it to the data or any respective folder you’re using, before loading the data into R. You can keep track of your directory in the files tab in R-Studio: As you can see, you also can create new folders here using the buttons on top. If, by any chance your working directory has change (e.g. a function using different directories going astray…), you can rested the working directory to the project folder here as well. Using the green arrow, navigate to your project folder again and them press More and Set As Working Directory: So, the one thing to remember when working in this way is: AVOID CHANGING YOUR WORKING DIRECTORY! or NEVER USE setwd()! . Both will break your code and you will either have to reset or rewrite. Especially in larger projects it becomes extremely tedious to keep track where you stored or saved data when using setwd() a lot. If you change computers or restructure your system, you will also need to change or restructure your R script (and/or file paths). But once you get used to this way of working, you will see it is very intuitive and saves a lot of time. If you still need to load data from elsewhere, use absolute file paths and for everything else, use your project folders and thus relative file paths. Hope you find that helpful :) 2.2 Setting up the R-Studio User interface When you first open up R-Studio, it probably looks something like this: Figure 2.2: Image source is this useful tutorial :) Your user interface (UI) is split into 4 windows and by default has the code editor in the top-left corner, and the Environment in the top-right. The code editor is where script files are created or opened and where you will spend most of your time. The console on the lower-left takes up a considerable amount of space, even though you only need it occasionally (whenever you run code that you don’t need to or don’t want to save to your script file). For my taste the window setup is counter-productive, because it takes the focus away from what is most important. You are going to work mainly with your script and environment pane. You can change the setup of your UI (and appearance) to make a bit more room and have R-Studio look like this: You will have lots of space to view and write your script, have the console placed on the top-right so you can see what’s happening when running through your lines of code, and everything else (such as the Environment, Files and Plots panes) are tabbed neatly on the lower-right. Additionally I have changed the appearance to a dark theme, because staring at a white for longer screen can be quite exhausting for the eyes and I prefer my text background in dark. To change to these (or any other preferred settings), go to the Tools tab at the top and select Global Options: In the new window, first click on Pane Layout and select these settings: These are of course just recommendations based on my preferences and you can change as you go. Nevertheless I found that these setting work best when spending a lot of time in R-Studio. Now when you first set up the panes like this and no script is open, your History panel might still peek out. This is probably the least important pane and we can hide it by clicking on the small reduce button in the top right: Figure 2.3: Click this one (once and then the one to the left of it once it shrunk halfway) to hide the History pane at the bottom and even open a new untitled script automatically. Next we can change the appearance. In Global Options, click on Appearance. I use these settings, but there are lots of combinations and even more preferences: You may need to restart once you have chosen a theme and editor font/theme, for the changes to become active. One thing I find important is the option of coloring function calls. This means any function is highlighted in a different color than the rest of your code, which makes it (in my opinion), more readable. To make these changes go to Code and to the Display tab: Figure 2.4: These are the settings I use. Most importantly, tick ‘Highlight R function calls’, everything else is option. ‘Show syntax highlighting in console input’ can be nice if you work in the console occasionally, as it will color written code in the same way it does in you script. I like to be able to scroll further than my lowest line, so that I don’t always write at the bottom of my document, so I tick ‘Allow scroll past end of document’. Try these out and see how you like either. You can always change settings later on if you feel something doesn’t make sense in your workflow. Happy R’ing in a professional setup! :) 2.3 Code folds and commenting One last thing that really helps organizing your scripts is using what is called code folds in R. You can imagine them like titles or headings in a Word document. They subdivide your code into parts and let you browse through them more easily. You can also collapse code folds to hide them, which is great for large scripts, where you can easily loose track of where you did what after a while. Folds will show a small triangle next to the line number, which you can use to collapse or open that section. You can add code folds manually by using a # and then following it by at least four -, = or #. If you have more than one code fold, by clicking the arrow to the left of the fold, you can then hide the code between two sections easily: # loading packages ---- library(tidyverse) # loading data and checking ---- koala&lt;-read.csv(&#39;data/koala.csv&#39;) str(koala) summary(koala) # subsetting ---- subset1&lt;-koala%&gt;%filter(sex == &#39;female&#39;)%&gt;% select(age, weight, size, color) subset2&lt;-koala%&gt;%filter(sex == &#39;male&#39;)%&gt;% select(age, weight, size, color) # modelling ---- model_female&lt;-lm(weight~size, data = subset1) model_male&lt;-lm(weight~size, data = subset2) model_all&lt;-lm(weight~size*sex, data = koala) # checking models ---- summary(model_female) summary(model_male) summary(model_all) Now that you have set up the code in this way, you could for example hide sections that you are not currently working on to not loose track of the important code that you’re editing or adding to. Your code might then look like this, if you’re e.g. currently working on creating models: Some examples for valid code folds are e.g. #### Section #### # Section ---- # Section ==== ##### # Section ------------------ ## Section ======= Anything that has one # in front and at least four #, - or = will work. The easiest way to insert a code break in R-Studio though is to use the keyboard shortcut CTRL+Shift+R, which will open a windows where you can enter a title for the new fold. This fold will then look like those: # Section ----------------------------------------------------------------- If you have a lot of fold, you can easily navigate through them by clicking the Jump to menu at toe bottom-left of your editor window: Which will open a drop-down menu with the titles of all your folds. When you click one, you will jump to the line of code, where the selected fold starts. Great stuff right? The # is also used in R to include comments into your code. Anything that come after at least a single # is not run, when executing the lines of code selected. This can be useful to add some notes into your code, or exclude lines of code from your script that you don’t want to run, but still keep in the script: # loading packages ---- library(tidyverse) #whenever I open R-Studion, I need to run this line! # loading data and checking ---- koala&lt;-read.csv(&#39;data/koala.csv&#39;) str(koala) summary(koala) # subsetting ---- subset1&lt;-koala%&gt;%filter(sex == &#39;female&#39;)%&gt;% select(age, weight, size, color) subset2&lt;-koala%&gt;%filter(sex == &#39;male&#39;)%&gt;% select(age, weight, size, color) # modelling ---- model_female&lt;-lm(weight~size, data = subset1) #I want to check if a seperate model of male and female makes sense #model_male&lt;-lm(weight~size, data = subset2) #no need to run this one at the moment model_all&lt;-lm(weight~size*sex, data = koala) #this is the overall model using the entire data # checking models ---- summary(model_female) #summary(model_male) #summary(model_all) If you add a # after some lines of code like this: model_female&lt;-lm(weight~size, data = subset1) #I want to check if a seperate model of male and female makes sense It will act as a comment to your code. This cane be useful if you won’t be touching your code in a while but will come back to it later, to follow up on what you’ve done in the past. A # at the start of the line will ‘comment-out’ that line, which means it won’t run when executing your code. #model_male&lt;-lm(weight~size, data = subset2) #no need to run this one at the moment The code will still remain in your script and you can un-comment it at any time. There are also keyboard shortcuts to comment and un-comment, which can make it easier to comment-out many lines at ones. First you select the lines of code with your mouse, then, to comment use CTRL+Shift+C to un-comment use CTRL+Shift+C again and the # will be removed. This is easier than clicking through your code sometimes. Nevertheless, it can only be used to ‘comment-out’ code, not to add comments at the end of code lines. I hope these few tips will help you when starting out with R or make for a smoother workflow if you are already using R for a while but did not know about these few minor improvements, that can speed up your coding experience a lot! Happy coding :) "],
["introducing-the-koala-dataset.html", "Chapter 3 Introducing the koala dataset 3.1 data types and variable types 3.2 Changing variable types 3.3 How the koala dataset was collated", " Chapter 3 Introducing the koala dataset Before we get started, we will have a peek at the dataset we’re working with. You can download the dataset for this tutorial here. Put it in a data folder within your project (or working directory). You can now load it like: koala&lt;-read.csv(&#39;data/koala.csv&#39;) The koala dataset contains the positions of koalas from throughout their range in Latitude and Longitude (X and Y) as well as variables describing their physiology, behavior and how they were recorded. The position of each koala was extracted from the Atlas of Living Australia. This is typical presence-only wildlife data, combining observations with some data describing each individual, which could e.g. be used for distribution modeling or to test influences of other variables such as climate on behavior and physiology of this particular species. 3.1 data types and variable types Let’s have a look at the data structure using str() str(koala) ## &#39;data.frame&#39;: 242 obs. of 15 variables: ## $ species: Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num 153 148 153 153 153 ... ## $ Y : num -27.5 -22.5 -27.5 -27.5 -27.5 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num 70.8 70.4 68.7 73 65.2 ... ## $ fur : num 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... The first line tells us what we’re dealing with: a data.frame object containing 242 observations and 15 variables. data.framess are a standard data type object used in R. You can compare it to an excel sheet. The observations are your rows, so this data.frame is 242 rows long. Variables are your columns, of which we have 15. You can select a specific column using the $ operator, to, e.g., print a column’s variables into the console: koala$region ## [1] northern northern northern northern northern northern northern ## [8] northern northern northern northern northern northern northern ## [15] northern northern southern southern southern southern southern ## [22] southern southern southern southern southern southern southern ## [29] southern southern southern southern southern southern southern ## [36] southern southern southern southern southern southern southern ## [43] southern southern southern southern southern southern southern ## [50] southern southern southern southern southern southern southern ## [57] southern southern southern southern southern northern northern ## [64] northern northern southern northern northern northern southern ## [71] northern northern southern northern northern southern northern ## [78] northern northern northern northern northern northern northern ## [85] northern northern northern northern northern northern southern ## [92] northern northern northern northern southern northern northern ## [99] northern northern southern northern northern northern northern ## [106] southern northern southern northern northern northern northern ## [113] northern southern northern northern northern northern northern ## [120] southern southern northern northern southern southern northern ## [127] northern northern northern southern northern northern northern ## [134] northern northern northern northern northern northern southern ## [141] northern northern southern northern southern southern southern ## [148] northern northern northern northern southern northern northern ## [155] northern northern northern northern southern northern northern ## [162] northern northern northern northern northern northern northern ## [169] southern northern southern northern southern northern northern ## [176] northern northern southern northern northern northern northern ## [183] northern northern northern northern northern northern northern ## [190] northern northern southern northern northern northern southern ## [197] southern northern northern northern northern southern northern ## [204] northern northern northern northern northern northern northern ## [211] northern northern northern northern northern northern northern ## [218] northern northern northern northern northern northern northern ## [225] northern northern northern northern northern northern northern ## [232] southern northern northern northern northern northern northern ## [239] northern northern northern southern ## Levels: northern southern While that isn’t very useful, it illustrates a basic function in R: how to define and address your data. It goes data.frame$variable, in this case koala$region, to access the region column of our data set. Using summary() on a variable gives us an overview of its distribution within the data.frame: summary(koala$region) ## northern southern ## 165 77 This is more helpful than printing all the values into the console. The reason why R is giving back a count is the variable type, this variable has been assigned. As you can see from the above str(koala), the region variable is a Factor. Factors are the variable type which are used to categorize data and store it as so-called levels. They can store both strings (combinations of letters, usually words) and integers (real numbers without decimals). To have a better idea what this means, run: str(koala$region) ## Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... And you will see it returns you variable type Factor with two levels (northern and southern). This variables describes the the looks of koalas, as the southern animals are distinct in their morphology from their northern cousins, even though they’re the same species. class() will give you only the variable type type: class(koala$region) ## [1] &quot;factor&quot; Other Factor variables in the data are: state: the Australian state in which the observation was made sex: the gender of each individual koala color: the fur color of the individual joey: Yes if the observed koala had an offspring during the observation. You will see this (logically) only applied to koalas that had sex == 'female' behav: what behavior was exhibited during the observation obs: the type of observation made by the observer. Was it opportunistic and thus by chance, or maybe during a spotlighting survey? You can explore what sort of factor levels each of these variables have as we did before using summary() summary(koala$behav) ## Feeding Just Chillin Sleeping ## 48 67 127 Honoring their species as being one of the chillest marsupials on the planet, almost 1/3 of all koalas were Just Chillin ;). From our str(koala) above we can make out two other data types: num or numeric variable type int or an integer variable type Decimal values are called num or numeric in R. It is the default data type. If we assign a decimal value to a variable x it will always be of numeric type. x&lt;-20.25 x ## [1] 20.25 class(x) ## [1] &quot;numeric&quot; Numeric data in our koala dataset are the X and Y columns, which describe the geographic location of each observation and then four variables describing their individual physiology: weight: an individuals weight in kilo size: an individuals height in centimeters fur: its fur thickness in cm. Southern koalas have thicker fur because of the colder weather :) tail: the tail length of each observed individual. If you’re wondering how these observations were made while only looking for koalas, you’ll find the surprising answer at the end of this document. The remaining variable type we have in our data is int or integer. These are non-decimal numbers. In our case the age is describer as an integer, a full number, describing the year of life each koala is in. It is hard to determine the exact age, so these variables will often be given as integer, rather than e.g. age == 3.234. If you want to assign integers, you need to specify so, as we have learned the standard variable type for numbers in R is num.: y = as.integer(8) y ## [1] 8 class(y) ## [1] &quot;integer&quot; #We can also declare an integer by appending an L suffix. z = 8L is.integer(z) ## [1] TRUE The is.xxx() function can be useful to test for variable types in large datasets. It works with any data such as is.integer, is.numeric or is.factor. 3.2 Changing variable types We can shift data types around in R easily using as.xxx(). Let’s make a new variable in our dataset that numerically codes the observation types. This might be important for some modelling approaches that force you to use numeric or integer data: koala$obs_code&lt;-as.numeric(koala$obs) #this assigns a new column to the dataframe and fills it with the function we specify after the assign operator `&lt;-` head(koala) ## species X Y state region sex ## 1 Phascolarctos cinereus 153.2155 -27.49284 Queensland northern male ## 2 Phascolarctos cinereus 148.1443 -22.47617 Queensland northern female ## 3 Phascolarctos cinereus 153.2285 -27.50298 Queensland northern male ## 4 Phascolarctos cinereus 152.6000 -27.50000 Queensland northern male ## 5 Phascolarctos cinereus 153.2817 -27.52589 Queensland northern female ## 6 Phascolarctos cinereus 152.8330 -27.20000 Queensland northern male ## weight size fur tail age color joey behav ## 1 7.119754 70.80159 1.858696 1.168241 8 grey No Sleeping ## 2 5.451345 70.38537 1.852801 1.562456 10 grey-brown Yes Sleeping ## 3 6.630577 68.65867 2.479280 1.056640 1 light grey No Just Chillin ## 4 6.470019 72.98919 1.923974 1.801244 1 grey No Sleeping ## 5 5.620447 65.19529 1.945341 1.625600 10 grey-brown No Sleeping ## 6 7.287674 70.56514 1.688897 1.086675 12 grey-brown No Feeding ## obs obs_code ## 1 Spotlighting 2 ## 2 Opportunistic 1 ## 3 Spotlighting 2 ## 4 Stagwatching 3 ## 5 Stagwatching 3 ## 6 Opportunistic 1 You can see e.g. spotlighting was assigned a 2, while Opportunistic is now a 1. Knowing these basic data transformations can be useful. This is especially often the case when assigning or merging data that is of type character, a variable type, we don’t have in our data. Let’s see what might be problematic about it: koala$good_bad&lt;-sample(c(&#39;good&#39;, &#39;evil&#39;), nrow(koala), prob=c(0.99, 0.01), replace = T) Using sample we can randomly assign a new variable, that is picked from a specified vector (c('good', 'evil')), based on a specified probability (prob=c(0.99, 0.05)). Let’s get a summary of it to see how many of our koalas are bad by random probability of 5%: summary(koala$good_bad) ## Length Class Mode ## 242 character character class(koala$good_bad) ## [1] &quot;character&quot; Hm, that isn’t very helpful, because we created a character type data, made up of strings (letters). If we transform it, we can get a better idea: summary(as.factor(koala$good_bad)) ## evil good ## 4 238 4 of our koalas are evil :( There are other variable types you will encounter, such as dates, or logicals (TRUE and FALSE), but we won’t go into more detail here. You’re already all set to dive into data manipulation and visualization using this data set now :) 3.3 How the koala dataset was collated Some of you might be interested how all this data came together. As mentioned before, the observation locations are all from the Atlas of Living Australia. You can download species records from their database as GIS shapefiles or .csv tables, containing the species name and X and Y coordinates as latitude and longitude. Now while most of the variables in the dataset may be collected during surveys, we did not have access to recent koala data or data from another animal throughout its Australian distribution, so we just made it up wit the power of R :) Here is how its done! 3.3.1 Loading the raw data You can find the raw data here. It simply contained four columns of genus, species, X and Y: koala_raw&lt;-read.csv(&#39;data/koala_raw.csv&#39;) Now the following steps are a bit more advanced and we will cover parts of it in our spatial data in R tutorial. This is simply to document how the dataset was collated and what can be done in R in a simple steps: 3.3.2 Adding the state names by location We need to use some tidyverse packages and the spatial package sf to load shapefiles: library(sf) library(tidyverse) We need to load a layer of Australia and transform our koala observations into a spatial data type (a shapefile). You can find the Australia layer here. Put it in data/Australia. #make shapefile from raw data koala_base_sf &lt;- st_as_sf(koala_raw, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326) #load states states layer states &lt;- st_read(&#39;data/Australia/Australia_proj.shp&#39;) ## Reading layer `Australia_proj&#39; from data source `C:\\Users\\khh\\Documents\\GitHub\\bookdown-playaround\\data\\Australia\\Australia_proj.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 8 features and 15 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -2063975 ymin: -4965263 xmax: 1891143 ymax: -1285856 ## proj4string: +proj=lcc +lat_1=-18 +lat_2=-36 +lat_0=0 +lon_0=134 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs #st_crs(states) #check CRS #reproject to match koala data states_proj&lt;-st_transform(states, crs = st_crs(koala_base_sf)) #extract and reduce to interesting columns koala_states&lt;-st_intersection(koala_base_sf, states_proj) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar koala_states_min&lt;-koala_states%&gt;%select(1,2,15) #check visually plot(states_proj$geometry) plot(koala_states_min$geometry, add = T, col = koala_states_min$STATENAME) That works. This was the only other real data in our dataset. Let’s make up some variables :) 3.3.3 Creating dummy data 3.3.3.1 Sex and weight First we want to add the weight of the animals. We want to make it realistic, so let’s check some online sources by googling: https://environment.des.qld.gov.au/wildlife/animals/living-with/koalas/facts https://koalainfo.com/average-weights-of-male-koalas Their sex ratio throughout their range seems to be 46:54 % in favor of females. Knowing the probability is great, so we can easily randomize that data over our observations: koala_states_min$sex&lt;-sample(c(&#39;male&#39;, &#39;female&#39;), nrow(koala_states_min), prob=c(0.46, 0.54), replace = T)%&gt;%as.factor() summary(koala_states_min$sex) ## female male ## 125 117 The weight depends on the sex and where the animal lives. Using a nested ifelse() statement will help us, to make assigning weights conditional on their sex: koala_states_min$weight&lt;-ifelse(koala_states_min$STATENAME == &#39;Queensland&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 5, max = 6), ifelse(koala_states_min$STATENAME == &#39;Queensland&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 6, max = 8), ifelse(koala_states_min$STATENAME == &#39;Victoria&#39; | koala_states_min$STATENAME == &#39;South Australia&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 7, max = 8), ifelse(koala_states_min$STATENAME == &#39;Victoria&#39; | koala_states_min$STATENAME == &#39;South Australia&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 15, max = 18), ifelse(koala_states_min$STATENAME == &#39;New South Wales&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 6, max = 7), ifelse(koala_states_min$STATENAME == &#39;New South Wales&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 8, max = 10),100)))))) 3.3.3.2 Adding the X and Y variables and region Using sf objects it is easy to add the coordinates using st_coordinates, but if we want to split them into two columns, we need to use a function: st_x = function(x) st_coordinates(x)[,1] st_y = function(x) st_coordinates(x)[,2] #st_x(koala_states_min) #st_y(koala_states_min) koala_states_min$X&lt;-st_x(koala_states_min) koala_states_min$Y&lt;-st_y(koala_states_min) Now we can add northern or southern by a condition based on latitude: koala_states_min$region&lt;-ifelse(koala_states_min$Y&gt;(-34),&#39;northern&#39;, &#39;southern&#39;)%&gt;%as.factor() plot(states_proj$geometry) plot(koala_states_min$geometry, add = T, col = koala_states_min$region) 3.3.3.3 Add other physiological variables Some googling allows us to add the other variables based on semi-realistic data: https://www.savethekoala.com/about-koalas/physical-characteristics-koala Sizes range from ~64 to over 80 cm, depending on sex and region: koala_states_min$size&lt;-ifelse(koala_states_min$region == &#39;southern&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 68, max = 73), ifelse(koala_states_min$region == &#39;southern&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 75, max = 82), ifelse(koala_states_min$region == &#39;northern&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 64.8, max = 72.3), ifelse(koala_states_min$region == &#39;northern&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 67.4, max = 73.6),100)))) https://en.wikipedia.org/wiki/Koala The fur color varies based on their region too with some different probabilities: koala_states_min$color&lt;-ifelse(koala_states_min$region == &#39;northern&#39;, sample(c(&#39;light grey&#39;, &#39;grey&#39;, &#39;grey-brown&#39;), nrow(koala_states_min), prob=c(0.3, 0.4, 0.3), replace = T), sample(c(&#39;dark grey&#39;, &#39;light brown&#39;, &#39;chocolate brown&#39;), nrow(koala_states_min), prob=c(0.4, 0.3, 0.3), replace = T)) https://www.theanimalfiles.com/mammals/marsupials/koala.html The tail length is constant between 1 - 2 cm, so we can randomize it throughout: koala_states_min$tail&lt;-runif(nrow(koala_states_min), min = 1, max = 2) Fur thickness varies by state and sex: koala_states_min$fur&lt;-ifelse(koala_states_min$STATENAME == &#39;Queensland&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 1, max = 2), ifelse(koala_states_min$STATENAME == &#39;Queensland&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 1.5, max = 2.5), ifelse(koala_states_min$STATENAME == &#39;Victoria&#39; | koala_states_min$STATENAME == &#39;South Australia&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 3, max = 5), ifelse(koala_states_min$STATENAME == &#39;Victoria&#39; | koala_states_min$STATENAME == &#39;South Australia&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 3.5, max = 6), ifelse(koala_states_min$STATENAME == &#39;New South Wales&#39; &amp; koala_states_min$sex == &#39;female&#39;, runif(nrow(koala_states_min), min = 2, max = 3), ifelse(koala_states_min$STATENAME == &#39;New South Wales&#39; &amp; koala_states_min$sex == &#39;male&#39;, runif(nrow(koala_states_min), min = 2.5, max = 3.5), 100)))))) https://environment.des.qld.gov.au/wildlife/animals/living-with/koalas/facts They can become up to 12 years old, let’s just randomize this throughout the range as well. The mean in that case should be around 6 years, as sample() assumes normally distributed data, if you do not specify the prob. We’re using replace = TRUE, to make sure ages 1:12 can be repeated and are distributed over our 242 rows. koala_states_min$age&lt;-sample(12, nrow(koala_states_min), replace = T) mean(koala_states_min$age) ## [1] 6.041322 hist(koala_states_min$age) 3.3.3.4 Adding some more factor variables Finally we can add some more factorial variables which we make up completely (with some insider knowledge ;) ). Of course only females can have a joey: #offspring koala_states_min$joey&lt;-ifelse(koala_states_min$sex == &#39;female&#39;, sample(c(&#39;Yes&#39;, &#39;No&#39;), nrow(koala_states_min), prob=c(0.38, 0.62), replace = T), &#39;No&#39;) #behavior koala_states_min$behav&lt;-sample(c(&#39;Sleeping&#39;, &#39;Feeding&#39;, &#39;Just Chillin&#39;), nrow(koala_states_min), prob=c(0.5, 0.2, 0.3), replace = T) #survey tech koala_states_min$obs&lt;-sample(c(&#39;Spotlighting&#39;, &#39;Stagwatching&#39;, &#39;Opportunistic&#39;), nrow(koala_states_min), prob=c(0.4, 0.3, 0.3), replace = T) And that’s all. Of course we can make up more variables, but for our purposes of practicing data manipulation, visualization, modeling and spatial data processing, this should be sufficient. 3.3.3.5 Polishing the data and saving Now we can remove the geometry from our sf, to make it a simple data.frame: koala_final&lt;-st_drop_geometry(koala_states_min)%&gt;%select(species = Species, X, Y, state = STATENAME, region, sex, weight, size, fur, tail, age, color, joey, behav, obs ) After that we brought the data in order using dplyr’s select() and while doing so, we can also rename some columns. Next we save the data to our outputs/ folder: write.csv(koala_final, &#39;outputs/koala_final.csv&#39;, row.names = F) All set for our tutorials! Let’s dive into some data manipulation in dplyr first! Have fun :) "],
["dplyr-a-brief-introduction-to-tidy-data-manipulation.html", "Chapter 4 dplyr - A brief introduction to tidy data manipulation 4.1 Loading and checking the data 4.2 Working with dplyr", " Chapter 4 dplyr - A brief introduction to tidy data manipulation 4.1 Loading and checking the data We will work with our koala dataset. You can download it here. Furthermore you need to install the tidyverse package, which contains dplyr. install.packages(&#39;tidyverse&#39;) First we need to load the dataset we’re working with. koala&lt;-read.csv(&#39;data/koala.csv&#39;) It should contain the following columns: names(koala) ## [1] &quot;species&quot; &quot;X&quot; &quot;Y&quot; &quot;state&quot; &quot;region&quot; &quot;sex&quot; &quot;weight&quot; ## [8] &quot;size&quot; &quot;fur&quot; &quot;tail&quot; &quot;age&quot; &quot;color&quot; &quot;joey&quot; &quot;behav&quot; ## [15] &quot;obs&quot; Lets look at a structure and summary of this dataset: str(koala) ## &#39;data.frame&#39;: 242 obs. of 15 variables: ## $ species: Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num 153 148 153 153 153 ... ## $ Y : num -27.5 -22.5 -27.5 -27.5 -27.5 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num 70.8 70.4 68.7 73 65.2 ... ## $ fur : num 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... summary(koala) ## species X Y ## Phascolarctos cinereus:242 Min. :138.6 Min. :-39.00 ## 1st Qu.:150.0 1st Qu.:-34.49 ## Median :152.0 Median :-32.67 ## Mean :150.3 Mean :-32.36 ## 3rd Qu.:152.9 3rd Qu.:-30.31 ## Max. :153.6 Max. :-21.39 ## state region sex weight ## New South Wales:181 northern:165 female:127 Min. : 5.406 ## Queensland : 16 southern: 77 male :115 1st Qu.: 6.574 ## South Australia: 14 Median : 7.277 ## Victoria : 31 Mean : 7.923 ## 3rd Qu.: 8.765 ## Max. :17.889 ## size fur tail age ## Min. :64.81 Min. :1.110 Min. :1.004 Min. : 1.00 ## 1st Qu.:68.43 1st Qu.:2.410 1st Qu.:1.272 1st Qu.: 3.00 ## Median :70.27 Median :2.797 Median :1.534 Median : 7.00 ## Mean :70.94 Mean :2.896 Mean :1.507 Mean : 6.43 ## 3rd Qu.:72.33 3rd Qu.:3.217 3rd Qu.:1.750 3rd Qu.: 9.00 ## Max. :81.91 Max. :5.876 Max. :1.981 Max. :12.00 ## color joey behav obs ## chocolate brown:21 No :185 Feeding : 48 Opportunistic:65 ## dark grey :36 Yes: 57 Just Chillin: 67 Spotlighting :94 ## grey :69 Sleeping :127 Stagwatching :83 ## grey-brown :53 ## light brown :20 ## light grey :43 This dataset contains the positions of each koala in Latitude and Longitude (X and Y) as well as variables describing their physiology, behavior and how they were recorded. It’s typical presence-only wildlife data, combining observations with some data describing each individual, which could e.g. be used for distribution modeling or to test influences of other variables such as climate on behavior and physiology of this particular species. You can read more about the dataset and typical R data types in the separate koala dataset tutorial, which will introduce you to the data and explain R data and variable types. Often in these types of studies, we are not interested in all the recorded variables and thus first need to ‘clean’ our data to make it easier to work with it. dplyr is a package designed to make data ‘cleaning’ and manipulation of large datasets easier by introducing specific syntax. Let’s see how it works and compares to base R functionality! 4.2 Working with dplyr Base R sub-setting can be very tedious. Imagine we want to got the mean age for our koalas, but split it by sex. Getting one mean is easy: mean(koala[koala$sex == &#39;male&#39;, &quot;age&quot;],na.rm = TRUE) ## [1] 6.626087 Summarizing both sexes and saving it in a table takes a few lines of code: female_mean&lt;-mean(koala[koala$sex == &#39;female&#39;, &quot;age&quot;],na.rm = TRUE) male_mean&lt;-mean(koala[koala$sex == &#39;male&#39;, &quot;age&quot;],na.rm = TRUE) means&lt;-rbind(c(female_mean, male_mean)) means&lt;-as.data.frame(means) names(means)&lt;-c(&#39;female&#39;, &#39;male&#39;) Lets have a look at the results. means ## female male ## 1 6.251969 6.626087 That’s all good, but with that many lines of code quite error prone … dplyr makes data manipulation simpler. For this example, we would only require one line of code! library(dplyr) mean_age_koala&lt;-koala%&gt;%group_by(sex)%&gt;%summarise(mean_age = mean(age)) ## # A tibble: 2 x 2 ## sex mean_age ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 6.25 ## 2 male 6.63 So simple, and looks even better than our base R table too! The main functions we will explore here are dplyr’s pipe %&gt;%, select(), filter(), group_by(), summarise() and mutate(). 4.2.1 select() and dplyr’s pipe If, for example, we wanted to move forward with only a few of the variables in our dataframe we could use the select() function. This will keep only the variables you select. koala_select&lt;-select(koala, species, sex, age) ## species sex age ## 1 Phascolarctos cinereus male 8 ## 2 Phascolarctos cinereus female 10 ## 3 Phascolarctos cinereus male 1 ## 4 Phascolarctos cinereus male 1 ## 5 Phascolarctos cinereus female 10 ## 6 Phascolarctos cinereus male 12 If we open up koala_select we’ll see that it only contains the species, sex and age columns. Above we used ‘normal’ R grammar, but the strengths of dplyr lie in combining several functions using pipes. Since the pipes grammar is unlike anything we’ve seen in R before, let’s repeat what we’ve done above using pipes. koala_select_pipe&lt;-koala%&gt;%select(species, sex, age) To help you understand why we wrote that in that way, let’s walk through it step by step. First we summon the koala dataframe and pass it on, using the pipe syntax %&gt;%, to the next step, which is the select() function. In this case we don’t specify which data object we use in the select() function since in gets that from the previous pipe. 4.2.2 filter() filter() is one of the most useful dplyr functions for data manipulation. Say you’re conducting a study of only male koalas. You won’t need any data on female koalas. So lets get rid of it! koala_filter&lt;-koala%&gt;%filter(sex == &#39;male&#39;) Did it work? summary(koala_filter$sex) ## female male ## 0 115 No more females in the data! Let’s test our knowledge with a challenge. 4.2.3 Challenge 1 Write a single command (which can span multiple lines and includes pipes) that will produce a dataframe that has the values for age, size and color for females only. How many rows and columns does your dataframe have and why? Extra challenge: out of this new dataset, filter only koalas &gt;70cm in size. How many are there? This should be your data structure: nrow(challenge1) ## [1] 127 ncol(challenge1) ## [1] 3 We removed all the males, so our row number reduces from 242 to 127. Then we filter our desired columns and are now at 3 instead of 15. nrow(challenge1.2) ## [1] 46 You can find the solutions to all challenges posed here at the end of the document. Don’t peek! 4.2.4 group_by() and summarise() Now, we were supposed to be reducing the error prone repetitiveness of what can be done with base R, but up to now we haven’t done that since we would have to repeat the above for each sex. Instead of filter(), which will only pass observations that meet your criteria (in the above: sex==\"female\"), we can use group_by(), which will essentially use every unique criteria that you could have used in filter(). Let’s see what happens with our data structure when using dplyr’s group_by(). koala_group&lt;-koala%&gt;%group_by(sex) str(koala_group) ## tibble [242 x 15] (S3: grouped_df/tbl_df/tbl/data.frame) ## $ species: Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num [1:242] 153 148 153 153 153 ... ## $ Y : num [1:242] -27.5 -22.5 -27.5 -27.5 -27.5 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num [1:242] 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num [1:242] 70.8 70.4 68.7 73 65.2 ... ## $ fur : num [1:242] 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num [1:242] 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int [1:242] 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... ## - attr(*, &quot;groups&quot;)= tibble [2 x 2] (S3: tbl_df/tbl/data.frame) ## ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 ## ..$ .rows:List of 2 ## .. ..$ : int [1:127] 2 5 9 10 12 13 15 17 20 22 ... ## .. ..$ : int [1:115] 1 3 4 6 7 8 11 14 16 18 ... ## ..- attr(*, &quot;.drop&quot;)= logi TRUE You will notice that the structure of the dataframe where we used group_by() (koala_group) is not the same as the original koala dataset. A grouped dataset can be thought of as a list where each item in the list is a data.frame which contains only the rows that correspond to the a particular value ‘Sex’ (at least in the example above). The above was a bit on the uneventful side because group_by() is only really useful in conjunction with summarise(). This will allow you to create new variable(s) by using functions that repeat for each of the sex-specific data frames. That is to say, using the group_by() function, we split our original dataframe into multiple pieces, then we can run functions such as mean() or sd() within summarise(): koala_group_sum&lt;-koala%&gt;%group_by(sex)%&gt;% summarise(mean_age=mean(age)) ## # A tibble: 2 x 2 ## sex mean_age ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 6.25 ## 2 male 6.63 And there we go. We got what we wanted and summarised the mean age of our koalas for both sexes separately. And we did that using only one simple line of code! I think it is time for another challenge to test our skills! 4.2.5 Challenge 2 Calculate the average weight value per state and Sex. Which combination of state and sex has the heaviest and which combination had the lightest koalas? ## # A tibble: 8 x 3 ## # Groups: state [4] ## state sex mean_weight ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 New South Wales female 6.54 ## 2 New South Wales male 9.07 ## 3 Queensland female 5.68 ## 4 Queensland male 6.82 ## 5 South Australia female 7.59 ## 6 South Australia male 16.8 ## 7 Victoria female 7.58 ## 8 Victoria male 7.48 That is already quite powerful, but it gets even better! You’re not limited to defining only one new variable in summarise(): challenge2_ext&lt;-koala%&gt;%group_by(state, sex)%&gt;% summarise(mean_weight = mean(weight), sd_weight = sd(weight), sample_no = n()) We can create a new dataframe with as many new variables as we want. Very useful for our initial data exploration! Let’s get our hands another very useful function: mutate(). 4.2.6 mutate() We can create an entirely new variables in our initial dataset prior to (or even after) summarizing information using mutate(). Let’s say we’re interested in the weight:size ratio of our Koalas. Also we want to give each individual a numeric identifier to be able to better work with our data later on. koala_mutate&lt;-koala%&gt;%mutate(weight_size_ratio = size/weight, ID = row_number()) ## [1] &quot;species&quot; &quot;X&quot; &quot;Y&quot; ## [4] &quot;state&quot; &quot;region&quot; &quot;sex&quot; ## [7] &quot;weight&quot; &quot;size&quot; &quot;fur&quot; ## [10] &quot;tail&quot; &quot;age&quot; &quot;color&quot; ## [13] &quot;joey&quot; &quot;behav&quot; &quot;obs&quot; ## [16] &quot;weight_size_ratio&quot; &quot;ID&quot; Our dataset now has two extra columns containing the variables we were interested in. If you do not want to manipulate your raw data, you can use mutate before grouping and summarising to create the summary table straight away: koala_mutate_weight_size&lt;-koala%&gt;%mutate(weight_size_ratio = size/weight)%&gt;% group_by(sex)%&gt;% summarise(mean_weight = mean(weight), sd_weight = sd(weight), mean_weight_size = mean (weight_size_ratio), max_weight_size = max(weight_size_ratio)) ## # A tibble: 2 x 5 ## sex mean_weight sd_weight mean_weight_size max_weight_size ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 6.67 0.533 10.4 12.9 ## 2 male 9.31 2.38 8.21 11.9 Great! Let’s end the lesson with another challenge, combining all the functions we have looked at today. 4.2.7 Challenge 3 Calculate the average tail length and fur thickness for a group of 20 randomly selected males and females from New South Wales. Then arrange the mean tail length in descending order. Hint: Use the dplyr functions arrange() and sample_n(), they have similar syntax to other dplyr functions. Look at the help by calling ‘?function’, e.g. ?arrange. ## # A tibble: 2 x 3 ## sex mean_tail mean_fur ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 1.54 2.52 ## 2 male 1.54 2.99 Since we are sampling randomly, these will look different for each of you. 4.2.8 Pivoting Another really useful processing option when manipulating or re-arranging data is pivoting, especially when you come from an Microsoft Excel background. For this, a bit of knowledge on tidy data is required. When we process data in R, it is extremely beneficial to have a long dataset, which we call tidy. These type of data follow three principles: Variables make up our columns Observations are in the rows Values are in the cells Our koala dataset is an example of tidy data. Let’s reduce our dataset to make an example: koala_subsample&lt;-koala%&gt;%select(species, region, sex, tail, age) head(koala_subsample) ## species region sex tail age ## 1 Phascolarctos cinereus northern male 1.168241 8 ## 2 Phascolarctos cinereus northern female 1.562456 10 ## 3 Phascolarctos cinereus northern male 1.056640 1 ## 4 Phascolarctos cinereus northern male 1.801244 1 ## 5 Phascolarctos cinereus northern female 1.625600 10 ## 6 Phascolarctos cinereus northern male 1.086675 12 Tidy datasets, are also called indexed dataset. We can look up a measure using both region and sex as an index (like when we were grouping before). Let’s assume we collect another measure of tail-length on another visit to our sites. In data-entry manner, this would probably go into another column: koala_subsample&lt;-koala_subsample%&gt;%mutate(tail2 = runif(nrow(koala_subsample), min = 2, max = 3)) head(koala_subsample) ## species region sex tail age tail2 ## 1 Phascolarctos cinereus northern male 1.168241 8 2.466089 ## 2 Phascolarctos cinereus northern female 1.562456 10 2.762181 ## 3 Phascolarctos cinereus northern male 1.056640 1 2.272856 ## 4 Phascolarctos cinereus northern male 1.801244 1 2.048094 ## 5 Phascolarctos cinereus northern female 1.625600 10 2.390759 ## 6 Phascolarctos cinereus northern male 1.086675 12 2.920746 Our table is now wide, because we added a variable, that is not indexed. This can cause problems, especially when we want to visualize these two measurements. We will introduce ggplot2 for visualizing in later tutorials, but here is just a basic boxplot ggplot(koala_subsample, aes(x=species, y=tail)) + geom_boxplot() ggplot(koala_subsample, aes(x=species, y=tail2)) + geom_boxplot() We can plot either column, but not both. Nevertheless, we can use another column that is already in long format, to separate the boxes: ggplot(koala_subsample, aes(x=species, y=tail, fill = region)) + geom_boxplot() Southern koalas have shorter tails, interesting :) If we want to compare our two measurements though, we need them in one column, and that is where pivoting comes in handy: koala_pivot&lt;-koala_subsample%&gt;%pivot_longer(cols = c(tail, tail2), names_to = &#39;tail_measure&#39;, values_to = &#39;tail_length&#39;) head(koala_pivot) ## # A tibble: 6 x 6 ## species region sex age tail_measure tail_length ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Phascolarctos cinereus northern male 8 tail 1.17 ## 2 Phascolarctos cinereus northern male 8 tail2 2.47 ## 3 Phascolarctos cinereus northern female 10 tail 1.56 ## 4 Phascolarctos cinereus northern female 10 tail2 2.76 ## 5 Phascolarctos cinereus northern male 1 tail 1.06 ## 6 Phascolarctos cinereus northern male 1 tail2 2.27 We create the index column tail_measure to address our two measurements. The column containing the measurements is now called tail_length. Let’s see what has changed between the first and second measurement: ggplot(koala_pivot, aes(x=species, y=tail_length, fill = tail_measure)) + geom_boxplot() Wow, seems the tails have grown a lot in the past ;) We can also facet to check both the different measures and the genders: ggplot(koala_pivot, aes(x=species, y=tail_length, fill = tail_measure)) + geom_boxplot()+ facet_grid(~sex) Great, now we know how to make our dataset long and thus tidy! Let’s create a dataset for you to practice: koala_subsample2&lt;-koala%&gt;%select(species, region, sex, age, tail, joey,)%&gt;% mutate(age2 = age+sample(3, nrow(koala_subsample), replace = T), tail2 = runif(nrow(koala_subsample), min = 2, max = 3), tail3 = runif(nrow(koala_subsample), min = 1, max = 1.5), joey2 = ifelse(koala$sex == &#39;female&#39;, sample(c(&#39;Yes&#39;, &#39;No&#39;), nrow(koala), prob=c(0.3, 0.7), replace = T), &#39;No&#39;), joey3 = ifelse(koala$sex == &#39;female&#39;, sample(c(&#39;Yes&#39;, &#39;No&#39;), nrow(koala), prob=c(0.5, 0.5), replace = T), &#39;No&#39;),) head(koala_subsample2) ## species region sex age tail joey age2 tail2 ## 1 Phascolarctos cinereus northern male 8 1.168241 No 10 2.659025 ## 2 Phascolarctos cinereus northern female 10 1.562456 Yes 12 2.370477 ## 3 Phascolarctos cinereus northern male 1 1.056640 No 2 2.934149 ## 4 Phascolarctos cinereus northern male 1 1.801244 No 2 2.380310 ## 5 Phascolarctos cinereus northern female 10 1.625600 No 11 2.220764 ## 6 Phascolarctos cinereus northern male 12 1.086675 No 13 2.534960 ## tail3 joey2 joey3 ## 1 1.279518 No No ## 2 1.146527 No No ## 3 1.228514 No No ## 4 1.269394 No No ## 5 1.203511 Yes No ## 6 1.249298 No No So we continued our measurements over 3 years, each koala aged 1-3 years which is expressed in the age2 column. When we revisited, we took additional measurements of tail length, and checked weather females had a joey. Can you tidy up this table? 4.2.9 Challenge 4 To make the changes more visible, give each individual an ID that describes the individual using mutatue(), as we did before. It’s easiest in steps! Refer to ?pivot_longer for help. Please sort the dataset as follows in the end: species, region, sex, all thew index columns, all measurement columns. ## # A tibble: 6 x 10 ## ID species region sex age_check tail_measure joey_check age tail ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 Phasco~ north~ male age tail joey 8 1.17 ## 2 1 Phasco~ north~ male age tail joey2 8 1.17 ## 3 1 Phasco~ north~ male age tail joey3 8 1.17 ## 4 1 Phasco~ north~ male age tail2 joey 8 2.66 ## 5 1 Phasco~ north~ male age tail2 joey2 8 2.66 ## 6 1 Phasco~ north~ male age tail2 joey3 8 2.66 ## # ... with 1 more variable: joey &lt;chr&gt; With this meany measurements, we can illustrate the long table format well. Each individual now has 18 rows, because our maximum measurement was 3x3 for joey and tail. Each ID number is one koala. You can easily reverse your changes using pivot_wider(). Let’s reverse the age changes: koala_pivot_wide&lt;-koala_pivot_subsample%&gt;%pivot_wider(names_from = age_check, values_from = age) head(koala_pivot_wide) ## # A tibble: 6 x 10 ## ID species region sex tail_measure joey_check tail joey age ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 Phasco~ north~ male tail joey 1.17 No 8 ## 2 1 Phasco~ north~ male tail joey2 1.17 No 8 ## 3 1 Phasco~ north~ male tail joey3 1.17 No 8 ## 4 1 Phasco~ north~ male tail2 joey 2.66 No 8 ## 5 1 Phasco~ north~ male tail2 joey2 2.66 No 8 ## 6 1 Phasco~ north~ male tail2 joey3 2.66 No 8 ## # ... with 1 more variable: age2 &lt;int&gt; Having each variable in a separate column is important e.g. in modeling, when you have to specify you predictor variables by column. Let’s practice pivot_wider() with some made-up koala data data: new_data&lt;-rbind(cbind.data.frame(ID = 1:40, sex = &#39;male&#39;, measurement = c(&#39;tail1&#39;, &#39;tail2&#39;, &#39;tail3&#39;, &#39;tail4&#39;)), cbind.data.frame(ID = 1:40, sex = &#39;female&#39;, measurement = c(&#39;tail1&#39;, &#39;tail2&#39;, &#39;tail3&#39;, &#39;tail4&#39;)))%&gt;% mutate(tail = runif(80, min = 1, max = 2), animal = rep(c(1:20), each = 4)) head(new_data) ## ID sex measurement tail animal ## 1 1 male tail1 1.694088 1 ## 2 2 male tail2 1.973582 1 ## 3 3 male tail3 1.214056 1 ## 4 4 male tail4 1.698419 1 ## 5 5 male tail1 1.927549 2 ## 6 6 male tail2 1.053456 2 4.2.10 Challenge 5 Create a wide dataset with one column per tail measurement, add an individual ID for each koala. Which column do you need to remove for this to work? ## # A tibble: 6 x 6 ## sex animal tail1 tail2 tail3 tail4 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 male 1 1.69 1.97 1.21 1.70 ## 2 male 2 1.93 1.05 1.36 1.84 ## 3 male 3 1.74 1.77 1.56 1.61 ## 4 male 4 1.48 1.91 1.32 1.24 ## 5 male 5 1.08 1.29 1.01 1.05 ## 6 male 6 1.29 1.91 1.41 1.29 Good job on all this! Let’s move on to data visualization using ggplot2 :) 4.2.11 Solution to all challenges 4.2.11.1 Challenge 1 challenge1&lt;-koala%&gt;%filter(sex == &#39;female&#39;)%&gt;% select(age, size, color) challenge1.2&lt;-challenge1%&gt;%filter(size&gt;70) 4.2.11.2 Challenge 2 challenge2&lt;-koala%&gt;%group_by(state, sex)%&gt;% summarise(mean_weight = mean(weight)) 4.2.11.3 Challenge 3 challenge3&lt;-koala%&gt;%filter(state == &#39;New South Wales&#39;)%&gt;% group_by(sex)%&gt;% sample_n(20)%&gt;% summarise(mean_tail = mean(tail), mean_fur = mean(fur))%&gt;% arrange(desc(mean_tail)) 4.2.11.4 Challenge 4 koala_pivot_subsample&lt;-koala_subsample2%&gt;%pivot_longer(cols = c(age, age2), names_to = &#39;age_check&#39;, values_to = &#39;age&#39;) koala_pivot_subsample&lt;-koala_pivot_subsample%&gt;%pivot_longer(cols = c(tail, tail2, tail3), names_to = &#39;tail_measure&#39;, values_to = &#39;tail&#39;) koala_pivot_subsample&lt;-koala_pivot_subsample%&gt;%pivot_longer(cols = c(joey, joey2, joey3), names_to = &#39;joey_check&#39;, values_to = &#39;joey&#39;) koala_pivot_subsample&lt;-koala_pivot_subsample%&gt;%select(1:3, 4,6,8,5,7,9) #instead of typing the names you can use the column number! 4.2.11.5 Challenge 5 new_data_wide&lt;-new_data%&gt;%select(-ID)%&gt;%pivot_wider(names_from = measurement, values_from = tail) "],
["introduction-to-data-visualization-in-ggplot2.html", "Chapter 5 Introduction to data visualization in ggplot2", " Chapter 5 Introduction to data visualization in ggplot2 Structure used by ggplot is basic.Identify data, specify a mapping, and then choose an appropriate geometry to display data. + is the key to constructing sophisticated ggplot2 graphics. It allows you to start simple, then get more and more complex, checking your work at each step. aesthetic = variable describing which variables in the layer data should be mapped to which aesthetics used by the paired geom/stat. The expression variable is evaluated within the layer data, so there is no need to refer to the original dataset (i.e., use ggplot(df,aes(variable)) instead of ggplot(df,aes(df$variable))). The names for x and y aesthetics are typically omitted because they are so common; all other aesthetics must be named. ggplot(data = koala) p &lt;- ggplot(data = koala, mapping = aes(x = weight, y = size)) To see the individual points, specify the geometry that you would like to use. For X,Y data, X, Y, … List of name value pairs. Elements must be either quoted calls, strings, onesided formulas or constants. we can use geom_point(). p + geom_point() p + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Add other features of the geom. Try playing around with shape=, size=, alpha=. For shape, use integer values from 0 to 20 (although there are many others to choose from). For size, use positive non-zero values (non-integers are OK). For alpha, use values from 0 to 1. You can use more than one of these at a time. Just separate them with commas in the geom statement. We can also add nice (or more detailed) labelling. To do this we just need to add the labs component to the overall statement (like adding the geom_point() or geom_smooth). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The boxplot compactly displays the distribution of a continuous variable. and coord_flip flips the x axis to y and reverse r&lt;-ggplot(data=koala, mapping=aes(x=state, y=tail)) r+geom_boxplot() r+geom_boxplot()+coord_flip() Reorder is a generic function. The “default” method treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric. r&lt;-ggplot(data=koala, aes(x=reorder(state, tail), y=tail)) r+geom_boxplot() stat_summary operates on unique x; stat_summary_bin operates on binned x. They are more flexible versions of stat_bin(): instead of just counting, they can compute any aggregate. ggplot(koala, aes(x=sex,y=tail))+geom_boxplot()+ stat_summary(fun.y = mean, geom=&quot;point&quot;, size=3,color=&quot;blue&quot;) Reverse the condition logic Its actually very simple with R and dplyr. Its !(exclamation mark). And, it goes like this. koala%&gt;% filter(!state==&quot;New South Wales&quot;)%&gt;% ggplot(aes(state,weight))+ geom_boxplot(aes(x=state, y=weight)) + stat_summary(fun.y = mean, geom=&quot;point&quot;, size=3,color=&quot;blue&quot;) "],
["introduction-to-linear-models-in-r.html", "Chapter 6 Introduction to linear models in R", " Chapter 6 Introduction to linear models in R linear regression model is represented as: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon \\] Each of the model assumptions concerns the error term of the regression model. These are: Individual observations are independent Response data are normally distributed Variance is homogeneous across range of predictor Data are linear qplot(koala$weight) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. qplot(log(koala$weight)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. qplot(koala$fur) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. linearmodel&lt;-lm(weight~fur, data=koala) summary(linearmodel) ## ## Call: ## lm(formula = weight ~ fur, data = koala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8853 -1.0215 -0.2989 0.9245 8.4996 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.4409 0.4569 7.531 1.02e-12 *** ## fur 1.5477 0.1527 10.137 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.792 on 240 degrees of freedom ## Multiple R-squared: 0.2998, Adjusted R-squared: 0.2969 ## F-statistic: 102.8 on 1 and 240 DF, p-value: &lt; 2.2e-16 anova(linearmodel) ## Analysis of Variance Table ## ## Response: weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fur 1 330.01 330.01 102.76 &lt; 2.2e-16 *** ## Residuals 240 770.77 3.21 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 check_model(linearmodel, check=c(&quot;qq&quot;, &quot;normality&quot;, &quot;ncv&quot;, &quot;outliers&quot;)) ## Not enough model terms in the conditional part of the model to check for multicollinearity. ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. lm&lt;-lm(size~fur, data=koala) summary(lm) ## ## Call: ## lm(formula = size ~ fur, data = koala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1884 -2.3545 -0.2146 1.8404 10.1324 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 63.266 0.853 74.168 &lt;2e-16 *** ## fur 2.650 0.285 9.295 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.346 on 240 degrees of freedom ## Multiple R-squared: 0.2647, Adjusted R-squared: 0.2616 ## F-statistic: 86.4 on 1 and 240 DF, p-value: &lt; 2.2e-16 anova(lm) ## Analysis of Variance Table ## ## Response: size ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fur 1 967.19 967.19 86.4 &lt; 2.2e-16 *** ## Residuals 240 2686.64 11.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 check_model(lm, check=c(&quot;qq&quot;, &quot;normality&quot;, &quot;ncv&quot;, &quot;outliers&quot;)) ## Not enough model terms in the conditional part of the model to check for multicollinearity. ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Correlation is a statistical technique that can show whether and how strongly pairs of variables are related.The main result of a correlation is called the correlation coefficient (or “r”). It ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related. corkoala&lt;-koala %&gt;% select_if(is.numeric)%&gt;% select(-c(1,2)) corMat&lt;-cor(corkoala, use=&quot;complete.obs&quot;, method = &quot;pearson&quot;) corrplot(corMat, method=&quot;shade&quot;, type=&quot;lower&quot;, diag = FALSE, addCoef.col = &quot;black&quot;) Determine if two or more samples are from the same population -H0: -Sample means are all equal(i.e., ??A= ) -There is no effect of the factor on the response variablel -If reject H0 -Suggests that at least one sample mean is different from the others -If don’t reject H0 -No evidence that any of the sample means are different from the overall population mean anova&lt;-aov(weight~age, koala) summary(anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 2.7 2.744 0.6 0.439 ## Residuals 240 1098.0 4.575 anova1&lt;-aov(tail~age, koala) summary(anova1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 0.344 0.3441 4.448 0.036 * ## Residuals 240 18.568 0.0774 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova2&lt;-aov(tail~age+sex, koala) summary(anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 2.7 2.744 0.6 0.439 ## Residuals 240 1098.0 4.575 anova(anova, anova1, anova2, test=&quot;chi&quot;) ## Analysis of Variance Table ## ## Response: weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 2.74 2.7436 0.5997 0.4395 ## Residuals 240 1098.04 4.5752 Making a new variable to start with presence absence data ifelse returns a value with the same shape as test which is filled with elements selected from either yes or no depending on whether the element of test is TRUE or FALSE. mutate Mutate adds new variables and preserves existing; transmute drops existing variables. koala&lt;-koala %&gt;% mutate(presabs=ifelse(joey == &quot;Yes&quot;,&quot;1&quot;,&quot;0&quot;)) summary(as.factor(koala$presabs)) ## 0 1 ## 185 57 A series of test/training partitions are created using createDataPartition Any supervised machine learning task require to split the data between a train set and a test set. You can create in the other supervised learning tutorials to create a train/test set. library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift set.seed(1234) train &lt;- createDataPartition(koala$presabs, time=1,#the number of partitions to create p = 0.7,#he percentage of data that goes to training list=F) data_train &lt;- koala[train,] data_test&lt;- koala[-train,] Build the model formula &lt;- as.numeric(presabs)~fur+size+weight pre &lt;- glm(formula, data = data_train, family = &#39;binomial&#39;) summary(pre) ## ## Call: ## glm(formula = formula, family = &quot;binomial&quot;, data = data_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.43208 -0.74687 -0.22938 -0.00134 2.00832 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 18.01763 5.03050 3.582 0.000341 *** ## fur 1.41075 0.41141 3.429 0.000606 *** ## size -0.19116 0.07451 -2.565 0.010304 * ## weight -1.37326 0.39065 -3.515 0.000439 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 185.50 on 169 degrees of freedom ## Residual deviance: 137.95 on 166 degrees of freedom ## AIC: 145.95 ## ## Number of Fisher Scoring iterations: 7 Assess the performance of the model Confusion Matrix The confusion matrix is a better choice to evaluate the classification performance compared with the different metrics you saw before. The general idea is to count the number of times True instances are classified are False. To compute the confusion matrix, you first need to have a set of predictions so that they can be compared to the actual targets. predict &lt;- predict(pre, data_test, type = &#39;response&#39;) # confusion matrix table_mat &lt;- table(data_test$presabs, predict &gt; 0.5) table_mat ## ## FALSE TRUE ## 0 53 2 ## 1 12 5 Each row in a confusion matrix represents an actual target, while each column represents a predicted target. You can calculate the model accuracy by summing the true positive + true negative over the total observation accuracy_Test &lt;- sum(diag(table_mat))/sum(table_mat) accuracy_Test ## [1] 0.8055556 😸 "],
["functions-in-r.html", "Chapter 7 Functions in R 7.1 Introduction 7.2 What is an R function? 7.3 Nesting functions 7.4 Assigning variables within functions 7.5 Challenge 1 7.6 Using return() 7.7 Challenge 2 7.8 Solutions to challenges", " Chapter 7 Functions in R 7.1 Introduction Basic mathematical operations are the heart of data analysis. While statistical analysis can seem far removed from the arithmetic you learned in primary school, complex analyses are still built on very basic mathematics! As a computing language, the main purpose of R is to streamline these operations, allowing you to build up complex analyses with relatively little work. Even very basic, commonly used operations like mean() are actually functions. That is, they take basic inputs and evaluate some string of mathematical operations, and produce an output. There are many built-in R functions, and endless others available in R packages. One of the most useful tools in R is the ability to write user-defined functions that simplify your processing and reduce the risk of making simple errors. This section will introduce basic function structure and show you how to write your own. 7.1.1 Mathematical operations A quick reminder about mathematical and conditional operators in R: Operator Description Addition Subtraction Multiplication / Division ^ Exponential &gt; Greater than &lt; Less than == *Equal to &gt;= and &lt;= Greater than/less than or equal to != Not equal to 1 The = symbol alone assigns a value; you need == for a conditional statement 7.2 What is an R function? Functions in R take an input value, run a chunk of code, and generate an output. R packages include numerous functions that can do nearly anything you desire, but research often requires specific processes or sequences of commands designed specifically to work with your data structure. You can write your own functions in R to perform these tasks. Here is a very simple function: fun1 &lt;- function (a, b){ a + b } Here, a and b are identified as the required input variables. The code written within the brackets describes the mathematical operation to be applied to the input variables. Let’s run the function on some numbers: fun1(a = 7, b = 12) ## [1] 19 A function can be applied to a vector of inputs. In this scenario, the function will be applied to each pair of values in the vector sequence: vals_a &lt;- c(4, 7, 1) vals_b &lt;- c(2, 1, 3) fun1(a = vals_a, b = vals_b) ## [1] 6 8 4 User defined functions can include as many variables as you want, and the inputs can even be labeled with words: fun2 &lt;- function (length, width, height){ length * width * height } When executing a function, you don’t need to type up the variable names, but they should be in the correct order. fun2(3,6,9) ## [1] 162 7.3 Nesting functions Functions can also include other functions, which is particularly useful in the context of repeating complex data analyses. Here is a basic example: fun3 &lt;- function (a, b){ mean(a) + b } fun3(vals_a, vals_b) ## [1] 6 5 7 It is important to note that in this situation, the internal function (mean()) takes a vector of input values. This function will calculate the mean of the vals_a vector and then add this to each separate value of vals_b. This can be very useful, but it’s critical that you know exactly how your function processes input values or you may run into problems! 7.3.1 Some basic built-in mathematical functions Here are a base R functions that might be useful (most are self explanatory!): Function Description mean() Mean max() Maximum value min() Minimum value median() Median sum() Sum var() Variance cor() Correlation sd() Standard deviation 7.4 Assigning variables within functions In some more complex analyses, the nested functions may generate data that needs to be stored as variables so they can be used as input for the next process. Here is a function that calculates the mean of our a values, adds b, and then squares the value calculated in the first step. fun4 &lt;- function (a, b){ val &lt;- mean(a) + b sqval &lt;- val^2 } output &lt;- fun4(a = vals_a, b = 3) output ## [1] 49 7.5 Challenge 1 Write a function that identifies the median value of a, adds the minimum value of a to the median value, and then calculates the square root of that value. Execute the function on this vector: a &lt;- c(1, 3, 4, 2, 8, 10, 3, 8) 7.6 Using return() If your function assigns multiple variables, you can use return() to determine the data that is output from the function. This can be useful if your function includes writing a data set, or includes another process that is not intended to return anything to R as well as some process intended to return data to the R environment. #Let&#39;s see what happens without using the return() function fun5 &lt;- function (a, b){ val &lt;- mean(a) + b sqval &lt;- val^2 write.csv(sqval, &quot;outputs/filename.csv&quot;) } output &lt;- fun5(a = vals_a, b = 3) output ## NULL Our output here is NULL because the default output for a function will come from the last command within the function. Here, write.csv() does not produce data to be returned to R. If we add return(), our function will write the .csv and also return the data we want into the environment: #Now let&#39;s add return() fun6 &lt;- function (a, b){ val &lt;- mean(a) + b sqval &lt;- val^2 write.csv(sqval, &quot;outputs/filename.csv&quot;) return(sqval) } output &lt;- fun6(a = vals_a, b = 3) output ## [1] 49 Much better! 7.7 Challenge 2 Write a function for two vectors a and b that: Calculates the sum of the means of a and b and assigns this value to a variable “m” Calculates m to the power of 7 and assgins this value to a variable “s” Removes “m” from the environment (Hint: rm()) Returns the value of “s” Execute the function on the following vectors: a &lt;- c(7, 2, 4) b &lt;- c(20, 3, 5) 7.8 Solutions to challenges 7.8.1 Challenge 1 a &lt;- c(1, 3, 4, 2, 8, 10, 3, 8) #One way: challenge1a &lt;- function(a) { medval &lt;- median(a) minval &lt;- min(a) (medval + minval)^-2 } challenge1a(a) ## [1] 0.04938272 #Even simpler: challenge1b &lt;- function(a) { (median(a) + min(a))^-2 } challenge1b(a) ## [1] 0.04938272 7.8.2 Challenge 2 a &lt;- c(7, 2, 4) b &lt;- c(20, 3, 5) challenge2 &lt;- function(a, b) { m &lt;- mean(a) + mean(b) s &lt;- m^7 rm(m) return(s) } challenge2(a, b) ## [1] 89050880 "],
["working-with-spatial-data-in-r.html", "Chapter 8 Working with spatial data in R 8.1 Transforming a dataset into an sf object 8.2 Loading shapefiles into R, transforming and plotting 8.3 Simple geometric operations 8.4 Making a map using ggplot2 8.5 R raster basics 8.6 Plotting rasters in ggplot2 8.7 Solutions to Challenges", " Chapter 8 Working with spatial data in R Many of you will be familiar with using spatial data such as vector data in form of shapefiles or raster data in GIS software, which can handle these files intuitively. Nevertheless, GIS do have some disadvantages when it comes to either large data sets or automation processes. R offers a full integration of spatial tools and references such as GDAL or PROJ and some great packages to deal with this data seamlessly. Today we are going to learn how to use the package sf and have a brief detour towards raster data in R. You are going to need to install the following packages for this tutorial: install.packages(c(&#39;tidyverse&#39;, &#39;sf&#39;, &#39;raster&#39;, &#39;viridis&#39;)) And then call the packages: library(sf) library(raster) library(viridis) library(tidyverse) We will be working on a dataset of koala observations, that you can download here, as well as a basic administrative Areas shapefile of Australia, which you can find here. For the last part we will be looking at rasters, with a digital elevation model (DEM) file as example. You can find it for download here. Please make sure these are stored within a data/ folder in an R project (or working directory). 8.1 Transforming a dataset into an sf object Let’s first load in our dataset and have a look at it’s structure koala &lt;- read.csv(&#39;data/koala.csv&#39;) head(koala) # see the first few rows in the console ## species X Y state region sex ## 1 Phascolarctos cinereus 153.2155 -27.49284 Queensland northern male ## 2 Phascolarctos cinereus 148.1443 -22.47617 Queensland northern female ## 3 Phascolarctos cinereus 153.2285 -27.50298 Queensland northern male ## 4 Phascolarctos cinereus 152.6000 -27.50000 Queensland northern male ## 5 Phascolarctos cinereus 153.2817 -27.52589 Queensland northern female ## 6 Phascolarctos cinereus 152.8330 -27.20000 Queensland northern male ## weight size fur tail age color joey behav ## 1 7.119754 70.80159 1.858696 1.168241 8 grey No Sleeping ## 2 5.451345 70.38537 1.852801 1.562456 10 grey-brown Yes Sleeping ## 3 6.630577 68.65867 2.479280 1.056640 1 light grey No Just Chillin ## 4 6.470019 72.98919 1.923974 1.801244 1 grey No Sleeping ## 5 5.620447 65.19529 1.945341 1.625600 10 grey-brown No Sleeping ## 6 7.287674 70.56514 1.688897 1.086675 12 grey-brown No Feeding ## obs ## 1 Spotlighting ## 2 Opportunistic ## 3 Spotlighting ## 4 Stagwatching ## 5 Stagwatching ## 6 Opportunistic As we can see it contains lots of variables related to each of the observed koalas, such as sex, weight or in which state the observation was made. Additionally, whoever collected this data was so kind to also include the X and Y coordinates, where the observation was made. This we can use to transform this into an sf (simple feature) object. koala_sf &lt;- st_as_sf(koala, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 4326) str(koala_sf) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 242 obs. of 14 variables: ## $ species : Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num 70.8 70.4 68.7 73 65.2 ... ## $ fur : num 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... ## $ geometry:sfc_POINT of length 242; first list element: &#39;XY&#39; num 153.2 -27.5 ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;names&quot;)= chr &quot;species&quot; &quot;state&quot; &quot;region&quot; &quot;sex&quot; ... The sf object is a data.frame with a geometry list-column. It supports different format and spatial references. The file as it exists in your R environment is what you would call the Attribute table in a GIS software. In R you can use and manipulate it as any other data.frame. When converting a table into a sf we need to indicate the columns containing coordinates using coords = c() and we can decide on a coordinate reference system using crs=. Here we decide to use WGS84 (CRS = 4326), a standard Mercator coordinate frame for the Earth given in Latitude and Longitude. We can plot our now spatial data with the standard base R command: plot(koala_sf) This will try to plot all columns (because they are now all spatially referenced). To only plot the geometries, you can: plot(koala_sf$geometry) So far so good, but we will need a bit more data to make a nice map! 8.2 Loading shapefiles into R, transforming and plotting You can load any vector file, such as .shp or .gpkg using st_read. Let’s get our Australia map into the environment and check it out: states &lt;- st_read(&quot;data/Australia/Australia_proj.shp&quot;) ## Reading layer `Australia_proj&#39; from data source `C:\\Users\\khh\\Documents\\GitHub\\bookdown-playaround\\data\\Australia\\Australia_proj.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 8 features and 15 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -2063975 ymin: -4965263 xmax: 1891143 ymax: -1285856 ## proj4string: +proj=lcc +lat_1=-18 +lat_2=-36 +lat_0=0 +lon_0=134 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs plot(states$geometry) Great, we even have states column, which will we will use a bit later. To plot, simply use base R syntax of plotting and then plotting another graph on top using add = T. Instead of calling plot(file$geometry), we can also call st_geometry() plot(st_geometry(states), axes = TRUE) plot(st_geometry(koala_sf), # why does this not work? col = &quot;blue&quot;, add = T) Hm, our states are plotting fine, but where are our koala locations? Maybe we should check the coordinate reference system to see if they match… Here’s how that’s done: st_crs(states) ## Coordinate Reference System: ## No user input ## wkt: ## PROJCS[&quot;GDA94_Geoscience_Australia_Lambert&quot;, ## GEOGCS[&quot;GCS_GDA_1994&quot;, ## DATUM[&quot;Geocentric_Datum_of_Australia_1994&quot;, ## SPHEROID[&quot;GRS_1980&quot;,6378137,298.257222101]], ## PRIMEM[&quot;Greenwich&quot;,0], ## UNIT[&quot;Degree&quot;,0.017453292519943295]], ## PROJECTION[&quot;Lambert_Conformal_Conic_2SP&quot;], ## PARAMETER[&quot;standard_parallel_1&quot;,-18], ## PARAMETER[&quot;standard_parallel_2&quot;,-36], ## PARAMETER[&quot;latitude_of_origin&quot;,0], ## PARAMETER[&quot;central_meridian&quot;,134], ## PARAMETER[&quot;false_easting&quot;,0], ## PARAMETER[&quot;false_northing&quot;,0], ## UNIT[&quot;Meter&quot;,1]] st_crs(koala_sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCS[&quot;WGS 84&quot;, ## DATUM[&quot;WGS_1984&quot;, ## SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563, ## AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]] Damn, they do not match. Our Australia shapefile has a different (weird) projection. If you feel it’s tedious to check the CRS in the console, simply type st_crs(states)==st_crs(koala_sf) ## [1] FALSE FALSE is returned, so they don’t match. We need to transform one of them to be able to plot our spatial points on top of the Australia map. sf has a simple solution for this using st_transform(). Let’s do that and plot straight away. koala_proj &lt;- st_transform(koala_sf, crs = st_crs(states)) plot(st_geometry(states), axes = TRUE) plot(st_geometry(koala_proj), col = &quot;blue&quot;, add = T) Excellent! In st_transform we can call any crs from another spatial file in the environment, our define our own using a proj4string. If we look at our plot, we can see that the coordinates on the axis look weird. We’d rather like latitude and longitude to make it look more comprehensive. That’s now up to you to fix it! 8.2.1 Challenge 1 Load the Australia_proj and koala data again and plot them on top of each other in Mercator projection (CRS: 4326). Note: check the spatial reference of the maps first! Extra challenge: color the koala points based on state! Check ?sf::plot for tips Looks good! You can find the solutions to all challenges posed here at the end of the document. Don’t peek! Let’s make our maps look even better. 8.3 Simple geometric operations sf supports any geometric operations you know from GIS software. We will just touch cropping here, but you can also merge, intersect, overlap and many more. For the purpose of mapping, st_crop() is most relevant. Let’s say we want to create a map of all our Koala locations, we won’t necessarily need to show all of Australia, knowing that Koalas are only distributed along the east coast. Let’s crop the Australia shapefile by the extent of our observations: australia_koala_area&lt;-st_crop(states_proj, koala_sf) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar plot(st_geometry(australia_koala_area), axes = T) plot(st_geometry(koala_sf), add = T) Great. Let’s say we want to save the new cropped shapefile to an outputs/ folder in our project. For this we simply use st_write(): st_write(australia_koala_area, &#39;outputs/koala_shape.gpkg&#39;) You can choose different file extensions. Here we are saving a geopackage, as it results in only one file, rather than 4-6 for a .shp extension. Up to you ;) Let’s make a good looking map with our data! 8.4 Making a map using ggplot2 ggplot2 supports plotting spatial data by calling geom_sf(), after defining your data to plot. In simple terms we can create a map of Australia by calling: ggplot(data = states_proj) + geom_sf() which already looks a lot better than base R. But there are a lot of improvements to be made to create a publication-ready map. First we can assign colors based on columns in your data. We can also remove the standard grey ggplot background with a more map-friendly white grid using theme_minimal() ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal() We don’t really need a legend here, let’s remove it: ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal()+ guides(fill = F) Using coord_sf() we can even change the projection while plotting in ggplot2, without the need of going back to reprojecting or transforming in sf. ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal()+ guides(fill = F)+ coord_sf(crs = 3112) Did you see what changed? Finally we can change the fore- and background color to make it look like proper map in a nice blue ocean: ggplot(data = states_proj)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ theme_minimal()+ coord_sf(crs = 3112)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;)) 8.4.1 Challenge 2 Add the koala positions on top of the Australia map in ggplot2. Only plot the area that has koalas and color the points by sex of the individual. Please also give the map in Mercator projection You will need to: Add a second layer to your ggplot Define the crs Good job! But unfortunately our cut-out looks like an island surrounded by seawater. If we want the map to look like a close-up of eastern Australia, we need to tell ggplot2 to start plotting at x/y = 0: ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;))+ scale_x_continuous(expand = c(0,0))+ scale_y_continuous(expand = c(0,0)) When working with these types of maps, you will often be asked to create an inset map that shows the entire area that you cropped from for reference. Let’s add a little Australia shape with a bounding box of our study area to our map. First we need to create the a base map with bounding box and save it to the environment: inset&lt;-ggplot(data = states_proj, show.legend = &quot;point&quot;)+ geom_sf()+ geom_rect(xmin = extent(australia_koala_area)[1], xmax = extent(australia_koala_area)[2], ymin = extent(australia_koala_area)[3], ymax = extent(australia_koala_area)[4], fill = NA, colour = &quot;black&quot;, size = 1.5)+ labs(x = &#39;&#39;, y = &#39;&#39;, title = &#39;&#39;)+ coord_sf(crs = 4326)+ theme_void() inset geom_rect can create any rectangular shape within your ggplot based on xmin, xmax and so on. Since these are dependent on your scales, for mapping we can use the extent of our cropped area from before. extent() is part of the raster package: extent(australia_koala_area) ## class : Extent ## xmin : 138.5795 ## xmax : 153.5651 ## ymin : -39 ## ymax : -21.3906 By using [1], [2] and so on, we call the respective coordinate from the four rows that this function is giving us. Using theme_void() is getting rid of any axis and grids, so that we have the map only. We also need to save our map as an object: map&lt;-ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;))+ scale_x_continuous(expand = c(0,0))+ scale_y_continuous(expand = c(0,0)) We use annotation_custom() and ggplotGrob() to place an inset into the main map: map+annotation_custom(ggplotGrob(inset), xmin = 139, xmax = 144, ymin = -26, ymax = -20) The size and position here is defined by xmin = etc. This needs a bit of fiddling around to get it right. Also be mindful that we are using a Mercator projection here and are in the southern hemisphere, so our ymin and ymax values need to be given in negative form. Now this is a proper map, ready to be put in your next publication! We will briefly talk about rasters, because you are likely to use them if you are working with shapefiles anyway. It is similarly easy to work with rasters in R: 8.5 R raster basics Raster support in R is made available using the package raster. One called, we can load and plot a raster using the same command. Let’s get our DEM to play with: dem&lt;-raster(&#39;data/DEM.tif&#39;) summary(dem) ## DEM ## Min. 0.00000 ## 1st Qu. 89.23475 ## Median 157.27505 ## 3rd Qu. 397.71189 ## Max. 2140.81299 ## NA&#39;s 0.00000 plot(dem) In a raster, cell (or pixel) has a spatial reference and a value. In this case we have the elevation in meters for each degree (0.01, 0.01) in southeastern Australia. Let’s say we’re only interested in Victoria. We can use a spatial vector to mask out the state area from the raster. First we need to create a mask. We already know that our Australia shapefile contains a STATE column. We can use that to subset the shapefile and use the subset as a mask. Simple dplyr commands and %&gt;% work on sf features, which makes working with them so easy in R! vic&lt;-states_proj%&gt;%filter(STATENAME == &#39;Victoria&#39;)%&gt;%st_transform(crs = st_crs(dem)) # we also transform the crs to be sure they match plot(dem) plot(vic$geometry, add = T) To remove anything outside of Victoria, we use raster’s mask() and end up with the DEM for Victoria only: dem_vic&lt;-mask(dem, vic) plot(dem_vic) We can now also filter our koala dataset to only have koalas in Victoria to plot over our DEM. koala_vic&lt;-koala_sf%&gt;%filter(state == &#39;Victoria&#39;)%&gt;%st_transform(crs = st_crs(dem)) plot(dem_vic) plot(koala_vic$geometry, color = &#39;black&#39;, add = T) Often we are using rasters and spatial vector data together to extract data that is generally stored in raster form (such as elevation, mean temperatures or other topographic variables). For example if we are interested to see at which elevation our Koalas were observed in Victoria to make assumptions about their habitat preferences or temperature tolerances. If the CRS of both the raster and shapefile match, we can use extract() to add data from the raster to our spatial points as a new column: koala_vic$ele&lt;-raster::extract(dem_vic, koala_vic) summary(koala_vic$ele) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.62 67.95 185.34 218.54 349.87 648.06 hist(koala_vic$ele) We can see most observations were made below 200 meters. The median elevation for koalas in our dataset was 185. Again if we want to make nice and informative maps out of rasters, even in combination with spatial vector data, ggplot2 is the way to go! 8.6 Plotting rasters in ggplot2 Before we can plot a raster, we have to make it into a data.frame containing of columns X and Y as coordinates and a column containing the raster value of each pixel. Since rasters can be large and the resulting dataframes even larger, we first take a sample of the raster, which reduces the resolution: sample_raster&lt;-sampleRegular(dem_vic, size = 5e5, asRaster = TRUE) %&gt;% as.data.frame(xy = TRUE, na.rm = TRUE)%&gt;% setNames(c(&#39;x&#39;, &#39;y&#39;, &#39;layer&#39;)) Make sure to always use xy = TRUE, so that the coordinates are included in the new data.frame. We now have a format that ggplot can work with using geom_raster() ## x y layer ## 902 140.9675 -33.9925 19.73859 ## 1803 140.9675 -34.0025 48.89558 ## 1804 140.9775 -34.0025 25.70812 ## 2704 140.9675 -34.0125 53.28782 ## 2705 140.9775 -34.0125 27.96507 ## 2706 140.9875 -34.0125 20.13142 ggplot(data = sample_raster, aes ( x = x, y = y, fill = layer))+ geom_raster() Let’s make it look a bit prettier. The package viridis offers a few great color palettes for gradients of continuous variables: ggplot(data = sample_raster, aes ( x = x, y = y, fill = layer))+ geom_raster()+ scale_fill_viridis(option = &#39;A&#39;)+ labs(x = &#39;Longitude&#39;, y = &#39;Latitude&#39;, fill = &#39;Elevation&#39;)+ coord_sf(crs = 4326)+ theme_bw() We can plot additional layers on top of the raster by adding more geom_’s. That means we can plot rasters and spatial vectors together to make great maps for your papers and reports. I’m sure you can figure it out yourself for the next and final challenge! 8.6.1 Challenge 3 Add koala observations, color points by a categorical variable of your choice and change the color scheme. hint: to add different types of data, each data layer needs its own data source!: (geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+…) ggplot()+ geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+ geom_sf(data=koala_vic, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ scale_fill_viridis(option = &quot;D&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, fill = &quot;Elevation&quot;) + coord_sf(crs = 4326) + theme_bw() 8.7 Solutions to Challenges 8.7.1 Challenge 1 states_proj &lt;- st_transform(states, crs = st_crs(koala_sf)) plot(st_geometry(states_proj), axes = TRUE) plot(st_geometry(koala_sf), col = koala_sf$state, add = T) 8.7.2 Challenge 2 ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex))+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;)) 8.7.3 Challenge 3 ggplot()+ geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+ geom_sf(data=koala_vic, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ scale_fill_viridis(option = &quot;D&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, fill = &quot;Elevation&quot;) + coord_sf(crs = 4326) + theme_bw() "],
["spatial-tools-for-forestry-and-ecological-applications.html", "Chapter 9 Spatial tools for forestry and ecological applications 9.1 An introduction to processing LiDAR data in R 9.2 Working with LiDAR data in R 9.3 Step 1. Loading and visualizing pointclouds 9.4 Step 2. Computing a canopy height model 9.5 Step 3. Individual tree segmentation 9.6 Additional analysis: Computing a digital terrain model (DTM) 9.7 Wrapping up 9.8 Solutions to the challenges", " Chapter 9 Spatial tools for forestry and ecological applications 9.1 An introduction to processing LiDAR data in R NOTE: This tutorial is based on lidR function Syntax pre version 3.0. Older function names have been depreceated, but are still working, thus not affecting the workflow of this tutorial. Nevertheless, befor you start working on your own data, you might want to check for updated function names and new functionalites in lidR 3.0 here. 9.2 Working with LiDAR data in R lidR is a R package to process, manipulate and visualize LiDAR (and photogrammetry) pointclouds for forestry applications. Useful functionality includes the simple and memory-efficient plotting and visualization of data, computation of canopy height models (CHMs), tree segmentation and the extraction of tree metrics such as tree height and crown area. For more advanced users it additionally offers batch functionality, called cataloging, to process large LiDAR chunks, which otherwise would be very computation intensive. This tutorial is a quick guide to getting familiar with the main functionality of lidR and based on the excellent package documentation that can be found here. You can find the github page for lidR at https://github.com/Jean-Romain/lidR and the package documentation at https://cran.r-project.org/web/packages/lidR/lidR.pdf. We will be working with two example files (Example.las and drawno.laz), that you need to store in a data/ folder in your project (or working) directory. The files can be downloaded here. Please make sure that you have downloaded and stored the two files before beginning the tutorial. You also need to install the following packages: install.packages(c(&#39;lidR&#39;, &#39;raster&#39;, &#39;rgdal&#39;, &#39;dplyr&#39;, &#39;sf&#39;)) We will also need the package EBImage for our tree segmentation, which is distributed as part of the Bioconductor project. To install, run the following code: install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;EBImage&quot;) If this prompts you to update existing packages (Update all/some/none? [a/s/n]:) in the console, type n and hit enter. Once all packages are installed, we need to call them: library(raster) library(EBImage) library(lidR) library(sf) library(rgdal) library(dplyr) 9.3 Step 1. Loading and visualizing pointclouds For those familiar with R, loading and plotting pointcloud files will be easy: we are using simple read and plot syntax. lidR can load both .las and .laz file formats. Let’s read one of our example files and plot it: forest&lt;-readLAS(&#39;data/drawno.laz&#39;) Using print() will give us a bit of information about the file we have just loaded, for example on the number of points and point density. print(forest) ## class : LAS (v1.2 format 1) ## memory : 11.5 Mb ## extent : 278200, 278300, 602200, 602300 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## area : 9996.22 units² ## points : 150.4 thousand points ## density : 15.05 points/units² We can see that this particular file has ~150,000 points with a point density of ~15 per unit. We can also see that there is no coordinate reference system (crs) associated with this file, which is why the unit is unknown. This might lead to issues later on, so always check that your raw pointcloud has a crs assigned, before starting to work with it. For the purpose of this tutorial, we can overlook this. Let’s visualize our pointcloud! plot(forest) Figure 9.1: A new window will open and your plot will look something like this You can change the color of your point cloud using colorPalette =. Make sure to close the previous plot window before plotting a new one. Let’s try a few options: plot(forest, colorPalette = terrain.colors(50)) plot(forest, colorPalette = heat.colors(50)) 9.4 Step 2. Computing a canopy height model Now that we have imported our LiDAR data in to R, we can start working with it using the functionality of the package. We want to compute a canopy height model (CHM) from our two example files. CHMs are rasterized versions of the forests captured using LiDAR. Each pixel value in this raster describes the height above ground. These CHMs are useful to get an idea about the forest structure of the area we are looking at. A CHM is derived from the pointcloud by subtracting a digital terrain model (DTM) describing the ground topography, from a digital surface model (DSM) describing points of the pointcloud above the ground: Figure 9.2: The theory behind a CHM. Graphic: Colin Williams, NEON 9.4.1 Ground detection and normalizing a point cloud While we could now go ahead and calculate a DSM and DTM from our raw pointcloud, lidR has built-in functions to make this easier for us. First of all, we need to normalize our pointcloud, so that all ground points are set to 0. Since you might have noticed that our example forest is already normalized, we will load our other example and start using lasground() raw_forest&lt;-readLAS(&#39;data/Example.las&#39;) Do not mind the warnings that pop up. These are associated to the data structure but are not a problem in this case. Let’s have a brief look at what this piece of forest looks like: plot(raw_forest) You will see that the terrain here is a bit undulated. To normalize ground elevation, we first need detect ground points in this raw data and save it as a new pointcloud: raw_forest_ground&lt;-lasground(raw_forest, csf()) csf() is one of two ground segmentation algorithms, built into lidR. If you want more detail on what it exactly does, call ?csf in the console. We can visualize what this function has done with the raw data by plotting it and coloring it based on the classification we just ran: plot(raw_forest_ground, color = &quot;Classification&quot;) Detected ground points are illustrated in blue, our trees in grey. You can check the same for our other example to confirm that this pointlcoud is already classified as we expected: plot(forest, color = &quot;Classification&quot;) Now that the points are detected, we can normalize our pointcloud, which sets all ground points to an elevation of 0. raw_forest_norm&lt;-lasnormalize(raw_forest_ground, tin()) Again tin() is the spatial interpolation algorithm at work here. We can see what has happened by plotting our raw and normalized data and comparing the point clouds: plot(raw_forest) plot(raw_forest_norm) Figure 9.3: We can see that the right, raw data exhibits ground topography, while the right, normalized pointcloud is completely flat We need the point cloud to be completely flat, to get the exact height of each tree above ground, without the influence topography might have on tree height. Now we are ready to start working on a CHM! 9.4.2 Building the initial CHM To compute a CHM, we simply apply the grid_canopy() function to the normalized pointcloud, but need to give two extra arguments: chm1 &lt;- grid_canopy(raw_forest_norm, 0.5, p2r()) The first argument is the desired resolution of the output, in our case 0.5 meters. The second argument is the algorithm to compute the CHM. lidR has multiple algorithms built in, here we are using p2r() which attributes the height of the highest point found for each pixel of the output raster. Let’s look at our result: plot(chm1, col = height.colors(50)) Hmm, that doesn’t look too great yet. A lot of white space (empty pixels) indicates that we have pits resulting from the relatively low point density of this point cloud. These will lead to a lot of NA data later on, messing with our analyses, so we should aim at closing these pits by playing around with the arguments within the grid_canopy() function. A simple improvement proposed by Martin Isenburg in lastools (the underlying software to lidR) can be obtained by replacing each LiDAR return with a small disk. Since a laser has a width and a footprint, this tweak may simulate this fact. We add a subcircle of 0.2 diameter in our p2r() algorithm and can see what happens. chm2 &lt;- grid_canopy(raw_forest_norm, 0.5, p2r(0.2)) plot(chm2, col = height.colors(50)) That looks much better already, but some pits still persist. Next we can try reducing the resolution a bit to counter these remaining empty pixels: chm3 &lt;- grid_canopy(raw_forest_norm, 0.75, p2r(0.2)) plot(chm3, col = height.colors(50)) More pits closed, great! We might loose some detail here, as the pixel size is now larger, but we gain detail from replacing NA’s with height values. Note that playing around with these two parameters might take a while but eventually you may find the right approach that fits your data. Another popular approach to avoid the empty pixel problem consists of the interpolation of first returns with a triangulated irregular network (TIN) and then rasterizing it onto a grid to create the CHM. For this we need to use the algorithm dstmin() in grid_canopy(): chm4 &lt;- grid_canopy(raw_forest_norm, 0.5, dsmtin()) plot(chm4, col = height.colors(50)) As explained by Martin Isenburg, the result has no more empty pixels but is full of pits because many laser pulses manage to deeply penetrate the canopy before producing the first return. In the end you might need to apply multiple algorithms and functions to reach the best results. Here is just an example of how this might look like, using multiple options to get a decent result for this pointcloud: algo &lt;- pitfree(c(0,2,5,10,15), c(0,1), subcircle = 0.2) chm5 &lt;- grid_canopy(raw_forest_norm, 0.5, algo) If you are interested in what you can do to improve your CHM, try ?grid_canopy and have a look at the options available. Once we are happy with our CHM, we can have a look at some simple height stats by running summary() for this chunk of forest: summary(chm5) ## Z ## Min. -0.14362 ## 1st Qu. 0.00000 ## Median 10.24074 ## 3rd Qu. 21.64095 ## Max. 52.24510 ## NA&#39;s 1808.00000 Let’s test our knowledge and create a CHM for the other example file! 9.4.3 Challenge 1 Produce a CHM for our other example file with as little pits as possible using the functions and algorithm options described above. Keep in mind the initial resolution and point density and play around with the functions having these in mind. Remember that this pointcloud is already ground normalized, so your might need to adapt the code a bit… Now that is a good looking CHM. Let’s get to using our CHM to segment individual trees as a basis of extracting tree metrics, without having ever to set foot into this forest! 9.5 Step 3. Individual tree segmentation This step requires us to use a pitfree and smooth canopy height model. Smoothing is a post-process technique using the package raster that aims at removing sharp edges and pixels from the CHM by calculating focal (“moving window”) values for the neighborhood of focal cells using a matrix of weights. Let’s do this in one step to get our tree segmentation on the way: # create a new chm to work with algo &lt;- pitfree(thresholds = c(0,10,20,30,40,50), subcircle = 0.2) chm &lt;- grid_canopy(raw_forest_norm, 0.5, algo) # smoothing post-process (here e.g. two pass (running focal twice), 3x3 median convolution) ker &lt;- matrix(1,3,3) chm &lt;- focal(chm, w = ker, fun = median) chm &lt;- focal(chm, w = ker, fun = median) # check the smoothed raster plot(chm, col = height.colors(50)) Comparing this raster to our previous ones, we can see that the ‘fuzziness’ around the edged has been removed. This will aid the next algorithm in finding the trees better. The segmentation using a CHM and normalized pointcloud is implemented in lidR using lastrees(). We will be using a watershed algorithm with a 4 meter threshold. A watershed transformation treats our raster like a topographic map and finds the lines in ‘depressions’ between our trees, treating the tree tops or crowns as ‘ridges’. # segment the trees algo &lt;- lidR::watershed(chm, th = 4) las &lt;- lastrees(raw_forest_norm, algo) # remove points that are not assigned to a tree trees &lt;- lasfilter(las, !is.na(treeID)) Let’s have a look at the now segmented pointcloud, which we called trees: plot(trees, color = &quot;treeID&quot;, colorPalette = pastel.colors(100)) Figure 9.4: Pretty, isn’t it? Each tree got a color assigned to it! To check how well our tree segmentation performed, we can additional detect the position and center of each tree using tree_detection() and plot these on top of each other for quality control. To extract further structural metrics from our forest, we will need these treetops anyway. treetops&lt;-tree_detection(raw_forest_norm, lmf(ws = 5, hmin = 10)) plot(chm) plot(treetops, add = T) The arguments that go into the lmf() algorithm describe 1. the length or diameter of the moving window used to detect the local maxima (ws), which corresponds to the crown width. This metric would need to be confirmed by doing some field measurements. And 2. we are using hmin to define the minimum height of a tree in this forest, which we’d also need to ground truth before running this. This is the threshold below which a pixel or a point cannot be a local maxima (and thus a treetop). For this one, we assumed hmin to be 10 meters. If you don’t have ground measurements to tune your algorithm, you need to try a few different values until you’re happy with the results. We can even plot and evaluate our results in 3D: plot(trees, color = &#39;treeID&#39;, colorPalette = pastel.colors(100)) %&gt;% add_treetops3d(treetops) Figure 9.5: The treetops as red dots on top of our segmented pointcloud. To make sure that these results are saved, we should write all these files from the R environment to our hard drive. That will also allow us to e.g. further work with them in a GIS or in a pointcloud viewer such as CloudCompare. We will create a folder for outputs first, then save our smooth CHM as a raster file (.tif), our normalized pointcloud as a LiDAR file (.las) and the treetops as a shapefile (.shp): dir.create(&#39;outputs&#39;) writeRaster(chm, &#39;outputs/smooth_chm.tif&#39;) writeLAS(raw_forest_norm, &#39;outputs/normalized.las&#39;) writeOGR(treetops, &#39;.&#39;, &#39;outputs/trees.shp&#39;, driver=&quot;ESRI Shapefile&quot;) We can now go on and compute some metrics from our segmented trees. We are today especially interested in extracting crown area and height for each of our segmented trees. First we need to derive a polygon for each crown that was segmented. For this lidR offers tree_metrics() and tree_hulls(), which work on the segmented pointcloud we derived earlier. We can then use dplyr to simply join both together to get a polygon shapefile containing crown area and height: metric &lt;- tree_metrics(trees, .stdtreemetrics) hulls &lt;- tree_hulls(trees) hulls@data &lt;- dplyr::left_join(hulls@data, metric@data) ## Joining, by = &quot;treeID&quot; spplot(hulls, &quot;Z&quot;) spplot from raster allows us to plot these hulls based on Z, which is the tree height for each of the crowns we identified. We can also plot all these together on our CHM to visually check accuracy: plot(chm) plot(hulls, add = T) plot(treetops, add = T) We can see some of our crowns and tree positions don’t seem quite right, so there might be some room for improvement. But always know that using pointclouds and CHMs is just an approximation of the reality by applying algorithms and models, trying to simulate the measured forest as close as possible. So we will never detect 100% of the trees and we will never delineate them perfectly. Nevertheless, using pointclouds, we will be able to measure thousands of trees in a very brief period of time, while when carrying out forest inventories we can only measure a few dozen per plot, requiring lots of hard labor. A general work flow using ground inventories and LiDAR data together would this be to collect an amount of data needed to get an idea of the structure of your forest on the plot level and tune your LiDAR algorithms by measuring actual trees. Then you can extrapolate your field measurements to a whole forest or larger landscape using the pointclouds collected outside your plot area. We want to get data on crown area and tree height together in a comprehensive table for further analysis. The rasterpackage will help us calculating the area and converting our polygon shapefile to a simple feature using sf will make it easier to work with the data. hulls$area&lt;-area(hulls) tree_metrics&lt;-st_as_sf(hulls) sf treats a shapefile’s attribute table like a data.frame and as we can see, the tree_metrics simple feature already contains everything we need: head(tree_metrics) ## Simple feature collection with 6 features and 8 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 488041.5 ymin: 5189194 xmax: 488051.9 ymax: 5189222 ## CRS: +proj=utm +zone=12 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0 ## treeID XTOP YTOP ZTOP Z npoints convhull_area area ## 1 332 488046.6 5189194 22.471 22.471 232 41.959 41.95850 ## 2 347 488042.0 5189202 22.740 22.740 115 24.596 24.59595 ## 3 215 488048.3 5189210 24.805 24.805 156 40.740 40.73950 ## 4 351 488041.8 5189208 20.266 20.266 90 21.656 21.65552 ## 5 271 488048.1 5189203 22.861 22.861 142 35.895 35.89526 ## 6 353 488042.2 5189220 20.852 20.852 49 10.103 10.10254 ## geometry ## 1 POLYGON ((488051.2 5189196,... ## 2 POLYGON ((488045.7 5189201,... ## 3 POLYGON ((488051.3 5189209,... ## 4 POLYGON ((488045.5 5189207,... ## 5 POLYGON ((488050.8 5189202,... ## 6 POLYGON ((488044.5 5189218,... Z is the height of each tree, identified by treeID and area is the exact area of each tree crown. You may see that convhull_area was already calculated by tree_metrics and corresponds to the area, so we could also use that variable instead and not add a new column. Let’s convert this from a sf to a table and remove all unnecessary information by selecting only the variables we are interested in: tree_table&lt;-as.data.frame(tree_metrics)%&gt;%select(&#39;treeID&#39;, &#39;Z&#39;, &#39;area&#39;) head(tree_table) ## treeID Z area ## 1 332 22.471 41.95850 ## 2 347 22.740 24.59595 ## 3 215 24.805 40.73950 ## 4 351 20.266 21.65552 ## 5 271 22.861 35.89526 ## 6 353 20.852 10.10254 We can now also save the polygon shapefile of our crowns, to have everything ready for GIS processing or inspection. Additionally we want to save our metrics table as a .csv. st_write(tree_metrics, &#39;outputs/trees_sf.shp&#39;) write.csv(tree_table, &#39;outputs/tree_table.csv&#39;) 9.5.1 Delineating the crowns Now the hulls are great, but for some applications we might actually need to know the exact shape of each tree crown, rather than its approximate hull, for example when studying crown cover or leaf area index (LAI). We can delineate crowns in more detail working directly on the CHM, without the need of using the pointcloud any longer, after we derived the canopy height model. We can apply watershed() directly to the CHM: crowns &lt;- lidR::watershed(chm, th = 4)() plot(crowns, col = pastel.colors(100)) Now that we have a new raster called crowns where each tree is its own object, we can polygonize it to extract the contours and thus the more precise tree crowns (Note that this might run for a while as it is quite computing intensive): contour &lt;- rasterToPolygons(crowns, dissolve = TRUE) plot(chm, col = height.colors(50)) plot(contour, add = T) plot(treetops, add = T) Before we save this, we need to make sure all our data has the same spatial projection. We can do so using crs(). crs(chm) ## CRS arguments: ## +proj=utm +zone=12 +datum=NAD83 +units=m +no_defs +ellps=GRS80 ## +towgs84=0,0,0 crs(contour) ## CRS arguments: NA Unfortunately rasterToPolygons() does not assign a coordinate reference system, so we have to do so ourselves. While we’re at it, let’s also transform this to the sf format: crs(contour)&lt;-crs(chm) contour&lt;-st_as_sf(contour)%&gt;%rename(treeID = layer) Next we can save the delineated polygons: st_write(contour, &#39;outputs/contour_sf.shp&#39;) Again we want to test our knowledge and apply what we’ve just learned to the other example file. 9.5.2 Challenge 2 Detect trees and treetops in our other example file based on the derived CHM and normalized pointcloud and plot smoothed CHM, hulls and detected treetops on top of one another 9.6 Additional analysis: Computing a digital terrain model (DTM) For many applications and studies, it is important to have detailed topographic data to e.g. assess slope, aspect or potential for water runoff in the ecosystem you’re studying. As LiDAR data penetrates through the canopy and captures laser returns from the ground as well, we can use the data not only to look at above ground structure, but also derive very high resolution digital terrain models (DTMs) that may be helpful in our analysis. Do to so, we are using the classified pointcloud that has ground points detected (which we named raw_forest_ground). grid_terrain() interpolates the ground points and creates a rasterized digital terrain model. The algorithm uses the points classified as “ground” to compute the interpolation. dtm &lt;- grid_terrain(raw_forest_ground, 1, algorithm = kriging(k = 10L)) ## Registered S3 method overwritten by &#39;xts&#39;: ## method from ## as.zoo.xts zoo plot(dtm) summary(dtm) ## Z ## Min. 312.7800 ## 1st Qu. 324.2830 ## Median 329.7095 ## 3rd Qu. 334.8963 ## Max. 347.8690 ## NA&#39;s 8.0000 What you can see when plotting is the topography of the ground under the trees in a unit of meters above sea level (which is derived from the coordinates of each point). We can see that our elevation varies between 313 to 348. The second argument of the function is again the resolution, which we set to 1 (1x1 m). This is an amazingly high resolution, when you think that the next best (free) DTM available from the Shuttle Radar Topography Mission is 30x30 meters. Very high resolution DTMs can be used for detailed analyses of stream flow in hydrological applications, geomorphology or modeling soil wetness with Cartographic Depth to Water Indexes (DTW-index). We are using another algorithm here (kriging) that applies a KNN (k-nearest neighbor) approach, commonly used in DTM creation. We can plot our crowns on top to see where the trees are distributed and how topography might be influencing e.g. the height or crown shape of trees here: plot(dtm) plot(contour$geometry, add = T) We can save our dtm the same way we saved our CHM before, using writeRaster(). lidR has some integrated LiDAR pointclouds to play around with, that might help illustrate this a bit better. Let’s look at one that has more depressions in the form of lakes, using a different algorithm as illustration: LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Topography.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) dtm1 &lt;- grid_terrain(las, algorithm = knnidw(k = 6L, p = 2)) plot(dtm1) lidR also allows to plot DTMs in 3D: plot_dtm3d(dtm1) Figure 9.6: The example data dtm in 3D Another integrated dataset can be called liked this. Feel free to use these to practice creating CHMs, segmenting trees, extracting tree metrics and calculating a DTM. LASfile2 &lt;- system.file(&quot;extdata&quot;, &quot;Megaplot.laz&quot;, package=&quot;lidR&quot;) las2 &lt;- readLAS(LASfile) Let’s try this on the other example too! 9.6.1 Challenge 3 Create a digital terrain model for the other example (forest) and plot the detected treetops on top of the final DTM. Finally, save your DTM as a .tif file to your outputs folder! 9.7 Wrapping up Good job everyone. You have learned how to get your LiDAR data into R and prepare it for further analysis by detecting ground points and normalizing the clouds. Doing that, we were able to derive a canopy height model and learned about how to improve it’s accuracy. Having derived that, we were able to extract metrics for each tree and delineate our crowns to look at single-tree attributes within our example forest. You were also able to compile your inventory data as a comprehensive table and save everything for further processing. Additionally we derived a very detailed digital terrain model. What follows now can be manifold, Maybe you are interested in the exact location and elevation above sea level of all your trees detected. raster allows you to easily extract data from a raster using a spatial vector such as a shape file by simply using extract: elevation&lt;-extract(dtm, treetops, cellnumbers = T, df=T)%&gt;%rename(treeID = ID, elevation = Z) elevation&lt;-cbind(elevation,coordinates(dtm)[elevation[,2],]) elevation&lt;-elevation%&gt;%select(-2) head(elevation) ## treeID elevation x y ## 1 1 335.211 488042.5 5189193 ## 2 2 335.021 488046.5 5189195 ## 3 3 335.255 488042.5 5189203 ## 4 4 334.875 488040.5 5189208 ## 5 5 334.401 488043.5 5189210 ## 6 6 333.578 488044.5 5189222 You could think of other uses, e.g. if you have raster layers describing climatic variables of your forest, you could get precise data for each of your trees and see how it might have affected their growth. You can also use the delineated crowns to extract all pixel data from e.g. multispectral imagery of the crown area of your trees to try and model forest nutrition to make assumptions on the quality of the forest for certain arboreal species. If you have any questions or comments on this tutorial, feel free to write me an email and I’m happy to try and help out. I have a few other (more basic) tutorials compiled like this one if you are interested. Good luck with any future LiDAR adventures in R! 9.8 Solutions to the challenges 9.8.1 Challenge 1 forest_norm &lt;- lasnormalize(forest, tin()) forest_chm &lt;- grid_canopy(forest_norm, 0.25, pitfree(c(0,2,5,10,15), c(0,1), subcircle = 0.2)) 9.8.2 Challenge 2 #data to use: #forest_chm #forest_norm # segment the trees algo &lt;- lidR::watershed(forest_chm, th = 4) las2 &lt;- lastrees(forest_norm, algo) trees_forest &lt;- lasfilter(las2, !is.na(treeID)) plot(trees_forest, color = &quot;treeID&quot;, colorPalette = pastel.colors(100)) # find treetops and hulls treetops_forest&lt;-tree_detection(forest_norm, lmf(ws = 5, hmin = 10)) metric_forest &lt;- tree_metrics(trees_forest, .stdtreemetrics) hulls_forest &lt;- tree_hulls(trees_forest) hulls_forest@data &lt;- dplyr::left_join(hulls@data, metric@data) plot(forest_chm) plot(hulls_forest, add = T) plot(treetops_forest, add = T) 9.8.3 Challenge 3 dtm2 &lt;- grid_terrain(forest, 1, algorithm = kriging(k = 10L)) writeRaster(dtm2, &#39;outputs/forest_DTM.tif&#39;) plot(dtm2) plot(treetops_forest, add = T) "],
["extracting-raster-fractions-and-area-using-a-polygon-mask.html", "Chapter 10 Extracting raster fractions and area using a polygon mask 10.1 Problem statement 10.2 Creating some dummy raster data 10.3 Create some reserves 10.4 Extracting data 10.5 Plotting the results", " Chapter 10 Extracting raster fractions and area using a polygon mask 10.1 Problem statement For this tutorial you will need to load the following packages. Please install if you don’t have them yet. library(raster) library(sf) library(exactextractr) library(tidyverse) Say you have a raster of habitat suitability for a certain species. You could now be interested, how much suitable habitat lies within certain defined areas. For example we could check weather the species is adequately protected by extracting how much suitable area lies within National Parks or other reserves Additionally we can test, how much suitable (and unsuitable) area lies outside the areas you want to test to have an idea of the potential for further protection, or the danger to the species from e.g. land clearing outside of parks. In the end we additionally want to know which fraction of the total area both suitable and unsuitable habitat occupies with our parks, reserves and outside ares. 10.2 Creating some dummy raster data First let’s get a raster to play around with: x &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x)&lt;-100 #make the resolution 100 x 100 meter, so one cell is 1 ha in size values(x)&lt;-runif(ncell(x)) #populate the raster with random values between 0 - 1 plot(x) This raster now contains makeshift values of habitat suitability on a percentage scale. Let’s say we found our cutoff point for suitable habitat at 0.5. Any values below this cutoff are then defined as unsuitable for our target species. We can reclassify our raster accordingly: x_re&lt;-reclassify(x, c(-Inf,0.5,0, 0.5,Inf,1)) plot(x_re) 0 now means non suitable and 1 suitable habitat. Since we randomized our values using runif() we have a equal distribution, meaning that 50% of the area is suitable and 50% of the area is unsuitable. This will help us in double-checking our results later but will probably never occur in real life examples. hist(x) barplot(x_re, col = c(&#39;darkred&#39;,&#39;darkgreen&#39;)) If we wanted to get different probabilities, we could also create our classified raster like this: x_re2 &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x_re2)&lt;-100 values(x_re2)&lt;-sample(0:1, ncell(x_re), replace = T, prob = c(80,20)) plot(x_re2) Using sample() with prob = c(80,20), assigns 80% of the area as unsuitable, but still on a normal pattern. 10.3 Create some reserves We also needs some reserve areas to extract values from. extent(x_re) #have a look at the extent of our raster to decide for extent of dummy areas ## class : Extent ## xmin : -10000 ## xmax : 10000 ## ymin : -10000 ## ymax : 10000 #create some extents a&lt;-extent(c(-1000, 1000, -5233, -2355)) b&lt;-extent(c(-7000, 6530, -400, 4223)) c&lt;-extent(c(-10000, -5427, -10000, -4785)) #transform them into polygons a_sf&lt;-as(a, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res1&#39;) b_sf&lt;-as(b, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res2&#39;) c_sf&lt;-as(c, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res3&#39;) #get the total area to create our outside areas all&lt;-as(extent(x_re), &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;outside&#39;) plot(x_re) plot(a_sf$geometry, add = T, lwd = 2, border = &#39;red&#39;) plot(b_sf$geometry, add = T, lwd = 2, border = &#39;blue&#39;) plot(c_sf$geometry, add = T, lwd = 2, border = &#39;orange&#39;) Now we have three areas that we assume to be our reserves. We still need a polygon for all other areas, also we need to combine our polygons into one. #bind reserves reserves&lt;-rbind(a_sf, b_sf, c_sf) #get the outside areas only outside&lt;-st_difference(all, st_combine(reserves)) #bind all for extraction all_areas&lt;-rbind(reserves, outside) plot(x_re) plot(st_geometry(outside$geometry), border = &#39;blue&#39;, lwd = 4, add = T) plot(st_geometry(reserves), border = as.factor(reserves$name), lwd = 3, add =T) The blue area is all raster area outside of our reserves. Our three reserves are colored in red, black and green. 10.4 Extracting data To extract the cell data from each polygon, we use exact_extract from the package exactextractr, which is a quicker alternative to raster::extract. It extracts data from each feature separately in parallel and saves the extracted values into a list. extract&lt;-exact_extract(x_re,all_areas, fun = NULL) ## | | | 0% | |================ | 25% | |================================ | 50% | |================================================= | 75% | |=================================================================| 100% for( i in seq_along(extract)){ extract[[i]]$ID &lt;- seq_along(extract)[i] } In the second step, we assign an ID column to each table in the created list. This will help us in matching the extracted values to the name of the area polygon we extracted from. We convert out list into a table and then add the lc column to assign each value the name of the polygon it belongs to: extract_table&lt;-dplyr::bind_rows(extract)%&gt;%dplyr::select(-2) extract_table$lc &lt;- as.factor(all_areas$name[extract_table$ID]) head(extract_table) ## value ID lc ## 1 1 1 res1 ## 2 0 1 res1 ## 3 0 1 res1 ## 4 0 1 res1 ## 5 0 1 res1 ## 6 0 1 res1 We can now compile a summary table using dplyr syntax. To calculate the area, we are working with the resolution of the raster and will need to save it as a variable fist. reso&lt;-res(x_re)[1] area_habitat&lt;-extract_table%&gt;%group_by(lc, value)%&gt;% summarise(pixelsum = sum(ID), areaha = (pixelsum*reso^2)/10000)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;%ungroup()%&gt;% mutate(sum_all = sum(pixelsum), pertotal = 100*pixelsum/sum_all) area_habitat ## # A tibble: 8 x 8 ## lc value pixelsum areaha sumA per sum_all pertotal ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 outside 0 61276 61276 123560 49.6 144258 42.5 ## 2 outside 1 62284 62284 123560 50.4 144258 43.2 ## 3 res1 0 299 299 600 49.8 144258 0.207 ## 4 res1 1 301 301 600 50.2 144258 0.209 ## 5 res2 0 6342 6342 12784 49.6 144258 4.40 ## 6 res2 1 6442 6442 12784 50.4 144258 4.47 ## 7 res3 0 3690 3690 7314 50.5 144258 2.56 ## 8 res3 1 3624 3624 7314 49.5 144258 2.51 What happens here is, we group by class (lc which refers to our polygon names) as well as value (0 for unsuitable and 1 for suitable habitat). We then tally all pixels using the ID column and calculate the area in ha from the pixelsum. In our case these match because the resolution is already 1 ha (each cell is 100x100 meters). Furthermore we can then tally all pixels to get the total raster area and from that calculate first the percentage of suitable and unsuitable habitat in each polygon and then (by using ungroup()) the fraction of this area compared to the total raster area. 10.5 Plotting the results Using ggplot2 we can then plot these results to better visualize them. The code below will work for any table that is in the format of our area_habitat table above, if you follow the steps of this guide. bar&lt;-ggplot(data = area_habitat, aes(x = reorder(as.factor(lc), -pixelsum), y = per, fill = as.factor(value)))+ geom_bar(stat = &#39;identity&#39;, color = &#39;black&#39;, position=position_fill(), show.legend = FALSE)+ scale_y_continuous(labels = scales::percent)+ geom_text(aes(label=round(per, digits = 1)), check_overlap = TRUE, position=position_fill(), vjust = 1.2, color = &#39;black&#39;)+ theme_classic()+ theme(axis.title.y = element_text(size = 15, face = &#39;bold&#39;), axis.title.x = element_text(size = 15, face = &#39;bold&#39;), axis.text.x = element_text(size = 15, face = &#39;bold&#39;), axis.text.y = element_text(size = 15, face = &#39;bold&#39;))+ ggtitle(&#39;Whatever this area is&#39;)+ xlab(&#39;&#39;)+ ylab(&#39;area percentage&#39;)+ scale_x_discrete(labels=c(&#39;Outside&#39;, &#39;Reserve 2&#39;, &#39;Reserve 3&#39;, &#39;Reserve 1&#39;))+ scale_fill_manual(values = c(&#39;lightgrey&#39;, &#39;darkgreen&#39;), labels = c(&#39;unsuitable&#39;, &#39;suitable&#39;), name = &#39;&#39;) bar Our plot shows for each area we wanted to test, which fraction is suitable and which unsuitable. As expected, in our example we have about a 50:50 distribution, but this may look quite different in a real-world example. Hope you can find a useful application for this code! :) "],
["calculating-class-area-from-raster.html", "Chapter 11 Calculating class area from raster 11.1 Simple area and area fraction calculations on a classified raster", " Chapter 11 Calculating class area from raster 11.1 Simple area and area fraction calculations on a classified raster For this tutorial you will need to load the following packages: library(raster) library(sf) library(tidyverse) You will often come across the task of extracting the area each of your raster classes occupies within your entire raster area. There is a simple way of doing this using raster and dplyr. The output will be a comprehensive table. First let’s create some dummy data to work with: 11.1.1 Creating dummy data x &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x)&lt;-100 # we are using a resolution of 100 x 100 so that each pixel is 1ha in size #populate the raster with values values(x)&lt;-base::sample(5, ncell(x), replace = T, prob = c(10,30,20,5,35)) plot(x) Our raster now contains 5 classes that could e.g. be land-use types such as forest, infrastructure or pasture. We additionally gave each class a probability of occurrence so that we can double check of our calculated areas are correct. In total, our raster x has 410^{4} cells. 11.1.2 Preparing the raster for area calculation To get area metrics, we need to transform the raster into a data frame: rast_df&lt;-x%&gt;%as.data.frame(xy = T, na.rm = T) To count each pixel, we can assign an extra column to this dataframe with an ID 1 to be able to tally all cells. Additionally, we are extracting the resolution of our raster as a variable to our environment to later calculate the area. rast_df$ID&lt;-1 reso&lt;-res(x)[1] head(rast_df) ## x y layer ID ## 1 -9950 9950 1 1 ## 2 -9850 9950 2 1 ## 3 -9750 9950 5 1 ## 4 -9650 9950 5 1 ## 5 -9550 9950 5 1 ## 6 -9450 9950 2 1 The layer column contains the class each pixel was assigned to. This will be universal for any raster you put in. ID is the same for each row, this is only needed in the next step. 11.1.3 Compiling a comprehensive table area&lt;-rast_df%&gt;%group_by(layer)%&gt;% summarise(pixelsum = sum(ID), area_ha = (pixelsum*reso^2)/10000)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;% rename(class = layer) area ## # A tibble: 5 x 5 ## class pixelsum area_ha sumA per ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3954 3954 40000 9.88 ## 2 2 11873 11873 40000 29.7 ## 3 3 8046 8046 40000 20.1 ## 4 4 2011 2011 40000 5.03 ## 5 5 14116 14116 40000 35.3 And there we go. Each class has it’s pixelsum calculated, then using the sum we can calculate the area in ha (or else, here you can alternate the code). In this case the pixel sum matches our area_ha because one pixel is already of size 1 ha. We can change the code to e.g. calculate area_km2. area_km2&lt;-rast_df%&gt;%group_by(layer)%&gt;% summarise(pixelsum = sum(ID), area_km2 = pixelsum/100)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;% rename(class = layer) area_km2 ## # A tibble: 5 x 5 ## class pixelsum area_km2 sumA per ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3954 39.5 40000 9.88 ## 2 2 11873 119. 40000 29.7 ## 3 3 8046 80.5 40000 20.1 ## 4 4 2011 20.1 40000 5.03 ## 5 5 14116 141. 40000 35.3 Next we calculate the sum of all pixels (sumA) using mutate() to get the total raster area. This should in this case be the same as ncell(x) (40000). To derive the percentage of the entire each class occupies, we just need to divide the pixelsum of each class by the total sum and multiply by 100. This should match our probabilities we assigned for each class when filling the raster with values: area_km2$per == summary(as.factor(values(x)))/400 ## 1 2 3 4 5 ## TRUE TRUE TRUE TRUE TRUE Enjoy trying it out on your own raster with some actual class ares! "]
]
