[
["index.html", "Intro to R for Forest Ecosystem Science Chapter 1 Introduction 1.1 Disclaimer", " Intro to R for Forest Ecosystem Science Anu Singh, Benjamin Wagner, and Kaitlyn Hammond Chapter 1 Introduction This is a (hopefully) continuously increasing collection of R tutorials and code collections relevant for Forest Ecosystem Sciences, curated by three R enthusiasts at the School of Ecosystem and Forest Sciences - Uni Melbourne. Nevertheless, most tutorials can be useful for anyone starting out in R or looking to broaden their skills and learn something new, that we had to learn ourselves. We currently cover: Introductions to using the tidyverse packages dplyr and ggplot2, for easy data manipulation and visualization An Introduction to using spatial data in R with both the sf and raster packages. Introduction to basic linear and generalized linear modeling in R A tutorial on functions and loops in R and the use of lapply and mapply A tutorial on using the package lidR to process pointclouds from LiDAR data and photogrammetry Code chunks for area calculations of rasters and fractions in classified rasters more is coming … watch this space! 1.1 Disclaimer If you enjoy our tutorials and code snippets, feel free to use and share this book and code compiled in it for the best purposes you can think of. Nevertheless, while doing so, please acknowledge our work and share the link to the source of this book or the github page at: https://github.com/BennyWag/bookdown-playaround. "],
["dplyr-a-brief-introduction-to-tidy-data-manipulation.html", "Chapter 2 dplyr - A brief introduction to tidy data manipulation 2.1 Introducing the koala dataset 2.2 Working with dplyr", " Chapter 2 dplyr - A brief introduction to tidy data manipulation 2.1 Introducing the koala dataset Today we will have a look at some koala data. You can download the dataset for this tutorial here. Futhermore you need to install the tidyverse package, which contains dplyr. install.packages(&#39;tidyverse&#39;) First we need to load the dataset we’re working with. koala&lt;-read.csv(&#39;data/koala.csv&#39;) It should contain the following colums: names(koala) ## [1] &quot;species&quot; &quot;X&quot; &quot;Y&quot; &quot;state&quot; &quot;region&quot; &quot;sex&quot; &quot;weight&quot; ## [8] &quot;size&quot; &quot;fur&quot; &quot;tail&quot; &quot;age&quot; &quot;color&quot; &quot;joey&quot; &quot;behav&quot; ## [15] &quot;obs&quot; Lets look at a structure and summary of this dataset: str(koala) ## &#39;data.frame&#39;: 242 obs. of 15 variables: ## $ species: Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num 153 148 153 153 153 ... ## $ Y : num -27.5 -22.5 -27.5 -27.5 -27.5 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num 70.8 70.4 68.7 73 65.2 ... ## $ fur : num 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... summary(koala) ## species X Y ## Phascolarctos cinereus:242 Min. :138.6 Min. :-39.00 ## 1st Qu.:150.0 1st Qu.:-34.49 ## Median :152.0 Median :-32.67 ## Mean :150.3 Mean :-32.36 ## 3rd Qu.:152.9 3rd Qu.:-30.31 ## Max. :153.6 Max. :-21.39 ## state region sex weight ## New South Wales:181 northern:165 female:127 Min. : 5.406 ## Queensland : 16 southern: 77 male :115 1st Qu.: 6.574 ## South Australia: 14 Median : 7.277 ## Victoria : 31 Mean : 7.923 ## 3rd Qu.: 8.765 ## Max. :17.889 ## size fur tail age ## Min. :64.81 Min. :1.110 Min. :1.004 Min. : 1.00 ## 1st Qu.:68.43 1st Qu.:2.410 1st Qu.:1.272 1st Qu.: 3.00 ## Median :70.27 Median :2.797 Median :1.534 Median : 7.00 ## Mean :70.94 Mean :2.896 Mean :1.507 Mean : 6.43 ## 3rd Qu.:72.33 3rd Qu.:3.217 3rd Qu.:1.750 3rd Qu.: 9.00 ## Max. :81.91 Max. :5.876 Max. :1.981 Max. :12.00 ## color joey behav obs ## chocolate brown:21 No :185 Feeding : 48 Opportunistic:65 ## dark grey :36 Yes: 57 Just Chillin: 67 Spotlighting :94 ## grey :69 Sleeping :127 Stagwatching :83 ## grey-brown :53 ## light brown :20 ## light grey :43 We can see that our dataset contains the positions of each koala in Latitude and Longitude (X and Y) as well as variables describing their physiology, behavior and how they were recorded. This is typical presence-only wildlife data, combining observations with some data describing each individual, which could e.g. be used for distribution modeling or to test influences of other variables such as climate on behavior and physiology of this particular speces. Often in these types of studies, we are not interested in all the recorded variables and thus first need to ‘clean’ our data to make it easier to work with it. dplyr is a package desinged to make data ‘cleaning’ and manipulation of large datasets easier by introducing specific syntax. Let’s see how it works and compares to base R functionality! 2.2 Working with dplyr Base R subsetting can be very tedious. Imagine we want to got the mean age for our koalas, but split it by sex. Getting one mean is easy: mean(koala[koala$sex == &#39;male&#39;, &quot;age&quot;],na.rm = TRUE) ## [1] 6.626087 Summarizing both sexes and savinf it in a table takes a few lines of code: female_mean&lt;-mean(koala[koala$sex == &#39;female&#39;, &quot;age&quot;],na.rm = TRUE) male_mean&lt;-mean(koala[koala$sex == &#39;male&#39;, &quot;age&quot;],na.rm = TRUE) means&lt;-rbind(c(female_mean, male_mean)) means&lt;-as.data.frame(means) names(means)&lt;-c(&#39;female&#39;, &#39;male&#39;) Lets have a look at the results. means ## female male ## 1 6.251969 6.626087 That’s all good, but with that many lines of code quite error prone … dplyr makes data manipulation simpler. For this example, we would only require one line of code! library(dplyr) mean_age_koala&lt;-koala%&gt;%group_by(sex)%&gt;%summarise(mean_age = mean(age)) ## # A tibble: 2 x 2 ## sex mean_age ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 6.25 ## 2 male 6.63 So simple, and looks even better than our base R table too! The main functions we will explore here are dplyr’s pipe %&gt;%, select(), filter(), group_by(), summarise() and mutate(). 2.2.1 select() and dplyr’s pipe If, for example, we wanted to move forward with only a few of the variables in our dataframe we could use the select() function. This will keep only the variables you select. koala_select&lt;-select(koala, species, sex, age) ## species sex age ## 1 Phascolarctos cinereus male 8 ## 2 Phascolarctos cinereus female 10 ## 3 Phascolarctos cinereus male 1 ## 4 Phascolarctos cinereus male 1 ## 5 Phascolarctos cinereus female 10 ## 6 Phascolarctos cinereus male 12 If we open up koala_select we’ll see that it only contains the species, sex and age columns. Above we used ‘normal’ R grammar, but the strengths of dplyr lie in combining several functions using pipes. Since the pipes grammar is unlike anything we’ve seen in R before, let’s repeat what we’ve done above using pipes. koala_select_pipe&lt;-koala%&gt;%select(species, sex, age) To help you understand why we wrote that in that way, let’s walk through it step by step. First we summon the koala dataframe and pass it on, using the pipe syntax %&gt;%, to the next step, which is the select() function. In this case we don’t specify which data object we use in the select() function since in gets that from the previous pipe. 2.2.2 filter() filter() is one of the most useful dplyr functions for data manipulation. Say you’re conducting a study of only male koalas. You won’t need any data on female koalas. So lets get rid of it! koala_filter&lt;-koala%&gt;%filter(sex == &#39;male&#39;) Did it work? summary(koala_filter$sex) ## female male ## 0 115 No more females in the data! Let’s test our knowledge with a challenge. 2.2.3 Challenge 1 Write a single command (which can span multiple lines and includes pipes) that will produce a dataframe that has the values for age, size and color for females only. How many rows and columns does your dataframe have and why? Extra challenge: out of this new dataset, filter only koalas &gt;70cm in size. How many are there? This should be your data structure: nrow(challenge1) ## [1] 127 ncol(challenge1) ## [1] 3 We removed all the males, so our row number reduces from 242 to 127. Then we filter our desired columns and are now at 3 instead of 15. nrow(challenge1.2) ## [1] 46 You can find the solutions to all challenges posed here at the end of the document. Don’t peek! 2.2.4 group_by() and summarise() Now, we were supposed to be reducing the error prone repetitiveness of what can be done with base R, but up to now we haven’t done that since we would have to repeat the above for each sex. Instead of filter(), which will only pass observations that meet your criteria (in the above: sex==\"female\"), we can use group_by(), which will essentially use every unique criteria that you could have used in filter(). Let’s see what happens with our data structure when using dplyr’s group_by(). koala_group&lt;-koala%&gt;%group_by(sex) str(koala_group) ## tibble [242 x 15] (S3: grouped_df/tbl_df/tbl/data.frame) ## $ species: Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num [1:242] 153 148 153 153 153 ... ## $ Y : num [1:242] -27.5 -22.5 -27.5 -27.5 -27.5 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num [1:242] 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num [1:242] 70.8 70.4 68.7 73 65.2 ... ## $ fur : num [1:242] 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num [1:242] 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int [1:242] 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... ## - attr(*, &quot;groups&quot;)= tibble [2 x 2] (S3: tbl_df/tbl/data.frame) ## ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 ## ..$ .rows:List of 2 ## .. ..$ : int [1:127] 2 5 9 10 12 13 15 17 20 22 ... ## .. ..$ : int [1:115] 1 3 4 6 7 8 11 14 16 18 ... ## ..- attr(*, &quot;.drop&quot;)= logi TRUE You will notice that the structure of the dataframe where we used group_by() (koala_group) is not the same as the original koala dataset. A grouped dataset can be thought of as a list where each item in the list is a data.frame which contains only the rows that correspond to the a particular value ‘Sex’ (at least in the example above). The above was a bit on the uneventful side because group_by() is only really useful in conjunction with summarise(). This will allow you to create new variable(s) by using functions that repeat for each of the sex-specific data frames. That is to say, using the group_by() function, we split our original dataframe into multiple pieces, then we can run functions such as mean() or sd() within summarise(): koala_group_sum&lt;-koala%&gt;%group_by(sex)%&gt;% summarise(mean_age=mean(age)) ## # A tibble: 2 x 2 ## sex mean_age ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 6.25 ## 2 male 6.63 And there we go. We got what we wanted and summarised the mean age of our koalas for both sexes separately. And we did that using only one simple line of code! I think it is time for another challenge to test our skills! 2.2.5 Challenge 2 Calculate the average weight value per state and Sex. Which combination of state and sex has the heaviest and which combination had the lightest koalas? ## # A tibble: 8 x 3 ## # Groups: state [4] ## state sex mean_weight ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 New South Wales female 6.54 ## 2 New South Wales male 9.07 ## 3 Queensland female 5.68 ## 4 Queensland male 6.82 ## 5 South Australia female 7.59 ## 6 South Australia male 16.8 ## 7 Victoria female 7.58 ## 8 Victoria male 7.48 That is already quite powerful, but it gets even better! You’re not limited to defining only one new variable in summarise(): challenge2_ext&lt;-koala%&gt;%group_by(state, sex)%&gt;% summarise(mean_weight = mean(weight), sd_weight = sd(weight), sample_no = n()) We can create a new dataframe with as many new variables as we want. Very useful for our inital data exploration! Let’s get our hands another very useful function: mutate(). 2.2.6 mutate() We can create an entirely new variables in our initial dataset prior to (or even after) summarizing information using mutate(). Let’s say we’re interested in the weight:size ratio of our Koalas. Also we want to give each individual a numeric identifier to be able to better work with our data later on. koala_mutate&lt;-koala%&gt;%mutate(weight_size_ratio = size/weight, ID = row_number()) ## [1] &quot;species&quot; &quot;X&quot; &quot;Y&quot; ## [4] &quot;state&quot; &quot;region&quot; &quot;sex&quot; ## [7] &quot;weight&quot; &quot;size&quot; &quot;fur&quot; ## [10] &quot;tail&quot; &quot;age&quot; &quot;color&quot; ## [13] &quot;joey&quot; &quot;behav&quot; &quot;obs&quot; ## [16] &quot;weight_size_ratio&quot; &quot;ID&quot; Our dataset now has two extra columns containing the variables we were interested in. If you do not want to manipulate your raw data, you can use mutate before grouping and summarising to create the summary table straight away: koala_mutate_weight_size&lt;-koala%&gt;%mutate(weight_size_ratio = size/weight)%&gt;% group_by(sex)%&gt;% summarise(mean_weight = mean(weight), sd_weight = sd(weight), mean_weight_size = mean (weight_size_ratio), max_weight_size = max(weight_size_ratio)) ## # A tibble: 2 x 5 ## sex mean_weight sd_weight mean_weight_size max_weight_size ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 6.67 0.533 10.4 12.9 ## 2 male 9.31 2.38 8.21 11.9 Great! Let’s end the lesson with another challenge, combining all the functions we have looked at today. 2.2.7 Challenge 3 Calculate the average tail length and fur thickness for a group of 20 randomly selected males and females from New South Wales. Then arrange the mean tail length in descending order. Hint: Use the dplyr functions arrange() and sample_n(), they have similar syntax to other dplyr functions. Look at the help by calling ‘?function’, e.g. ?arrange. ## # A tibble: 2 x 3 ## sex mean_tail mean_fur ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 male 1.48 2.88 ## 2 female 1.41 2.49 Since we are sampling randomly, these will look different for each of you. 2.2.8 Solution to all challenges 2.2.8.1 Challenge 1 challenge1&lt;-koala%&gt;%filter(sex == &#39;female&#39;)%&gt;% select(age, size, color) challenge1.2&lt;-challenge1%&gt;%filter(size&gt;70) 2.2.8.2 Challenge 2 challenge2&lt;-koala%&gt;%group_by(state, sex)%&gt;% summarise(mean_weight = mean(weight)) 2.2.8.3 Challenge 3 challenge3&lt;-koala%&gt;%filter(state == &#39;New South Wales&#39;)%&gt;% group_by(sex)%&gt;% sample_n(20)%&gt;% summarise(mean_tail = mean(tail), mean_fur = mean(fur))%&gt;% arrange(desc(mean_tail)) "],
["working-with-spatial-data-in-r.html", "Chapter 3 Working with spatial data in R 3.1 Transforming a dataset into an sf object 3.2 Loading shapefiles into R, transforming and plotting 3.3 Simple geometric operations 3.4 Making a map using ggplot2 3.5 R raster basics 3.6 Plotting rasters in ggplot2 3.7 Solutions to Challenges", " Chapter 3 Working with spatial data in R Many of you will be familiar with using spatial data such as vector data in form of shapefiles or raster data in GIS software, which can handle these files intuitively. Nevertheless, GIS do have some disadvantages when it comes to either large data sets or automation processes. R offers a full integration of spatial tools and references such as GDAL or PROJ and some great packages to deal with this data seamlessly. Today we are going to learn how to use the package sf and have a brief detour towards raster data in R. You are going to need to install the following packages for this tutorial: install.packages(c(&#39;tidyverse&#39;, &#39;sf&#39;, &#39;raster&#39;, &#39;viridis&#39;)) And then call the packages: library(sf) library(raster) library(viridis) library(tidyverse) We will be working on a dataset of koala observations, that you can download here, as well as a basic administrative Areas shapefile of Australia, which you can find here. For the last part we will be looking at rasters, with a digital elevation model (DEM) file as example. You can find it for download here. Please make sure these are stored within a data/ folder in an R project (or working directory). 3.1 Transforming a dataset into an sf object Let’s first load in our dataset and have a look at it’s structure koala &lt;- read.csv(&#39;data/koala.csv&#39;) head(koala) # see the first few rows in the console ## species X Y state region sex weight ## 1 Phascolarctos cinereus 153.2155 -27.49284 Queensland northern male 7.119754 ## 2 Phascolarctos cinereus 148.1443 -22.47617 Queensland northern female 5.451345 ## 3 Phascolarctos cinereus 153.2285 -27.50298 Queensland northern male 6.630577 ## 4 Phascolarctos cinereus 152.6000 -27.50000 Queensland northern male 6.470019 ## 5 Phascolarctos cinereus 153.2817 -27.52589 Queensland northern female 5.620447 ## 6 Phascolarctos cinereus 152.8330 -27.20000 Queensland northern male 7.287674 ## size fur tail age color joey behav obs ## 1 70.80159 1.858696 1.168241 8 grey No Sleeping Spotlighting ## 2 70.38537 1.852801 1.562456 10 grey-brown Yes Sleeping Opportunistic ## 3 68.65867 2.479280 1.056640 1 light grey No Just Chillin Spotlighting ## 4 72.98919 1.923974 1.801244 1 grey No Sleeping Stagwatching ## 5 65.19529 1.945341 1.625600 10 grey-brown No Sleeping Stagwatching ## 6 70.56514 1.688897 1.086675 12 grey-brown No Feeding Opportunistic As we can see it contains lots of variables related to each of the observed koalas, such as sex, weight or in which state the observation was made. Additionally, whoever collected this data was so kind to also include the X and Y coordinates, where the observation was made. This we can use to transform this into an sf (simple feature) object. koala_sf &lt;- st_as_sf(koala, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 4326) str(koala_sf) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 242 obs. of 14 variables: ## $ species : Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num 70.8 70.4 68.7 73 65.2 ... ## $ fur : num 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... ## $ geometry:sfc_POINT of length 242; first list element: &#39;XY&#39; num 153.2 -27.5 ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;names&quot;)= chr &quot;species&quot; &quot;state&quot; &quot;region&quot; &quot;sex&quot; ... The sf object is a data.frame with a geometry list-column. It supports different format and spatial references. The file as it exists in your R environment is what you would call the Attribute table in a GIS software. In R you can use and manipulate it as any other data.frame. When converting a table into a sf we need to indicate the columns containing coordinates using coords = c() and we can decide on a coordinate reference system using crs=. Here we decide to use WGS84 (CRS = 4326), a standard Mercator coordinate frame for the Earth given in Latitude and Longitude. We can plot our now spatial data with the standard base R command: plot(koala_sf) ## Warning: plotting the first 10 out of 13 attributes; use max.plot = 13 to plot ## all This will try to plot all columns (because they are now all spatially referenced). To only plot the geometries, you can: plot(koala_sf$geometry) So far so good, but we will need a bit more data to make a nice map! 3.2 Loading shapefiles into R, transforming and plotting You can load any vector file, such as .shp or .gpkg using st_read. Let’s get our Australia map into the environment and check it out: states &lt;- st_read(&quot;data/Australia/Australia_proj.shp&quot;) ## Reading layer `Australia_proj&#39; from data source `D:\\OneDrive\\OneDrive - The University of Melbourne\\Github\\bookdown-playaround\\data\\Australia\\Australia_proj.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 8 features and 15 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -2063975 ymin: -4965263 xmax: 1891143 ymax: -1285856 ## projected CRS: GDA94 / Geoscience Australia Lambert plot(states$geometry) Great, we even have states column, which will we will use a bit later. To plot, simply use base R syntax of plotting and then plotting another graph on top using add = T. Instead of calling plot(file$geometry), we can also call st_geometry() plot(st_geometry(states), axes = TRUE) plot(st_geometry(koala_sf), # why does this not work? col = &quot;blue&quot;, add = T) Hm, our states are plotting fine, but where are our koala locations? Maybe we should check the coordinate reference system to see if they match… Here’s how that’s done: st_crs(states) ## Coordinate Reference System: ## User input: GDA94 / Geoscience Australia Lambert ## wkt: ## PROJCRS[&quot;GDA94 / Geoscience Australia Lambert&quot;, ## BASEGEOGCRS[&quot;GDA94&quot;, ## DATUM[&quot;Geocentric Datum of Australia 1994&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4283]], ## CONVERSION[&quot;Geoscience Australia Standard National Scale Lambert Projection&quot;, ## METHOD[&quot;Lambert Conic Conformal (2SP)&quot;, ## ID[&quot;EPSG&quot;,9802]], ## PARAMETER[&quot;Latitude of false origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,134, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,-18, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,-36, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;Australia - onshore&quot;], ## BBOX[-43.7,112.85,-9.86,153.69]], ## ID[&quot;EPSG&quot;,3112]] st_crs(koala_sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;World&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] Damn, they do not match. Our Australia shapefile has a different (weird) projection. If you feel it’s tedious to check the CRS in the console, simply type st_crs(states)==st_crs(koala_sf) ## [1] FALSE FALSE is returned, so they don’t match. We need to transform one of them to be able to plot our spatial points on top of the Australia map. sf has a simple solution for this using st_transform(). Let’s do that and plot straight away. koala_proj &lt;- st_transform(koala_sf, crs = st_crs(states)) plot(st_geometry(states), axes = TRUE) plot(st_geometry(koala_proj), col = &quot;blue&quot;, add = T) Excellent! In st_transform we can call any crs from another spatial file in the environment, our define our own using a proj4string. If we look at our plot, we can see that the coordinates on the axis look weird. We’d rather like latitude and longitude to make it look more comprehensive. That’s now up to you to fix it! 3.2.1 Challenge 1 Load the Australia_proj and koala data again and plot them on top of each other in Mercator projection (CRS: 4326). Note: check the spatial reference of the maps first! Extra challenge: color the koala points based on state! Check ?sf::plot for tips Looks good! You can find the solutions to all challenges posed here at the end of the document. Don’t peek! Let’s make our maps look even better. 3.3 Simple geometric operations sf supports any geometric operations you know from GIS software. We will just touch cropping here, but you can also merge, intersect, overlap and many more. For the purpose of mapping, st_crop() is most relevant. Let’s say we want to create a map of all our Koala locations, we won’t necessarily need to show all of Australia, knowing that Koalas are only distributed along the east coast. Let’s crop the Australia shapefile by the extent of our observations: australia_koala_area&lt;-st_crop(states_proj, koala_sf) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries plot(st_geometry(australia_koala_area), axes = T) plot(st_geometry(koala_sf), add = T) Great. Let’s say we want to save the new cropped shapefile to an outputs/ folder in our project. For this we simply use st_write(): st_write(australia_koala_area, &#39;outputs/koala_shape.gpkg&#39;) You can choose different file extensions. Here we are saving a geopackage, as it results in only one file, rather than 4-6 for a .shp extension. Up to you ;) Let’s make a good looking map with our data! 3.4 Making a map using ggplot2 ggplot2 supports plotting spatial data by calling geom_sf(), after defining your data to plot. In simple terms we can create a map of Australia by calling: ggplot(data = states_proj) + geom_sf() which already looks a lot better than base R. But there are a lot of improvements to be made to create a publication-ready map. First we can assign colors based on columns in your data. We can also remove the standard grey ggplot background with a more map-friendly white grid using theme_minimal() ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal() We don’t really need a legend here, let’s remove it: ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal()+ guides(fill = F) Using coord_sf() we can even change the projection while plotting in ggplot2, without the need of going back to reprojecting or transforming in sf. ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal()+ guides(fill = F)+ coord_sf(crs = 3112) Did you see what changed? Finally we can change the fore- and background color to make it look like proper map in a nice blue ocean: ggplot(data = states_proj)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ theme_minimal()+ coord_sf(crs = 3112)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;)) 3.4.1 Challenge 2 Add the koala positions on top of the Australia map in ggplot2. Only plot the area that has koalas and color the points by sex of the individual. Please also give the map in Mercator projection You will need to: Add a second layer to your ggplot Define the crs Good job! But unfortunately our cut-out looks like an island surrounded by seawater. If we want the map to look like a close-up of eastern Australia, we need to tell ggplot2 to start plotting at x/y = 0: ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;))+ scale_x_continuous(expand = c(0,0))+ scale_y_continuous(expand = c(0,0)) When working with these types of maps, you will often be asked to create an inset map that shows the entire area that you cropped from for reference. Let’s add a little Australia shape with a bounding box of our study area to our map. First we need to create the a base map with bounding box and save it to the environment: inset&lt;-ggplot(data = states_proj, show.legend = &quot;point&quot;)+ geom_sf()+ geom_rect(xmin = extent(australia_koala_area)[1], xmax = extent(australia_koala_area)[2], ymin = extent(australia_koala_area)[3], ymax = extent(australia_koala_area)[4], fill = NA, colour = &quot;black&quot;, size = 1.5)+ labs(x = &#39;&#39;, y = &#39;&#39;, title = &#39;&#39;)+ coord_sf(crs = 4326)+ theme_void() inset geom_rect can create any rectangular shape within your ggplot based on xmin, xmax and so on. Since these are dependent on your scales, for mapping we can use the extent of our cropped area from before. extent() is part of the raster package: extent(australia_koala_area) ## class : Extent ## xmin : 138.5795 ## xmax : 153.5651 ## ymin : -39 ## ymax : -21.3906 By using [1], [2] and so on, we call the respective coordinate from the four rows that this function is giving us. Using theme_void() is getting rid of any axis and grids, so that we have the map only. We also need to save our map as an object: map&lt;-ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;))+ scale_x_continuous(expand = c(0,0))+ scale_y_continuous(expand = c(0,0)) We use annotation_custom() and ggplotGrob() to place an inset into the main map: map+annotation_custom(ggplotGrob(inset), xmin = 139, xmax = 144, ymin = -26, ymax = -20) The size and position here is defined by xmin = etc. This needs a bit of fiddling around to get it right. Also be mindful that we are using a Mercator projection here and are in the southern hemisphere, so our ymin and ymax values need to be given in negative form. Now this is a proper map, ready to be put in your next publication! We will briefly talk about rasters, because you are likely to use them if you are working with shapefiles anyway. It is similarly easy to work with rasters in R: 3.5 R raster basics Raster support in R is made available using the package raster. One called, we can load and plot a raster using the same command. Let’s get our DEM to play with: dem&lt;-raster(&#39;data/DEM.tif&#39;) summary(dem) ## Warning in .local(object, ...): summary is an estimate based on a sample of 1e+05 cells (21.51% of all cells) ## DEM ## Min. 0.00000 ## 1st Qu. 89.65413 ## Median 156.76884 ## 3rd Qu. 397.68890 ## Max. 2139.29932 ## NA&#39;s 0.00000 plot(dem) In a raster, cell (or pixel) has a spatial reference and a value. In this case we have the elevation in meters for each degree (0.01, 0.01) in southeastern Australia. Let’s say we’re only interested in Victoria. We can use a spatial vector to mask out the state area from the raster. First we need to create a mask. We already know that our Australia shapefile contains a STATE column. We can use that to subset the shapefile and use the subset as a mask. Simple dplyr commands and %&gt;% work on sf features, which makes working with them so easy in R! vic&lt;-states_proj%&gt;%filter(STATENAME == &#39;Victoria&#39;)%&gt;%st_transform(crs = st_crs(dem)) # we also transform the crs to be sure they match plot(dem) plot(vic$geometry, add = T) To remove anything outside of Victoria, we use raster’s mask() and end up with the DEM for Victoria only: dem_vic&lt;-mask(dem, vic) plot(dem_vic) We can now also filter our koala dataset to only have koalas in Victoria to plot over our DEM. koala_vic&lt;-koala_sf%&gt;%filter(state == &#39;Victoria&#39;)%&gt;%st_transform(crs = st_crs(dem)) plot(dem_vic) plot(koala_vic$geometry, color = &#39;black&#39;, add = T) Often we are using rasters and spatial vector data together to extract data that is generally stored in raster form (such as elevation, mean temperatures or other topographic variables). For example if we are interested to see at which elevation our Koalas were observed in Victoria to make assumptions about their habitat preferences or temperature tolerances. If the CRS of both the raster and shapefile match, we can use extract() to add data from the raster to our spatial points as a new column: koala_vic$ele&lt;-raster::extract(dem_vic, koala_vic) summary(koala_vic$ele) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.62 67.95 185.34 218.54 349.87 648.06 hist(koala_vic$ele) We can see most observations were made below 200 meters. The median elevation for koalas in our dataset was 185. Again if we want to make nice and informative maps out of rasters, even in combination with spatial vector data, ggplot2 is the way to go! 3.6 Plotting rasters in ggplot2 Before we can plot a raster, we have to make it into a data.frame containing of columns X and Y as coordinates and a column containing the raster value of each pixel. Since rasters can be large and the resulting dataframes even larger, we first take a sample of the raster, which reduces the resolution: sample_raster&lt;-sampleRegular(dem_vic, size = 5e5, asRaster = TRUE) %&gt;% as.data.frame(xy = TRUE, na.rm = TRUE)%&gt;% setNames(c(&#39;x&#39;, &#39;y&#39;, &#39;layer&#39;)) Make sure to always use xy = TRUE, so that the coordinates are included in the new data.frame. We now have a format that ggplot can work with using geom_raster() ## x y layer ## 902 140.9675 -33.9925 19.73859 ## 1803 140.9675 -34.0025 48.89558 ## 1804 140.9775 -34.0025 25.70812 ## 2704 140.9675 -34.0125 53.28782 ## 2705 140.9775 -34.0125 27.96507 ## 2706 140.9875 -34.0125 20.13142 ggplot(data = sample_raster, aes ( x = x, y = y, fill = layer))+ geom_raster() Let’s make it look a bit prettier. The package viridis offers a few great color palettes for gradients of continuous variables: ggplot(data = sample_raster, aes ( x = x, y = y, fill = layer))+ geom_raster()+ scale_fill_viridis(option = &#39;A&#39;)+ labs(x = &#39;Longitude&#39;, y = &#39;Latitude&#39;, fill = &#39;Elevation&#39;)+ coord_sf(crs = 4326)+ theme_bw() We can plot additional layers on top of the raster by adding more geom_’s. That means we can plot rasters and spatial vectors together to make great maps for your papers and reports. I’m sure you can figure it out yourself for the next and final challenge! 3.6.1 Challenge 3 Add koala observations, color points by a categorical variable of your choice and change the color scheme. hint: to add different types of data, each data layer needs its own data source!: (geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+…) ggplot()+ geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+ geom_sf(data=koala_vic, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ scale_fill_viridis(option = &quot;D&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, fill = &quot;Elevation&quot;) + coord_sf(crs = 4326) + theme_bw() 3.7 Solutions to Challenges 3.7.1 Challenge 1 states_proj &lt;- st_transform(states, crs = st_crs(koala_sf)) plot(st_geometry(states_proj), axes = TRUE) plot(st_geometry(koala_sf), col = koala_sf$state, add = T) 3.7.2 Challenge 2 ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex))+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;)) 3.7.3 Challenge 3 ggplot()+ geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+ geom_sf(data=koala_vic, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ scale_fill_viridis(option = &quot;D&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, fill = &quot;Elevation&quot;) + coord_sf(crs = 4326) + theme_bw() "],
["functions-loops-and-apply.html", "Chapter 4 Functions, loops, and apply 4.1 Introduction 4.2 Writing basic functions 4.3 Using apply functions 4.4 Loops", " Chapter 4 Functions, loops, and apply 4.1 Introduction Most data analysis in R requires executing a particular task on multiple data observations, groups of observations, or separate data sets. Many novice users begin with the strategy of assigning a new variable for each input: x1 &lt;- dataset1 x2 &lt;- dataset2 meanx1 &lt;- mean(x1) meanx2 &lt;- mean(x2) While this certainly works, it can be very time consuming and introduces substantial risk of user error. There are several alternative strategies that can be used to repeat mathematical operations or sequences of R functions for multiple variables or data sets that avoid this risk. The foundation for any of these strategies is a function to be repeated. Once you have a function that you would like to repeat on multiple inputs, there are a few approaches for iterating through each repetition. In this tutorial, we will focus on the apply functions, which are a set of base R functions that can be used to execute a particular chunk of code on multiple inputs and generate appropriate outputs. The apply functions are simple to use and create very neat code, but one limitation is that they can only be used in operations for which the order of iteration is not important. We will also introduce for and while loops, which can be used when the order of operations does matter. 4.2 Writing basic functions Functions in R take an input value, apply a process, and generate an output. R packages include numerous functions that can do nearly anything you desire, but research often requires specific processes or sequences of commands designed specifically to work with your data structure. You can write your own functions in R to perform these tasks. Here is a very simple function: fun1 &lt;- function (x, y){ x + y } Here, x and y are identified as the required input variables. The code written within the brackets describes the mathematical operation to be applied to the input variables. fun1(x = 7, y = 12) ## [1] 19 A function can be applied to a vector of inputs. In this scenario, the function will be applied to each pair of values in the vector sequence: vals_x &lt;- c(4, 7, 1) vals_y &lt;- c(2, 1, 3) fun1(x = vals_x, y = vals_y) ## [1] 6 8 4 4.2.0.1 Nesting functions Functions can also include other functions, which is particularly useful in the context of repeating complex data analyses. Here is a basic example: fun2 &lt;- function (x, y){ mean(x) + y } fun2(x = vals_x, y = vals_y) ## [1] 6 5 7 It is important to note that in this situation, the internal function (mean()) takes a vector of input values. This function will calculate the mean of the vals_x vector and then add this to each separate value of vals_y. This can be very useful, but it’s critical that you know exactly how your function processes input values or you may run into major problems! 4.2.0.2 Assigning variables within functions and return() In some more complex analyses, the nested functions may generate data that needs to be stored as variables so they can be used as input for the next process. Here is a function that calculates the mean of our x values, adds y, and then generates 10 numbers from a normal distribution with a mean of the value calculated in the first step. fun3 &lt;- function (x, y){ val &lt;- mean(x) + y dist &lt;- rnorm(1:10, mean = val, sd = 1) } output &lt;- fun3(x = vals_x, y = 3) output ## [1] 8.027953 6.652956 6.710001 6.700189 7.478872 8.528269 7.168081 7.257157 ## [9] 7.237757 7.056581 If your function assigns multiple variables, you can use return() to determine the data that is output from the function. This can be useful if your function includes writing a data set, or includes another process that is not intended to return anything to R as well as some process intended to return data to the R environment. dir.create(&#39;outputs&#39;) #Let&#39;s see what happens without using the return() function fun4 &lt;- function (x, y){ val &lt;- mean(x) + y dist &lt;- rnorm(1:10, mean = x, sd = 1) write.csv(dist, &quot;outputs/filename.csv&quot;) } output &lt;- fun4(x = vals_x, y = 3) output ## NULL Our output here is NULL because the default output for a function will come from the last command within the function. Here, write.csv() does not produce data to be returned to R. If we add return(), our function will write the .csv and also return the data we want into the environment: #Now let&#39;s add return() fun4 &lt;- function (x, y){ val &lt;- mean(x) + y dist &lt;- rnorm(1:10, mean = x, sd = 1) write.csv(dist, &quot;outputs/filename.csv&quot;) return(dist) } output &lt;- fun4(x = vals_x, y = 3) output ## [1] 1.4882016 7.7739377 0.5915476 3.8264728 9.0731398 1.3346512 4.0727321 ## [8] 6.9882593 2.6671801 4.8254556 Much better! 4.3 Using apply functions Once you have written a function, you will likely need to execute that function on multiple data sets or variables. This is where the apply functions come in handy. The apply functions are a set of functions that can be used to, well, apply functions to lists, dataframes, and matrices. apply functions are similar to loops, but have a much simpler syntax. There are several different functions that can be used depending upon the inputs and desired outputs. We will work through the following apply functions: apply - used with arrays, including matrices, and returns a vector, array, or list. sapply - (simplified apply) applies the function to each element of a vector, list, or matrix, and returns the outputs as a vector or matrix. lapply - (list apply) is similar to sapply, but returns a list of objects, rather than a vector. mapply - (multivariate apply) applies a function to the first elements of two or more vectors or lists, and then the second pair of elements, etc.. If this doesn’t make much sense, don’t stress! We will walk though an example of each function and situations for which they might be useful. 4.3.1 apply Let’s generate a theoretical transition matrix for a “before” and “after” data set that shows the number of transitions from one type to another. before &lt;- rpois(1000, lambda = 1) after &lt;- rpois(1000, lambda = 1) tab &lt;- table(before, after) tab ## after ## before 0 1 2 3 4 5 ## 0 138 117 68 18 7 0 ## 1 130 139 71 32 4 2 ## 2 49 77 38 18 3 0 ## 3 29 25 16 2 1 0 ## 4 3 5 4 2 0 0 ## 5 0 0 1 1 0 0 We would like to convert this table to the relative probability for each transition- that is, what is the probability that an object of type “0” in the “before” dataset transitions to a 1? To a 2? 3? To calculate the probability, we will need to get the sum of observations in each row, and then divide each transition in that particular row by the row sum. We can do this using the apply function. apply has three arguments: X is the array to which the function will be applied. MARGIN is used to identify whether the function should be applied to the rows, columns, or both. ‘1’ is for rows, ‘2’ is for columns, and c(1,2) can be used for both. FUN is the function to be applied. In this scenario, we set MARGIN = 1 so it will perform the function row wise. Up to this point, we have defined our functions outside of other commands, but in this scenario, it is a very simple function and so we can define the function within the command itself. *Note: The t() function surrounding the command transposes the data set so it returns the matrix in the original orientation (before as rows and after as columns) rel_prob &lt;- t(apply(tab, MARGIN = 1, FUN = function (i) {i/sum(i)} )) rel_prob ## after ## before 0 1 2 3 4 5 ## 0 0.3965517 0.3362069 0.1954023 0.05172414 0.02011494 0.000000000 ## 1 0.3439153 0.3677249 0.1878307 0.08465608 0.01058201 0.005291005 ## 2 0.2648649 0.4162162 0.2054054 0.09729730 0.01621622 0.000000000 ## 3 0.3972603 0.3424658 0.2191781 0.02739726 0.01369863 0.000000000 ## 4 0.2142857 0.3571429 0.2857143 0.14285714 0.00000000 0.000000000 ## 5 0.0000000 0.0000000 0.5000000 0.50000000 0.00000000 0.000000000 Hurray! Please not that there are other ways to perform the same task, but this one is a conceptually a simple example :) 4.3.2 sapply Let’s check out the Loblolly dataset. It contains the height, age, and seed source of a sample of Loblolly pines. library(datasets) data(Loblolly) head(Loblolly) ## height age Seed ## 1 4.51 3 301 ## 15 10.89 5 301 ## 29 28.72 10 301 ## 43 41.74 15 301 ## 57 52.70 20 301 ## 71 60.92 25 301 The data set includes a range of ages. Let’s see if we can write a function that calculates the mean height for each age group. #Create a list with each unique age group ages &lt;- unique(Loblolly$age) ages ## [1] 3 5 10 15 20 25 #Create a function that calculates the mean height for each age group fun5 &lt;- function (x){ agegroup &lt;- subset(Loblolly, age == x) mean_height &lt;- mean(agegroup$height) return(mean_height) } fun5(ages) ## [1] 32.3644 Hmmm, this returns one value rather than a separate mean for each group. Why is that? In this function, x is a vector, and so the subset function selects the cases where age is equal to any of the values included in the ages vector. Because we have selected every value of age, we will get the mean height for the entire dataset. If we would like a vector of the means for each height, we will can use sapply. #Create a function that calculates the mean height for each age group mean_heights &lt;- sapply(X = ages, FUN = fun5) mean_heights ## [1] 4.237857 10.205000 27.442143 40.543571 51.468571 60.289286 Excellent! We now have a vector with the mean height for each age group! 4.3.3 lapply You can use the lapply function to get the same information, but in a list format. heights &lt;- lapply(X = ages, FUN = fun5) heights ## [[1]] ## [1] 4.237857 ## ## [[2]] ## [1] 10.205 ## ## [[3]] ## [1] 27.44214 ## ## [[4]] ## [1] 40.54357 ## ## [[5]] ## [1] 51.46857 ## ## [[6]] ## [1] 60.28929 In this scenario, sapply might be a better fit. lapply is generally more useful if your output is a an object like a data.frame or produces objects of different types. Let’s write a function that creates a subsetted data.frame for each value of tree age. ages &lt;- unique(Loblolly$age) func6 &lt;- function (x){ subset(Loblolly, age == x) } sub_df &lt;- lapply(ages, func6) #Let&#39;s look at items 1 and 2 in the list sub_df[1:2] ## [[1]] ## height age Seed ## 1 4.51 3 301 ## 2 4.55 3 303 ## 3 4.79 3 305 ## 4 3.91 3 307 ## 5 4.81 3 309 ## 6 3.88 3 311 ## 7 4.32 3 315 ## 8 4.57 3 319 ## 9 3.77 3 321 ## 10 4.33 3 323 ## 11 4.38 3 325 ## 12 4.12 3 327 ## 13 3.93 3 329 ## 14 3.46 3 331 ## ## [[2]] ## height age Seed ## 15 10.89 5 301 ## 16 10.92 5 303 ## 17 11.37 5 305 ## 18 9.48 5 307 ## 19 11.20 5 309 ## 20 9.40 5 311 ## 21 10.43 5 315 ## 22 10.57 5 319 ## 23 9.03 5 321 ## 24 10.79 5 323 ## 25 10.48 5 325 ## 26 9.92 5 327 ## 27 9.34 5 329 ## 28 9.05 5 331 4.3.4 mapply mapply is an extremely useful function that is a great way to avoid using for loops! With mapply, you can use multiple input variables and it will iterate through each pair (or trio or more) of variables as inputs for the function. Let’s say we would like to generate a theoretical normal distribution of tree height for each age group based on the mean and standard deviation of height. We will need a vector of means and a vector of heights to use as input values. We have already genereated a vector of means in our sapply example, so we will use a similar equation to generate a vector of sd. fun6 &lt;- function (x){ agegroup &lt;- subset(Loblolly, age == x) sd(agegroup$height) } sd_height &lt;- sapply(ages, fun6) sd_height ## [1] 0.4036026 0.8155767 1.5378866 1.9508444 2.2118278 2.2688339 Now, we can write a function that takes the two variables and applied it to each pair of input values to generate 6 six distributions. fun7 &lt;- function (x, y){ rnorm(100, mean = x, sd = y) } height_dists &lt;- mapply(FUN = fun7, x = mean_heights, y = sd_height) colnames(height_dists) &lt;- ages #This names the columns by the tree age summary(height_dists) ## 3 5 10 15 ## Min. :3.177 Min. : 7.648 Min. :24.16 Min. :35.13 ## 1st Qu.:3.916 1st Qu.: 9.797 1st Qu.:26.10 1st Qu.:39.08 ## Median :4.237 Median :10.172 Median :27.15 Median :40.62 ## Mean :4.227 Mean :10.245 Mean :27.20 Mean :40.79 ## 3rd Qu.:4.525 3rd Qu.:10.709 3rd Qu.:28.22 3rd Qu.:42.22 ## Max. :5.259 Max. :12.147 Max. :31.35 Max. :46.80 ## 20 25 ## Min. :47.22 Min. :55.41 ## 1st Qu.:50.18 1st Qu.:58.82 ## Median :51.27 Median :60.71 ## Mean :51.47 Mean :60.63 ## 3rd Qu.:52.68 3rd Qu.:62.38 ## Max. :55.86 Max. :67.61 4.4 Loops I haven’t done this part yet! "],
["spatial-tools-for-forestry-and-ecological-applications.html", "Chapter 5 Spatial tools for forestry and ecological applications 5.1 An introduction to processing LiDAR data in R 5.2 Working with LiDAR data in R 5.3 Step 1. Loading and visualizing pointclouds 5.4 Step 2. Computing a canopy height model 5.5 Step 3. Individual tree segmentation 5.6 Additional analysis: Computing a digital terrain model (DTM) 5.7 Wrapping up 5.8 Solutions to the challenges", " Chapter 5 Spatial tools for forestry and ecological applications 5.1 An introduction to processing LiDAR data in R NOTE: This tutorial is based on lidR function Syntax pre version 3.0. Older function names have been depreceated, but are still working, thus not affecting the workflow of this tutorial. Nevertheless, befor you start working on your own data, you might want to check for updated function names and new functionalites in lidR 3.0 here. 5.2 Working with LiDAR data in R lidR is a R package to process, manipulate and visualize LiDAR (and photogrammetry) pointclouds for forestry applications. Useful functionality includes the simple and memory-efficient plotting and visualization of data, computation of canopy height models (CHMs), tree segmentation and the extraction of tree metrics such as tree height and crown area. For more advanced users it additionally offers batch functionality, called cataloging, to process large LiDAR chunks, which otherwise would be very computation intensive. This tutorial is a quick guide to getting familiar with the main functionality of lidR and based on the excellent package documentation that can be found here. You can find the github page for lidR at https://github.com/Jean-Romain/lidR and the package documentation at https://cran.r-project.org/web/packages/lidR/lidR.pdf. We will be working with two example files (Example.las and drawno.laz), that you need to store in a data/ folder in your project (or working) directory. The files can be downloaded here. Please make sure that you have downloaded and stored the two files before beginning the tutorial. You also need to install the following packages: install.packages(c(&#39;lidR&#39;, &#39;raster&#39;, &#39;rgdal&#39;, &#39;dplyr&#39;, &#39;sf&#39;)) We will also need the package EBImage for our tree segmentation, which is distributed as part of the Bioconductor project. To install, run the following code: install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;EBImage&quot;) If this prompts you to update existing packages (Update all/some/none? [a/s/n]:) in the console, type n and hit enter. Once all packages are installed, we need to call them: library(raster) library(EBImage) library(lidR) library(sf) library(rgdal) library(dplyr) 5.3 Step 1. Loading and visualizing pointclouds For those familiar with R, loading and plotting pointcloud files will be easy: we are using simple read and plot syntax. lidR can load both .las and .laz file formats. Let’s read one of our example files and plot it: forest&lt;-readLAS(&#39;data/drawno.laz&#39;) Using print() will give us a bit of information about the file we have just loaded, for example on the number of points and point density. print(forest) ## class : LAS (LASF v1.2) ## point format : 1 ## memory : 11.5 Mb ## extent :278200, 278300, 602200, 602300 (xmin, xmax, ymin, ymax) ## coord. ref. : NA ## area : 9996.22 units² ## points : 150.4 thousand points ## density : 15.05 points/units² ## names : X Y Z gpstime Intensity ReturnNumber NumberOfReturns ScanDirectionFlag EdgeOfFlightline Classification Synthetic_flag Keypoint_flag Withheld_flag ScanAngleRank UserData PointSourceID We can see that this particular file has ~150,000 points with a point density of ~15 per unit. We can also see that there is no coordinate reference system (crs) associated with this file, which is why the unit is unknown. This might lead to issues later on, so always check that your raw pointcloud has a crs assigned, before starting to work with it. For the purpose of this tutorial, we can overlook this. Let’s visualize our pointcloud! plot(forest) Figure 5.1: A new window will open and your plot will look something like this You can change the color of your point cloud using colorPalette =. Make sure to close the previous plot window before plotting a new one. Let’s try a few options: plot(forest, colorPalette = terrain.colors(50)) plot(forest, colorPalette = heat.colors(50)) 5.4 Step 2. Computing a canopy height model Now that we have imported our LiDAR data in to R, we can start working with it using the functionality of the package. We want to compute a canopy height model (CHM) from our two example files. CHMs are rasterized versions of the forests captured using LiDAR. Each pixel value in this raster describes the height above ground. These CHMs are useful to get an idea about the forest structure of the area we are looking at. A CHM is derived from the pointcloud by subtracting a digital terrain model (DTM) describing the ground topography, from a digital surface model (DSM) describing points of the pointcloud above the ground: Figure 5.2: The theory behind a CHM. Graphic: Colin Williams, NEON 5.4.1 Ground detection and normalizing a point cloud While we could now go ahead and calculate a DSM and DTM from our raw pointcloud, lidR has built-in functions to make this easier for us. First of all, we need to normalize our pointcloud, so that all ground points are set to 0. Since you might have noticed that our example forest is already normalized, we will load our other example and start using lasground() raw_forest&lt;-readLAS(&#39;data/Example.las&#39;) ## Warning: Invalid data: 194244 points with a number of returns equal to 0 found. ## Warning: Invalid data: 194244 points with a &#39;return number&#39; greater than the ## &#39;number of returns&#39;. Do not mind the warnings that pop up. These are associated to the data structure but are not a problem in this case. Let’s have a brief look at what this piece of forest looks like: plot(raw_forest) You will see that the terrain here is a bit undulated. To normalize ground elevation, we first need detect ground points in this raw data and save it as a new pointcloud: raw_forest_ground&lt;-lasground(raw_forest, csf()) ## Warning: Zero last return found. Cannot use the option &#39;last_returns&#39;, all the ## points will be used. csf() is one of two ground segmentation algorithms, built into lidR. If you want more detail on what it exactly does, call ?csf in the console. We can visualize what this function has done with the raw data by plotting it and coloring it based on the classification we just ran: plot(raw_forest_ground, color = &quot;Classification&quot;) Detected ground points are illustrated in blue, our trees in grey. You can check the same for our other example to confirm that this pointlcoud is already classified as we expected: plot(forest, color = &quot;Classification&quot;) Now that the points are detected, we can normalize our pointcloud, which sets all ground points to an elevation of 0. raw_forest_norm&lt;-lasnormalize(raw_forest_ground, tin()) Again tin() is the spatial interpolation algorithm at work here. We can see what has happened by plotting our raw and normalized data and comparing the point clouds: plot(raw_forest) plot(raw_forest_norm) Figure 5.3: We can see that the right, raw data exhibits ground topography, while the right, normalized pointcloud is completely flat We need the point cloud to be completely flat, to get the exact height of each tree above ground, without the influence topography might have on tree height. Now we are ready to start working on a CHM! 5.4.2 Building the initial CHM To compute a CHM, we simply apply the grid_canopy() function to the normalized pointcloud, but need to give two extra arguments: chm1 &lt;- grid_canopy(raw_forest_norm, 0.5, p2r()) The first argument is the desired resolution of the output, in our case 0.5 meters. The second argument is the algorithm to compute the CHM. lidR has multiple algorithms built in, here we are using p2r() which attributes the height of the highest point found for each pixel of the output raster. Let’s look at our result: plot(chm1, col = height.colors(50)) Hmm, that doesn’t look too great yet. A lot of white space (empty pixels) indicates that we have pits resulting from the relatively low point density of this point cloud. These will lead to a lot of NA data later on, messing with our analyses, so we should aim at closing these pits by playing around with the arguments within the grid_canopy() function. A simple improvement proposed by Martin Isenburg in lastools (the underlying software to lidR) can be obtained by replacing each LiDAR return with a small disk. Since a laser has a width and a footprint, this tweak may simulate this fact. We add a subcircle of 0.2 diameter in our p2r() algorithm and can see what happens. chm2 &lt;- grid_canopy(raw_forest_norm, 0.5, p2r(0.2)) plot(chm2, col = height.colors(50)) That looks much better already, but some pits still persist. Next we can try reducing the resolution a bit to counter these remaining empty pixels: chm3 &lt;- grid_canopy(raw_forest_norm, 0.75, p2r(0.2)) plot(chm3, col = height.colors(50)) More pits closed, great! We might loose some detail here, as the pixel size is now larger, but we gain detail from replacing NA’s with height values. Note that playing around with these two parameters might take a while but eventually you may find the right approach that fits your data. Another popular approach to avoid the empty pixel problem consists of the interpolation of first returns with a triangulated irregular network (TIN) and then rasterizing it onto a grid to create the CHM. For this we need to use the algorithm dstmin() in grid_canopy(): chm4 &lt;- grid_canopy(raw_forest_norm, 0.5, dsmtin()) plot(chm4, col = height.colors(50)) As explained by Martin Isenburg, the result has no more empty pixels but is full of pits because many laser pulses manage to deeply penetrate the canopy before producing the first return. In the end you might need to apply multiple algorithms and functions to reach the best results. Here is just an example of how this might look like, using multiple options to get a decent result for this pointcloud: algo &lt;- pitfree(c(0,2,5,10,15), c(0,1), subcircle = 0.2) chm5 &lt;- grid_canopy(raw_forest_norm, 0.5, algo) If you are interested in what you can do to improve your CHM, try ?grid_canopy and have a look at the options available. Once we are happy with our CHM, we can have a look at some simple height stats by running summary() for this chunk of forest: summary(chm5) ## Z ## Min. -0.14362 ## 1st Qu. 0.00000 ## Median 10.24074 ## 3rd Qu. 21.64095 ## Max. 52.24510 ## NA&#39;s 1808.00000 Let’s test our knowledge and create a CHM for the other example file! 5.4.3 Challenge 1 Produce a CHM for our other example file with as little pits as possible using the functions and algorithm options described above. Keep in mind the initial resolution and point density and play around with the functions having these in mind. Remember that this pointcloud is already ground normalized, so your might need to adapt the code a bit… Now that is a good looking CHM. Let’s get to using our CHM to segment individual trees as a basis of extracting tree metrics, without having ever to set foot into this forest! 5.5 Step 3. Individual tree segmentation This step requires us to use a pitfree and smooth canopy height model. Smoothing is a post-process technique using the package raster that aims at removing sharp edges and pixels from the CHM by calculating focal (“moving window”) values for the neighborhood of focal cells using a matrix of weights. Let’s do this in one step to get our tree segmentation on the way: # create a new chm to work with algo &lt;- pitfree(thresholds = c(0,10,20,30,40,50), subcircle = 0.2) chm &lt;- grid_canopy(raw_forest_norm, 0.5, algo) # smoothing post-process (here e.g. two pass (running focal twice), 3x3 median convolution) ker &lt;- matrix(1,3,3) chm &lt;- focal(chm, w = ker, fun = median) chm &lt;- focal(chm, w = ker, fun = median) # check the smoothed raster plot(chm, col = height.colors(50)) Comparing this raster to our previous ones, we can see that the ‘fuzziness’ around the edged has been removed. This will aid the next algorithm in finding the trees better. The segmentation using a CHM and normalized pointcloud is implemented in lidR using lastrees(). We will be using a watershed algorithm with a 4 meter threshold. A watershed transformation treats our raster like a topographic map and finds the lines in ‘depressions’ between our trees, treating the tree tops or crowns as ‘ridges’. # segment the trees algo &lt;- lidR::watershed(chm, th = 4) las &lt;- lastrees(raw_forest_norm, algo) # remove points that are not assigned to a tree trees &lt;- lasfilter(las, !is.na(treeID)) Let’s have a look at the now segmented pointcloud, which we called trees: plot(trees, color = &quot;treeID&quot;, colorPalette = pastel.colors(100)) Figure 5.4: Pretty, isn’t it? Each tree got a color assigned to it! To check how well our tree segmentation performed, we can additional detect the position and center of each tree using tree_detection() and plot these on top of each other for quality control. To extract further structural metrics from our forest, we will need these treetops anyway. treetops&lt;-tree_detection(raw_forest_norm, lmf(ws = 5, hmin = 10)) plot(chm) plot(treetops, add = T) The arguments that go into the lmf() algorithm describe 1. the length or diameter of the moving window used to detect the local maxima (ws), which corresponds to the crown width. This metric would need to be confirmed by doing some field measurements. And 2. we are using hmin to define the minimum height of a tree in this forest, which we’d also need to ground truth before running this. This is the threshold below which a pixel or a point cannot be a local maxima (and thus a treetop). For this one, we assumed hmin to be 10 meters. If you don’t have ground measurements to tune your algorithm, you need to try a few different values until you’re happy with the results. We can even plot and evaluate our results in 3D: plot(trees, color = &#39;treeID&#39;, colorPalette = pastel.colors(100)) %&gt;% add_treetops3d(treetops) Figure 5.5: The treetops as red dots on top of our segmented pointcloud. To make sure that these results are saved, we should write all these files from the R environment to our hard drive. That will also allow us to e.g. further work with them in a GIS or in a pointcloud viewer such as CloudCompare. We will create a folder for outputs first, then save our smooth CHM as a raster file (.tif), our normalized pointcloud as a LiDAR file (.las) and the treetops as a shapefile (.shp): dir.create(&#39;outputs&#39;) writeRaster(chm, &#39;outputs/smooth_chm.tif&#39;) writeLAS(raw_forest_norm, &#39;outputs/normalized.las&#39;) writeOGR(treetops, &#39;.&#39;, &#39;outputs/trees.shp&#39;, driver=&quot;ESRI Shapefile&quot;) We can now go on and compute some metrics from our segmented trees. We are today especially interested in extracting crown area and height for each of our segmented trees. First we need to derive a polygon for each crown that was segmented. For this lidR offers tree_metrics() and tree_hulls(), which work on the segmented pointcloud we derived earlier. We can then use dplyr to simply join both together to get a polygon shapefile containing crown area and height: metric &lt;- tree_metrics(trees, .stdtreemetrics) hulls &lt;- tree_hulls(trees) hulls@data &lt;- dplyr::left_join(hulls@data, metric@data) ## Joining, by = &quot;treeID&quot; spplot(hulls, &quot;Z&quot;) spplot from raster allows us to plot these hulls based on Z, which is the tree height for each of the crowns we identified. We can also plot all these together on our CHM to visually check accuracy: plot(chm) plot(hulls, add = T) plot(treetops, add = T) We can see some of our crowns and tree positions don’t seem quite right, so there might be some room for improvement. But always know that using pointclouds and CHMs is just an approximation of the reality by applying algorithms and models, trying to simulate the measured forest as close as possible. So we will never detect 100% of the trees and we will never delineate them perfectly. Nevertheless, using pointclouds, we will be able to measure thousands of trees in a very brief period of time, while when carrying out forest inventories we can only measure a few dozen per plot, requiring lots of hard labor. A general work flow using ground inventories and LiDAR data together would this be to collect an amount of data needed to get an idea of the structure of your forest on the plot level and tune your LiDAR algorithms by measuring actual trees. Then you can extrapolate your field measurements to a whole forest or larger landscape using the pointclouds collected outside your plot area. We want to get data on crown area and tree height together in a comprehensive table for further analysis. The rasterpackage will help us calculating the area and converting our polygon shapefile to a simple feature using sf will make it easier to work with the data. hulls$area&lt;-area(hulls) tree_metrics&lt;-st_as_sf(hulls) sf treats a shapefile’s attribute table like a data.frame and as we can see, the tree_metrics simple feature already contains everything we need: head(tree_metrics) ## Simple feature collection with 6 features and 5 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 488041.5 ymin: 5189194 xmax: 488052.4 ymax: 5189222 ## CRS: +proj=utm +zone=12 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0 ## treeID Z npoints convhull_area area geometry ## 1 332 22.471 234 42.749 42.74878 POLYGON ((488052.4 5189194,... ## 2 347 22.740 114 21.355 21.35498 POLYGON ((488045.3 5189202,... ## 3 215 24.805 158 40.559 40.55957 POLYGON ((488051.3 5189209,... ## 4 351 20.266 91 21.656 21.65552 POLYGON ((488045.5 5189207,... ## 5 271 22.861 146 35.303 35.30347 POLYGON ((488050.5 5189203,... ## 6 353 20.852 51 10.223 10.22339 POLYGON ((488044.2 5189219,... Z is the height of each tree, identified by treeID and area is the exact area of each tree crown. You may see that convhull_area was already calculated by tree_metrics and corresponds to the area, so we could also use that variable instead and not add a new column. Let’s convert this from a sf to a table and remove all unnecessary information by selecting only the variables we are interested in: tree_table&lt;-as.data.frame(tree_metrics)%&gt;%select(&#39;treeID&#39;, &#39;Z&#39;, &#39;area&#39;) head(tree_table) ## treeID Z area ## 1 332 22.471 42.74878 ## 2 347 22.740 21.35498 ## 3 215 24.805 40.55957 ## 4 351 20.266 21.65552 ## 5 271 22.861 35.30347 ## 6 353 20.852 10.22339 We can now also save the polygon shapefile of our crowns, to have everything ready for GIS processing or inspection. Additionally we want to save our metrics table as a .csv. st_write(tree_metrics, &#39;outputs/trees_sf.shp&#39;) write.csv(tree_table, &#39;outputs/tree_table.csv&#39;) 5.5.1 Delineating the crowns Now the hulls are great, but for some applications we might actually need to know the exact shape of each tree crown, rather than its approximate hull, for example when studying crown cover or leaf area index (LAI). We can delineate crowns in more detail working directly on the CHM, without the need of using the pointcloud any longer, after we derived the canopy height model. We can apply watershed() directly to the CHM: crowns &lt;- lidR::watershed(chm, th = 4)() plot(crowns, col = pastel.colors(100)) Now that we have a new raster called crowns where each tree is its own object, we can polygonize it to extract the contours and thus the more precise tree crowns (Note that this might run for a while as it is quite computing intensive): contour &lt;- rasterToPolygons(crowns, dissolve = TRUE) plot(chm, col = height.colors(50)) plot(contour, add = T) plot(treetops, add = T) Before we save this, we need to make sure all our data has the same spatial projection. We can do so using crs(). crs(chm) ## CRS arguments: ## +proj=utm +zone=12 +datum=NAD83 +units=m +no_defs +ellps=GRS80 ## +towgs84=0,0,0 crs(contour) ## CRS arguments: NA Unfortunately rasterToPolygons() does not assign a coordinate reference system, so we have to do so ourselves. While we’re at it, let’s also transform this to the sf format: crs(contour)&lt;-crs(chm) contour&lt;-st_as_sf(contour)%&gt;%rename(treeID = layer) Next we can save the delineated polygons: st_write(contour, &#39;outputs/contour_sf.shp&#39;) Again we want to test our knowledge and apply what we’ve just learned to the other example file. 5.5.2 Challenge 2 Detect trees and treetops in our other example file based on the derived CHM and normalized pointcloud and plot smoothed CHM, hulls and detected treetops on top of one another 5.6 Additional analysis: Computing a digital terrain model (DTM) For many applications and studies, it is important to have detailed topographic data to e.g. assess slope, aspect or potential for water runoff in the ecosystem you’re studying. As LiDAR data penetrates through the canopy and captures laser returns from the ground as well, we can use the data not only to look at above ground structure, but also derive very high resolution digital terrain models (DTMs) that may be helpful in our analysis. Do to so, we are using the classified pointcloud that has ground points detected (which we named raw_forest_ground). grid_terrain() interpolates the ground points and creates a rasterized digital terrain model. The algorithm uses the points classified as “ground” to compute the interpolation. dtm &lt;- grid_terrain(raw_forest_ground, 1, algorithm = kriging(k = 10L)) plot(dtm) summary(dtm) ## Z ## Min. 312.7797 ## 1st Qu. 324.2834 ## Median 329.7094 ## 3rd Qu. 334.8961 ## Max. 347.8688 ## NA&#39;s 8.0000 What you can see when plotting is the topography of the ground under the trees in a unit of meters above sea level (which is derived from the coordinates of each point). We can see that our elevation varies between 313 to 348. The second argument of the function is again the resolution, which we set to 1 (1x1 m). This is an amazingly high resolution, when you think that the next best (free) DTM available from the Shuttle Radar Topography Mission is 30x30 meters. Very high resolution DTMs can be used for detailed analyses of stream flow in hydrological applications, geomorphology or modeling soil wetness with Cartographic Depth to Water Indexes (DTW-index). We are using another algorithm here (kriging) that applies a KNN (k-nearest neighbor) approach, commonly used in DTM creation. We can plot our crowns on top to see where the trees are distributed and how topography might be influencing e.g. the height or crown shape of trees here: plot(dtm) plot(contour$geometry, add = T) We can save our dtm the same way we saved our CHM before, using writeRaster(). lidR has some integrated LiDAR pointclouds to play around with, that might help illustrate this a bit better. Let’s look at one that has more depressions in the form of lakes, using a different algorithm as illustration: LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Topography.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) dtm1 &lt;- grid_terrain(las, algorithm = knnidw(k = 6L, p = 2)) plot(dtm1) lidR also allows to plot DTMs in 3D: plot_dtm3d(dtm1) Figure 5.6: The example data dtm in 3D Another integrated dataset can be called liked this. Feel free to use these to practice creating CHMs, segmenting trees, extracting tree metrics and calculating a DTM. LASfile2 &lt;- system.file(&quot;extdata&quot;, &quot;Megaplot.laz&quot;, package=&quot;lidR&quot;) las2 &lt;- readLAS(LASfile) Let’s try this on the other example too! 5.6.1 Challenge 3 Create a digital terrain model for the other example (forest) and plot the detected treetops on top of the final DTM. Finally, save your DTM as a .tif file to your outputs folder! 5.7 Wrapping up Good job everyone. You have learned how to get your LiDAR data into R and prepare it for further analysis by detecting ground points and normalizing the clouds. Doing that, we were able to derive a canopy height model and learned about how to improve it’s accuracy. Having derived that, we were able to extract metrics for each tree and delineate our crowns to look at single-tree attributes within our example forest. You were also able to compile your inventory data as a comprehensive table and save everything for further processing. Additionally we derived a very detailed digital terrain model. What follows now can be manifold, Maybe you are interested in the exact location and elevation above sea level of all your trees detected. raster allows you to easily extract data from a raster using a spatial vector such as a shape file by simply using extract: elevation&lt;-extract(dtm, treetops, cellnumbers = T, df=T)%&gt;%rename(treeID = ID, elevation = Z) elevation&lt;-cbind(elevation,coordinates(dtm)[elevation[,2],]) elevation&lt;-elevation%&gt;%select(-2) head(elevation) ## treeID elevation x y ## 1 1 335.2109 488042.5 5189193 ## 2 2 335.0209 488046.5 5189195 ## 3 3 335.2547 488042.5 5189203 ## 4 4 334.8753 488040.5 5189208 ## 5 5 334.4009 488043.5 5189210 ## 6 6 333.5780 488044.5 5189222 You could think of other uses, e.g. if you have raster layers describing climatic variables of your forest, you could get precise data for each of your trees and see how it might have affected their growth. You can also use the delineated crowns to extract all pixel data from e.g. multispectral imagery of the crown area of your trees to try and model forest nutrition to make assumptions on the quality of the forest for certain arboreal species. If you have any questions or comments on this tutorial, feel free to write me an email and I’m happy to try and help out. I have a few other (more basic) tutorials compiled like this one if you are interested. Good luck with any future LiDAR adventures in R! 5.8 Solutions to the challenges 5.8.1 Challenge 1 forest_norm &lt;- lasnormalize(forest, tin()) forest_chm &lt;- grid_canopy(forest_norm, 0.25, pitfree(c(0,2,5,10,15), c(0,1), subcircle = 0.2)) 5.8.2 Challenge 2 #data to use: #forest_chm #forest_norm # segment the trees algo &lt;- lidR::watershed(forest_chm, th = 4) las2 &lt;- lastrees(forest_norm, algo) trees_forest &lt;- lasfilter(las2, !is.na(treeID)) plot(trees_forest, color = &quot;treeID&quot;, colorPalette = pastel.colors(100)) # find treetops and hulls treetops_forest&lt;-tree_detection(forest_norm, lmf(ws = 5, hmin = 10)) metric_forest &lt;- tree_metrics(trees_forest, .stdtreemetrics) hulls_forest &lt;- tree_hulls(trees_forest) hulls_forest@data &lt;- dplyr::left_join(hulls@data, metric@data) plot(forest_chm) plot(hulls_forest, add = T) plot(treetops_forest, add = T) 5.8.3 Challenge 3 dtm2 &lt;- grid_terrain(forest, 1, algorithm = kriging(k = 10L)) writeRaster(dtm2, &#39;outputs/forest_DTM.tif&#39;) plot(dtm2) plot(treetops_forest, add = T) "],
["calculating-class-area-from-raster.html", "Chapter 6 Calculating class area from raster 6.1 Simple area and area fraction calculations on a classified raster", " Chapter 6 Calculating class area from raster 6.1 Simple area and area fraction calculations on a classified raster For this tutorial you will need to load the following packages: library(raster) library(sf) library(tidyverse) You will often come across the task of extracting the area each of your raster classes occupies within your entire raster area. There is a simple way of doing this using raster and dplyr. The output will be a comprehensive table. First let’s create some dummy data to work with: 6.1.1 Creating dummy data x &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x)&lt;-100 # we are using a resolution of 100 x 100 so that each pixel is 1ha in size #populate the raster with values values(x)&lt;-base::sample(5, ncell(x), replace = T, prob = c(10,30,20,5,35)) plot(x) Our raster now contains 5 classes that could e.g. be land-use types such as forest, infrastructure or pasture. We additionally gave each class a probability of occurrence so that we can double check of our calculated areas are correct. In total, our raster x has 410^{4} cells. 6.1.2 Preparing the raster for area calculation To get area metrics, we need to transform the raster into a data frame: rast_df&lt;-x%&gt;%as.data.frame(xy = T, na.rm = T) To count each pixel, we can assign an extra column to this dataframe with an ID 1 to be able to tally all cells. Additionally, we are extracting the resolution of our raster as a variable to our environment to later calculate the area. rast_df$ID&lt;-1 reso&lt;-res(x)[1] head(rast_df) ## x y layer ID ## 1 -9950 9950 5 1 ## 2 -9850 9950 2 1 ## 3 -9750 9950 5 1 ## 4 -9650 9950 5 1 ## 5 -9550 9950 2 1 ## 6 -9450 9950 2 1 The layer column contains the class each pixel was assigned to. This will be universal for any raster you put in. ID is the same for each row, this is only needed in the next step. 6.1.3 Compiling a comprehensive table area&lt;-rast_df%&gt;%group_by(layer)%&gt;% summarise(pixelsum = sum(ID), area_ha = (pixelsum*reso^2)/10000)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;% rename(class = layer) area ## # A tibble: 5 x 5 ## class pixelsum area_ha sumA per ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3961 3961 40000 9.90 ## 2 2 11867 11867 40000 29.7 ## 3 3 8068 8068 40000 20.2 ## 4 4 1993 1993 40000 4.98 ## 5 5 14111 14111 40000 35.3 And there we go. Each class has it’s pixelsum calculated, then using the sum we can calculate the area in ha (or else, here you can alternate the code). In this case the pixel sum matches our area_ha because one pixel is already of size 1 ha. We can change the code to e.g. calculate area_km2. area_km2&lt;-rast_df%&gt;%group_by(layer)%&gt;% summarise(pixelsum = sum(ID), area_km2 = pixelsum/100)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;% rename(class = layer) area_km2 ## # A tibble: 5 x 5 ## class pixelsum area_km2 sumA per ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3961 39.6 40000 9.90 ## 2 2 11867 119. 40000 29.7 ## 3 3 8068 80.7 40000 20.2 ## 4 4 1993 19.9 40000 4.98 ## 5 5 14111 141. 40000 35.3 Next we calculate the sum of all pixels (sumA) using mutate() to get the total raster area. This should in this case be the same as ncell(x) (40000). To derive the percentage of the entire each class occupies, we just need to divide the pixelsum of each class by the total sum and multiply by 100. This should match our probabilities we assigned for each class when filling the raster with values: area_km2$per == summary(as.factor(values(x)))/400 ## 1 2 3 4 5 ## TRUE TRUE TRUE TRUE TRUE Enjoy trying it out on your own raster with some actual class ares! "],
["extracting-raster-fractions-and-area-using-a-polygon-mask.html", "Chapter 7 Extracting raster fractions and area using a polygon mask 7.1 Problem statement 7.2 Creating some dummy raster data 7.3 Create some reserves 7.4 Extracting data 7.5 Plotting the results", " Chapter 7 Extracting raster fractions and area using a polygon mask 7.1 Problem statement For this tutorial you will need to load the following packages. Please install if you don’t have them yet. library(raster) library(sf) library(exactextractr) library(tidyverse) Say you have a raster of habitat suitability for a certain species. You could now be interested, how much suitable habitat lies within certain defined areas. For example we could check weather the species is adequately protected by extracting how much suitable area lies within National Parks or other reserves Additionally we can test, how much suitable (and unsuitable) area lies outside the areas you want to test to have an idea of the potential for further protection, or the danger to the species from e.g. land clearing outside of parks. In the end we additionally want to know which fraction of the total area both suitable and unsuitable habitat occupies with our parks, reserves and outside ares. 7.2 Creating some dummy raster data First let’s get a raster to play around with: x &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x)&lt;-100 #make the resolution 100 x 100 meter, so one cell is 1 ha in size values(x)&lt;-runif(ncell(x)) #populate the raster with random values between 0 - 1 plot(x) This raster now contains makeshift values of habitat suitability on a percentage scale. Let’s say we found our cutoff point for suitable habitat at 0.5. Any values below this cutoff are then defined as unsuitable for our target species. We can reclassify our raster accordingly: x_re&lt;-reclassify(x, c(-Inf,0.5,0, 0.5,Inf,1)) plot(x_re) 0 now means non suitable and 1 suitable habitat. Since we randomized our values using runif() we have a equal distribution, meaning that 50% of the area is suitable and 50% of the area is unsuitable. This will help us in double-checking our results later but will probably never occur in real life examples. hist(x) barplot(x_re, col = c(&#39;darkred&#39;,&#39;darkgreen&#39;)) If we wanted to get different probabilities, we could also create our classified raster like this: x_re2 &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x_re2)&lt;-100 values(x_re2)&lt;-sample(0:1, ncell(x_re), replace = T, prob = c(80,20)) plot(x_re2) Using sample() with prob = c(80,20), assigns 80% of the area as unsuitable, but still on a normal pattern. 7.3 Create some reserves We also needs some reserve areas to extract values from. extent(x_re) #have a look at the extent of our raster to decide for extent of dummy areas ## class : Extent ## xmin : -10000 ## xmax : 10000 ## ymin : -10000 ## ymax : 10000 #create some extents a&lt;-extent(c(-1000, 1000, -5233, -2355)) b&lt;-extent(c(-7000, 6530, -400, 4223)) c&lt;-extent(c(-10000, -5427, -10000, -4785)) #transform them into polygons a_sf&lt;-as(a, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res1&#39;) b_sf&lt;-as(b, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res2&#39;) c_sf&lt;-as(c, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res3&#39;) #get the total area to create our outside areas all&lt;-as(extent(x_re), &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;outside&#39;) plot(x_re) plot(a_sf$geometry, add = T, lwd = 2, border = &#39;red&#39;) plot(b_sf$geometry, add = T, lwd = 2, border = &#39;blue&#39;) plot(c_sf$geometry, add = T, lwd = 2, border = &#39;orange&#39;) Now we have three areas that we assume to be our reserves. We still need a polygon for all other areas, also we need to combine our polygons into one. #bind reserves reserves&lt;-rbind(a_sf, b_sf, c_sf) #get the outside areas only outside&lt;-st_difference(all, st_combine(reserves)) #bind all for extraction all_areas&lt;-rbind(reserves, outside) plot(x_re) plot(st_geometry(outside$geometry), border = &#39;blue&#39;, lwd = 4, add = T) plot(st_geometry(reserves), border = as.factor(reserves$name), lwd = 3, add =T) The blue area is all raster area outside of our reserves. Our three reserves are colored in red, black and green. 7.4 Extracting data To extract the cell data from each polygon, we use exact_extract from the package exactextractr, which is a quicker alternative to raster::extract. It extracts data from each feature separately in parallel and saves the extracted values into a list. extract&lt;-exact_extract(x_re,all_areas, fun = NULL) ## | | | 0% | |================== | 25% | |=================================== | 50% | |==================================================== | 75% | |======================================================================| 100% for( i in seq_along(extract)){ extract[[i]]$ID &lt;- seq_along(extract)[i] } In the second step, we assign an ID column to each table in the created list. This will help us in matching the extracted values to the name of the area polygon we extracted from. We convert out list into a table and then add the lc column to assign each value the name of the polygon it belongs to: extract_table&lt;-dplyr::bind_rows(extract)%&gt;%dplyr::select(-2) extract_table$lc &lt;- as.factor(all_areas$name[extract_table$ID]) head(extract_table) ## value ID lc ## 1 0 1 res1 ## 2 0 1 res1 ## 3 0 1 res1 ## 4 0 1 res1 ## 5 1 1 res1 ## 6 1 1 res1 We can now compile a summary table using dplyr syntax. To calculate the area, we are working with the resolution of the raster and will need to save it as a variable fist. reso&lt;-res(x_re)[1] area_habitat&lt;-extract_table%&gt;%group_by(lc, value)%&gt;% summarise(pixelsum = sum(ID), areaha = (pixelsum*reso^2)/10000)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;%ungroup()%&gt;% mutate(sum_all = sum(pixelsum), pertotal = 100*pixelsum/sum_all) area_habitat ## # A tibble: 8 x 8 ## lc value pixelsum areaha sumA per sum_all pertotal ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 outside 0 61328 61328 123560 49.6 144258 42.5 ## 2 outside 1 62232 62232 123560 50.4 144258 43.1 ## 3 res1 0 296 296 600 49.3 144258 0.205 ## 4 res1 1 304 304 600 50.7 144258 0.211 ## 5 res2 0 6370 6370 12784 49.8 144258 4.42 ## 6 res2 1 6414 6414 12784 50.2 144258 4.45 ## 7 res3 0 3726 3726 7314 50.9 144258 2.58 ## 8 res3 1 3588 3588 7314 49.1 144258 2.49 What happens here is, we group by class (lc which refers to our polygon names) as well as value (0 for unsuitable and 1 for suitable habitat). We then tally all pixels using the ID column and calculate the area in ha from the pixelsum. In our case these match because the resolution is already 1 ha (each cell is 100x100 meters). Furthermore we can then tally all pixels to get the total raster area and from that calculate first the percentage of suitable and unsuitable habitat in each polygon and then (by using ungroup()) the fraction of this area compared to the total raster area. 7.5 Plotting the results Using ggplot2 we can then plot these results to better visualize them. The code below will work for any table that is in the format of our area_habitat table above, if you follow the steps of this guide. bar&lt;-ggplot(data = area_habitat, aes(x = reorder(as.factor(lc), -pixelsum), y = per, fill = as.factor(value)))+ geom_bar(stat = &#39;identity&#39;, color = &#39;black&#39;, position=position_fill(), show.legend = FALSE)+ scale_y_continuous(labels = scales::percent)+ geom_text(aes(label=round(per, digits = 1)), check_overlap = TRUE, position=position_fill(), vjust = 1.2, color = &#39;black&#39;)+ theme_classic()+ theme(axis.title.y = element_text(size = 15, face = &#39;bold&#39;), axis.title.x = element_text(size = 15, face = &#39;bold&#39;), axis.text.x = element_text(size = 15, face = &#39;bold&#39;), axis.text.y = element_text(size = 15, face = &#39;bold&#39;))+ ggtitle(&#39;Whatever this area is&#39;)+ xlab(&#39;&#39;)+ ylab(&#39;area percentage&#39;)+ scale_x_discrete(labels=c(&#39;Outside&#39;, &#39;Reserve 2&#39;, &#39;Reserve 3&#39;, &#39;Reserve 1&#39;))+ scale_fill_manual(values = c(&#39;lightgrey&#39;, &#39;darkgreen&#39;), labels = c(&#39;unsuitable&#39;, &#39;suitable&#39;), name = &#39;&#39;) bar Our plot shows for each area we wanted to test, which fraction is suitable and which unsuitable. As expected, in our example we have about a 50:50 distribution, but this may look quite different in a real-world example. Hope you can find a useful application for this code! :) "]
]
