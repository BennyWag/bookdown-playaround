[
["index.html", "Intro to R for Forest Ecosystem Science Chapter 1 Introduction", " Intro to R for Forest Ecosystem Science Anu Singh, Benjamin Wagner, and Kaitlyn Hammond Chapter 1 Introduction We can write something about why we’re doing this here "],
["dplyr-a-brief-introduction-to-tidy-data-manipulation.html", "Chapter 2 dplyr - A brief introduction to tidy data manipulation 2.1 Introducing the koala dataset 2.2 Working with dplyr", " Chapter 2 dplyr - A brief introduction to tidy data manipulation 2.1 Introducing the koala dataset Today we will have a look at some koala data. You can download the dataset for this tutorial here. Futhermore you need to install the tidyverse package, which contains dplyr. install.packages(&#39;tidyverse&#39;) First we need to load the dataset we’re working with. koala&lt;-read.csv(&#39;data/koala.csv&#39;) It should contain the following colums: names(koala) ## [1] &quot;species&quot; &quot;X&quot; &quot;Y&quot; &quot;state&quot; &quot;region&quot; &quot;sex&quot; &quot;weight&quot; ## [8] &quot;size&quot; &quot;fur&quot; &quot;tail&quot; &quot;age&quot; &quot;color&quot; &quot;joey&quot; &quot;behav&quot; ## [15] &quot;obs&quot; Lets look at a structure and summary of this dataset: str(koala) ## &#39;data.frame&#39;: 242 obs. of 15 variables: ## $ species: Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num 153 148 153 153 153 ... ## $ Y : num -27.5 -22.5 -27.5 -27.5 -27.5 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num 70.8 70.4 68.7 73 65.2 ... ## $ fur : num 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... summary(koala) ## species X Y ## Phascolarctos cinereus:242 Min. :138.6 Min. :-39.00 ## 1st Qu.:150.0 1st Qu.:-34.49 ## Median :152.0 Median :-32.67 ## Mean :150.3 Mean :-32.36 ## 3rd Qu.:152.9 3rd Qu.:-30.31 ## Max. :153.6 Max. :-21.39 ## state region sex weight ## New South Wales:181 northern:165 female:127 Min. : 5.406 ## Queensland : 16 southern: 77 male :115 1st Qu.: 6.574 ## South Australia: 14 Median : 7.277 ## Victoria : 31 Mean : 7.923 ## 3rd Qu.: 8.765 ## Max. :17.889 ## size fur tail age ## Min. :64.81 Min. :1.110 Min. :1.004 Min. : 1.00 ## 1st Qu.:68.43 1st Qu.:2.410 1st Qu.:1.272 1st Qu.: 3.00 ## Median :70.27 Median :2.797 Median :1.534 Median : 7.00 ## Mean :70.94 Mean :2.896 Mean :1.507 Mean : 6.43 ## 3rd Qu.:72.33 3rd Qu.:3.217 3rd Qu.:1.750 3rd Qu.: 9.00 ## Max. :81.91 Max. :5.876 Max. :1.981 Max. :12.00 ## color joey behav obs ## chocolate brown:21 No :185 Feeding : 48 Opportunistic:65 ## dark grey :36 Yes: 57 Just Chillin: 67 Spotlighting :94 ## grey :69 Sleeping :127 Stagwatching :83 ## grey-brown :53 ## light brown :20 ## light grey :43 We can see that our dataset contains the positions of each koala in Latitude and Longitude (X and Y) as well as variables describing their physiology, behavior and how they were recorded. This is typical presence-only wildlife data, combining observations with some data describing each individual, which could e.g. be used for distribution modeling or to test influences of other variables such as climate on behavior and physiology of this particular speces. Often in these types of studies, we are not interested in all the recorded variables and thus first need to ‘clean’ our data to make it easier to work with it. dplyr is a package desinged to make data ‘cleaning’ and manipulation of large datasets easier by introducing specific syntax. Let’s see how it works and compares to base R functionality! 2.2 Working with dplyr Base R subsetting can be very tedious. Imagine we want to got the mean age for our koalas, but split it by sex. Getting one mean is easy: mean(koala[koala$sex == &#39;male&#39;, &quot;age&quot;],na.rm = TRUE) ## [1] 6.626087 Summarizing both sexes and savinf it in a table takes a few lines of code: female_mean&lt;-mean(koala[koala$sex == &#39;female&#39;, &quot;age&quot;],na.rm = TRUE) male_mean&lt;-mean(koala[koala$sex == &#39;male&#39;, &quot;age&quot;],na.rm = TRUE) means&lt;-rbind(c(female_mean, male_mean)) means&lt;-as.data.frame(means) names(means)&lt;-c(&#39;female&#39;, &#39;male&#39;) Lets have a look at the results. means ## female male ## 1 6.251969 6.626087 That’s all good, but with that many lines of code quite error prone … dplyr makes data manipulation simpler. For this example, we would only require one line of code! library(dplyr) mean_age_koala&lt;-koala%&gt;%group_by(sex)%&gt;%summarise(mean_age = mean(age)) ## # A tibble: 2 x 2 ## sex mean_age ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 6.25 ## 2 male 6.63 So simple, and looks even better than our base R table too! The main functions we will explore here are dplyr’s pipe %&gt;%, select(), filter(), group_by(), summarise() and mutate(). 2.2.1 select() and dplyr’s pipe If, for example, we wanted to move forward with only a few of the variables in our dataframe we could use the select() function. This will keep only the variables you select. koala_select&lt;-select(koala, species, sex, age) ## species sex age ## 1 Phascolarctos cinereus male 8 ## 2 Phascolarctos cinereus female 10 ## 3 Phascolarctos cinereus male 1 ## 4 Phascolarctos cinereus male 1 ## 5 Phascolarctos cinereus female 10 ## 6 Phascolarctos cinereus male 12 If we open up koala_select we’ll see that it only contains the species, sex and age columns. Above we used ‘normal’ R grammar, but the strengths of dplyr lie in combining several functions using pipes. Since the pipes grammar is unlike anything we’ve seen in R before, let’s repeat what we’ve done above using pipes. koala_select_pipe&lt;-koala%&gt;%select(species, sex, age) To help you understand why we wrote that in that way, let’s walk through it step by step. First we summon the koala dataframe and pass it on, using the pipe syntax %&gt;%, to the next step, which is the select() function. In this case we don’t specify which data object we use in the select() function since in gets that from the previous pipe. 2.2.2 filter() filter() is one of the most useful dplyr functions for data manipulation. Say you’re conducting a study of only male koalas. You won’t need any data on female koalas. So lets get rid of it! koala_filter&lt;-koala%&gt;%filter(sex == &#39;male&#39;) Did it work? summary(koala_filter$sex) ## female male ## 0 115 No more females in the data! Let’s test our knowledge with a challenge. 2.2.3 Challenge 1 Write a single command (which can span multiple lines and includes pipes) that will produce a dataframe that has the values for age, size and color for females only. How many rows and columns does your dataframe have and why? Extra challenge: out of this new dataset, filter only koalas &gt;70cm in size. How many are there? This should be your data structure: nrow(challenge1) ## [1] 127 ncol(challenge1) ## [1] 3 We removed all the males, so our row number reduces from 242 to 127. Then we filter our desired columns and are now at 3 instead of 15. nrow(challenge1.2) ## [1] 46 You can find the solutions to all challenges posed here at the end of the document. Don’t peek! 2.2.4 group_by() and summarise() Now, we were supposed to be reducing the error prone repetitiveness of what can be done with base R, but up to now we haven’t done that since we would have to repeat the above for each sex. Instead of filter(), which will only pass observations that meet your criteria (in the above: sex==\"female\"), we can use group_by(), which will essentially use every unique criteria that you could have used in filter(). Let’s see what happens with our data structure when using dplyr’s group_by(). koala_group&lt;-koala%&gt;%group_by(sex) str(koala_group) ## tibble [242 x 15] (S3: grouped_df/tbl_df/tbl/data.frame) ## $ species: Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num [1:242] 153 148 153 153 153 ... ## $ Y : num [1:242] -27.5 -22.5 -27.5 -27.5 -27.5 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num [1:242] 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num [1:242] 70.8 70.4 68.7 73 65.2 ... ## $ fur : num [1:242] 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num [1:242] 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int [1:242] 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... ## - attr(*, &quot;groups&quot;)= tibble [2 x 2] (S3: tbl_df/tbl/data.frame) ## ..$ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 ## ..$ .rows:List of 2 ## .. ..$ : int [1:127] 2 5 9 10 12 13 15 17 20 22 ... ## .. ..$ : int [1:115] 1 3 4 6 7 8 11 14 16 18 ... ## ..- attr(*, &quot;.drop&quot;)= logi TRUE You will notice that the structure of the dataframe where we used group_by() (koala_group) is not the same as the original koala dataset. A grouped dataset can be thought of as a list where each item in the list is a data.frame which contains only the rows that correspond to the a particular value ‘Sex’ (at least in the example above). The above was a bit on the uneventful side because group_by() is only really useful in conjunction with summarise(). This will allow you to create new variable(s) by using functions that repeat for each of the sex-specific data frames. That is to say, using the group_by() function, we split our original dataframe into multiple pieces, then we can run functions such as mean() or sd() within summarise(): koala_group_sum&lt;-koala%&gt;%group_by(sex)%&gt;% summarise(mean_age=mean(age)) ## # A tibble: 2 x 2 ## sex mean_age ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 6.25 ## 2 male 6.63 And there we go. We got what we wanted and summarised the mean age of our koalas for both sexes separately. And we did that using only one simple line of code! I think it is time for another challenge to test our skills! 2.2.5 Challenge 2 Calculate the average weight value per state and Sex. Which combination of state and sex has the heaviest and which combination had the lightest koalas? ## # A tibble: 8 x 3 ## # Groups: state [4] ## state sex mean_weight ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 New South Wales female 6.54 ## 2 New South Wales male 9.07 ## 3 Queensland female 5.68 ## 4 Queensland male 6.82 ## 5 South Australia female 7.59 ## 6 South Australia male 16.8 ## 7 Victoria female 7.58 ## 8 Victoria male 7.48 That is already quite powerful, but it gets even better! You’re not limited to defining only one new variable in summarise(): challenge2_ext&lt;-koala%&gt;%group_by(state, sex)%&gt;% summarise(mean_weight = mean(weight), sd_weight = sd(weight), sample_no = n()) We can create a new dataframe with as many new variables as we want. Very useful for our inital data exploration! Let’s get our hands another very useful function: mutate(). 2.2.6 mutate() We can create an entirely new variables in our initial dataset prior to (or even after) summarizing information using mutate(). Let’s say we’re interested in the weight:size ratio of our Koalas. Also we want to give each individual a numeric identifier to be able to better work with our data later on. koala_mutate&lt;-koala%&gt;%mutate(weight_size_ratio = size/weight, ID = row_number()) ## [1] &quot;species&quot; &quot;X&quot; &quot;Y&quot; ## [4] &quot;state&quot; &quot;region&quot; &quot;sex&quot; ## [7] &quot;weight&quot; &quot;size&quot; &quot;fur&quot; ## [10] &quot;tail&quot; &quot;age&quot; &quot;color&quot; ## [13] &quot;joey&quot; &quot;behav&quot; &quot;obs&quot; ## [16] &quot;weight_size_ratio&quot; &quot;ID&quot; Our dataset now has two extra columns containing the variables we were interested in. If you do not want to manipulate your raw data, you can use mutate before grouping and summarising to create the summary table straight away: koala_mutate_weight_size&lt;-koala%&gt;%mutate(weight_size_ratio = size/weight)%&gt;% group_by(sex)%&gt;% summarise(mean_weight = mean(weight), sd_weight = sd(weight), mean_weight_size = mean (weight_size_ratio), max_weight_size = max(weight_size_ratio)) ## # A tibble: 2 x 5 ## sex mean_weight sd_weight mean_weight_size max_weight_size ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 6.67 0.533 10.4 12.9 ## 2 male 9.31 2.38 8.21 11.9 Great! Let’s end the lesson with another challenge, combining all the functions we have looked at today. 2.2.7 Challenge 3 Calculate the average tail length and fur thickness for a group of 20 randomly selected males and females from New South Wales. Then arrange the mean tail length in descending order. Hint: Use the dplyr functions arrange() and sample_n(), they have similar syntax to other dplyr functions. Look at the help by calling ‘?function’, e.g. ?arrange. ## # A tibble: 2 x 3 ## sex mean_tail mean_fur ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 male 1.55 2.81 ## 2 female 1.43 2.48 Since we are sampling randomly, these will look different for each of you. 2.2.8 Solution to all challenges 2.2.8.1 Challenge 1 challenge1&lt;-koala%&gt;%filter(sex == &#39;female&#39;)%&gt;% select(age, size, color) challenge1.2&lt;-challenge1%&gt;%filter(size&gt;70) 2.2.8.2 Challenge 2 challenge2&lt;-koala%&gt;%group_by(state, sex)%&gt;% summarise(mean_weight = mean(weight)) 2.2.8.3 Challenge 3 challenge3&lt;-koala%&gt;%filter(state == &#39;New South Wales&#39;)%&gt;% group_by(sex)%&gt;% sample_n(20)%&gt;% summarise(mean_tail = mean(tail), mean_fur = mean(fur))%&gt;% arrange(desc(mean_tail)) "],
["working-with-spatial-data-in-r.html", "Chapter 3 Working with spatial data in R 3.1 Transforming a dataset into an sf object 3.2 Loading shapefiles into R, transforming and plotting 3.3 Simple geometric operations 3.4 Making a map using ggplot2 3.5 R raster basics 3.6 Plotting rasters in ggplot2 3.7 Solutions to Challenges", " Chapter 3 Working with spatial data in R Many of you will be familiar with using spatial data such as vector data in form of shapefiles or raster data in GIS software, which can handle these files intuitively. Nevertheless, GIS do have some disadvantages when it comes to either large data sets or automation processes. R offers a full integration of spatial tools and references such as GDAL or PROJ and some great packages to deal with this data seamlessly. Today we are going to learn how to use the package sf and have a brief detour towards raster data in R. You are going to need to install the following packages for this tutorial: install.packages(c(&#39;tidyverse&#39;, &#39;sf&#39;, &#39;raster&#39;, &#39;viridis&#39;)) And then call the packages: library(sf) library(raster) library(viridis) library(tidyverse) We will be working on a dataset of koala observations, that you can download here, as well as a basic administrative Areas shapefile of Australia, which you can find here. For the last part we will be looking at rasters, with a digital elevation model (DEM) file as example. You can find it for download here. Please make sure these are stored within a data/ folder in an R project (or working directory). 3.1 Transforming a dataset into an sf object Let’s first load in our dataset and have a look at it’s structure koala &lt;- read.csv(&#39;data/koala.csv&#39;) head(koala) # see the first few rows in the console ## species X Y state region sex ## 1 Phascolarctos cinereus 153.2155 -27.49284 Queensland northern male ## 2 Phascolarctos cinereus 148.1443 -22.47617 Queensland northern female ## 3 Phascolarctos cinereus 153.2285 -27.50298 Queensland northern male ## 4 Phascolarctos cinereus 152.6000 -27.50000 Queensland northern male ## 5 Phascolarctos cinereus 153.2817 -27.52589 Queensland northern female ## 6 Phascolarctos cinereus 152.8330 -27.20000 Queensland northern male ## weight size fur tail age color joey behav ## 1 7.119754 70.80159 1.858696 1.168241 8 grey No Sleeping ## 2 5.451345 70.38537 1.852801 1.562456 10 grey-brown Yes Sleeping ## 3 6.630577 68.65867 2.479280 1.056640 1 light grey No Just Chillin ## 4 6.470019 72.98919 1.923974 1.801244 1 grey No Sleeping ## 5 5.620447 65.19529 1.945341 1.625600 10 grey-brown No Sleeping ## 6 7.287674 70.56514 1.688897 1.086675 12 grey-brown No Feeding ## obs ## 1 Spotlighting ## 2 Opportunistic ## 3 Spotlighting ## 4 Stagwatching ## 5 Stagwatching ## 6 Opportunistic As we can see it contains lots of variables related to each of the observed koalas, such as sex, weight or in which state the observation was made. Additionally, whoever collected this data was so kind to also include the X and Y coordinates, where the observation was made. This we can use to transform this into an sf (simple feature) object. koala_sf &lt;- st_as_sf(koala, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 4326) str(koala_sf) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 242 obs. of 14 variables: ## $ species : Factor w/ 1 level &quot;Phascolarctos cinereus&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ state : Factor w/ 4 levels &quot;New South Wales&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ region : Factor w/ 2 levels &quot;northern&quot;,&quot;southern&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 2 2 1 1 ... ## $ weight : num 7.12 5.45 6.63 6.47 5.62 ... ## $ size : num 70.8 70.4 68.7 73 65.2 ... ## $ fur : num 1.86 1.85 2.48 1.92 1.95 ... ## $ tail : num 1.17 1.56 1.06 1.8 1.63 ... ## $ age : int 8 10 1 1 10 12 9 1 1 1 ... ## $ color : Factor w/ 6 levels &quot;chocolate brown&quot;,..: 3 4 6 3 4 4 6 4 3 3 ... ## $ joey : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 1 1 1 1 1 ... ## $ behav : Factor w/ 3 levels &quot;Feeding&quot;,&quot;Just Chillin&quot;,..: 3 3 2 3 3 1 2 3 1 3 ... ## $ obs : Factor w/ 3 levels &quot;Opportunistic&quot;,..: 2 1 2 3 3 1 3 2 2 2 ... ## $ geometry:sfc_POINT of length 242; first list element: &#39;XY&#39; num 153.2 -27.5 ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;names&quot;)= chr &quot;species&quot; &quot;state&quot; &quot;region&quot; &quot;sex&quot; ... The sf object is a data.frame with a geometry list-column. It supports different format and spatial references. The file as it exists in your R environment is what you would call the Attribute table in a GIS software. In R you can use and manipulate it as any other data.frame. When converting a table into a sf we need to indicate the columns containing coordinates using coords = c() and we can decide on a coordinate reference system using crs=. Here we decide to use WGS84 (CRS = 4326), a standard Mercator coordinate frame for the Earth given in Latitude and Longitude. We can plot our now spatial data with the standard base R command: plot(koala_sf) ## Warning: plotting the first 10 out of 13 attributes; use max.plot = 13 to ## plot all This will try to plot all columns (because they are now all spatially referenced). To only plot the geometries, you can: plot(koala_sf$geometry) So far so good, but we will need a bit more data to make a nice map! 3.2 Loading shapefiles into R, transforming and plotting You can load any vector file, such as .shp or .gpkg using st_read. Let’s get our Australia map into the environment and check it out: states &lt;- st_read(&quot;data/Australia/Australia_proj.shp&quot;) ## Reading layer `Australia_proj&#39; from data source `C:\\Users\\khh\\Documents\\GitHub\\bookdown-playaround\\data\\Australia\\Australia_proj.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 8 features and 15 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -2063975 ymin: -4965263 xmax: 1891143 ymax: -1285856 ## proj4string: +proj=lcc +lat_1=-18 +lat_2=-36 +lat_0=0 +lon_0=134 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs plot(states$geometry) Great, we even have states column, which will we will use a bit later. To plot, simply use base R syntax of plotting and then plotting another graph on top using add = T. Instead of calling plot(file$geometry), we can also call st_geometry() plot(st_geometry(states), axes = TRUE) plot(st_geometry(koala_sf), # why does this not work? col = &quot;blue&quot;, add = T) Hm, our states are plotting fine, but where are our koala locations? Maybe we should check the coordinate reference system to see if they match… Here’s how that’s done: st_crs(states) ## Coordinate Reference System: ## No user input ## wkt: ## PROJCS[&quot;GDA94_Geoscience_Australia_Lambert&quot;, ## GEOGCS[&quot;GCS_GDA_1994&quot;, ## DATUM[&quot;Geocentric_Datum_of_Australia_1994&quot;, ## SPHEROID[&quot;GRS_1980&quot;,6378137,298.257222101]], ## PRIMEM[&quot;Greenwich&quot;,0], ## UNIT[&quot;Degree&quot;,0.017453292519943295]], ## PROJECTION[&quot;Lambert_Conformal_Conic_2SP&quot;], ## PARAMETER[&quot;standard_parallel_1&quot;,-18], ## PARAMETER[&quot;standard_parallel_2&quot;,-36], ## PARAMETER[&quot;latitude_of_origin&quot;,0], ## PARAMETER[&quot;central_meridian&quot;,134], ## PARAMETER[&quot;false_easting&quot;,0], ## PARAMETER[&quot;false_northing&quot;,0], ## UNIT[&quot;Meter&quot;,1]] st_crs(koala_sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCS[&quot;WGS 84&quot;, ## DATUM[&quot;WGS_1984&quot;, ## SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563, ## AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]], ## PRIMEM[&quot;Greenwich&quot;,0, ## AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], ## UNIT[&quot;degree&quot;,0.0174532925199433, ## AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], ## AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]] Damn, they do not match. Our Australia shapefile has a different (weird) projection. If you feel it’s tedious to check the CRS in the console, simply type st_crs(states)==st_crs(koala_sf) ## [1] FALSE FALSE is returned, so they don’t match. We need to transform one of them to be able to plot our spatial points on top of the Australia map. sf has a simple solution for this using st_transform(). Let’s do that and plot straight away. koala_proj &lt;- st_transform(koala_sf, crs = st_crs(states)) plot(st_geometry(states), axes = TRUE) plot(st_geometry(koala_proj), col = &quot;blue&quot;, add = T) Excellent! In st_transform we can call any crs from another spatial file in the environment, our define our own using a proj4string. If we look at our plot, we can see that the coordinates on the axis look weird. We’d rather like latitude and longitude to make it look more comprehensive. That’s now up to you to fix it! 3.2.1 Challenge 1 Load the Australia_proj and koala data again and plot them on top of each other in Mercator projection (CRS: 4326). Note: check the spatial reference of the maps first! Extra challenge: color the koala points based on state! Check ?sf::plot for tips Looks good! You can find the solutions to all challenges posed here at the end of the document. Don’t peek! Let’s make our maps look even better. 3.3 Simple geometric operations sf supports any geometric operations you know from GIS software. We will just touch cropping here, but you can also merge, intersect, overlap and many more. For the purpose of mapping, st_crop() is most relevant. Let’s say we want to create a map of all our Koala locations, we won’t necessarily need to show all of Australia, knowing that Koalas are only distributed along the east coast. Let’s crop the Australia shapefile by the extent of our observations: australia_koala_area&lt;-st_crop(states_proj, koala_sf) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant ## throughout all geometries plot(st_geometry(australia_koala_area), axes = T) plot(st_geometry(koala_sf), add = T) Great. Let’s say we want to save the new cropped shapefile to an outputs/ folder in our project. For this we simply use st_write(): st_write(australia_koala_area, &#39;outputs/koala_shape.gpkg&#39;) You can choose different file extensions. Here we are saving a geopackage, as it results in only one file, rather than 4-6 for a .shp extension. Up to you ;) Let’s make a good looking map with our data! 3.4 Making a map using ggplot2 ggplot2 supports plotting spatial data by calling geom_sf(), after defining your data to plot. In simple terms we can create a map of Australia by calling: ggplot(data = states_proj) + geom_sf() which already looks a lot better than base R. But there are a lot of improvements to be made to create a publication-ready map. First we can assign colors based on columns in your data. We can also remove the standard grey ggplot background with a more map-friendly white grid using theme_minimal() ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal() We don’t really need a legend here, let’s remove it: ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal()+ guides(fill = F) Using coord_sf() we can even change the projection while plotting in ggplot2, without the need of going back to reprojecting or transforming in sf. ggplot(data = states_proj)+ geom_sf(aes(fill = STATENAME))+ theme_minimal()+ guides(fill = F)+ coord_sf(crs = 3112) Did you see what changed? Finally we can change the fore- and background color to make it look like proper map in a nice blue ocean: ggplot(data = states_proj)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ theme_minimal()+ coord_sf(crs = 3112)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;)) 3.4.1 Challenge 2 Add the koala positions on top of the Australia map in ggplot2. Only plot the area that has koalas and color the points by sex of the individual. Please also give the map in Mercator projection You will need to: Add a second layer to your ggplot Define the crs Good job! But unfortunately our cut-out looks like an island surrounded by seawater. If we want the map to look like a close-up of eastern Australia, we need to tell ggplot2 to start plotting at x/y = 0: ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;))+ scale_x_continuous(expand = c(0,0))+ scale_y_continuous(expand = c(0,0)) When working with these types of maps, you will often be asked to create an inset map that shows the entire area that you cropped from for reference. Let’s add a little Australia shape with a bounding box of our study area to our map. First we need to create the a base map with bounding box and save it to the environment: inset&lt;-ggplot(data = states_proj, show.legend = &quot;point&quot;)+ geom_sf()+ geom_rect(xmin = extent(australia_koala_area)[1], xmax = extent(australia_koala_area)[2], ymin = extent(australia_koala_area)[3], ymax = extent(australia_koala_area)[4], fill = NA, colour = &quot;black&quot;, size = 1.5)+ labs(x = &#39;&#39;, y = &#39;&#39;, title = &#39;&#39;)+ coord_sf(crs = 4326)+ theme_void() inset geom_rect can create any rectangular shape within your ggplot based on xmin, xmax and so on. Since these are dependent on your scales, for mapping we can use the extent of our cropped area from before. extent() is part of the raster package: extent(australia_koala_area) ## class : Extent ## xmin : 138.5795 ## xmax : 153.5651 ## ymin : -39 ## ymax : -21.3906 By using [1], [2] and so on, we call the respective coordinate from the four rows that this function is giving us. Using theme_void() is getting rid of any axis and grids, so that we have the map only. We also need to save our map as an object: map&lt;-ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;))+ scale_x_continuous(expand = c(0,0))+ scale_y_continuous(expand = c(0,0)) We use annotation_custom() and ggplotGrob() to place an inset into the main map: map+annotation_custom(ggplotGrob(inset), xmin = 139, xmax = 144, ymin = -26, ymax = -20) The size and position here is defined by xmin = etc. This needs a bit of fiddling around to get it right. Also be mindful that we are using a Mercator projection here and are in the southern hemisphere, so our ymin and ymax values need to be given in negative form. Now this is a proper map, ready to be put in your next publication! We will briefly talk about rasters, because you are likely to use them if you are working with shapefiles anyway. It is similarly easy to work with rasters in R: 3.5 R raster basics Raster support in R is made available using the package raster. One called, we can load and plot a raster using the same command. Let’s get our DEM to play with: dem&lt;-raster(&#39;data/DEM.tif&#39;) summary(dem) ## Warning in .local(object, ...): summary is an estimate based on a sample of 1e+05 cells (21.51% of all cells) ## DEM ## Min. 0.00000 ## 1st Qu. 89.43125 ## Median 156.95216 ## 3rd Qu. 399.96889 ## Max. 2139.29932 ## NA&#39;s 0.00000 plot(dem) In a raster, cell (or pixel) has a spatial reference and a value. In this case we have the elevation in meters for each degree (0.01, 0.01) in southeastern Australia. Let’s say we’re only interested in Victoria. We can use a spatial vector to mask out the state area from the raster. First we need to create a mask. We already know that our Australia shapefile contains a STATE column. We can use that to subset the shapefile and use the subset as a mask. Simple dplyr commands and %&gt;% work on sf features, which makes working with them so easy in R! vic&lt;-states_proj%&gt;%filter(STATENAME == &#39;Victoria&#39;)%&gt;%st_transform(crs = st_crs(dem)) # we also transform the crs to be sure they match plot(dem) plot(vic$geometry, add = T) To remove anything outside of Victoria, we use raster’s mask() and end up with the DEM for Victoria only: dem_vic&lt;-mask(dem, vic) plot(dem_vic) We can now also filter our koala dataset to only have koalas in Victoria to plot over our DEM. koala_vic&lt;-koala_sf%&gt;%filter(state == &#39;Victoria&#39;)%&gt;%st_transform(crs = st_crs(dem)) plot(dem_vic) plot(koala_vic$geometry, color = &#39;black&#39;, add = T) Often we are using rasters and spatial vector data together to extract data that is generally stored in raster form (such as elevation, mean temperatures or other topographic variables). For example if we are interested to see at which elevation our Koalas were observed in Victoria to make assumptions about their habitat preferences or temperature tolerances. If the CRS of both the raster and shapefile match, we can use extract() to add data from the raster to our spatial points as a new column: koala_vic$ele&lt;-raster::extract(dem_vic, koala_vic) summary(koala_vic$ele) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.62 67.95 185.34 218.54 349.87 648.06 hist(koala_vic$ele) We can see most observations were made below 200 meters. The median elevation for koalas in our dataset was 185. Again if we want to make nice and informative maps out of rasters, even in combination with spatial vector data, ggplot2 is the way to go! 3.6 Plotting rasters in ggplot2 Before we can plot a raster, we have to make it into a data.frame containing of columns X and Y as coordinates and a column containing the raster value of each pixel. Since rasters can be large and the resulting dataframes even larger, we first take a sample of the raster, which reduces the resolution: sample_raster&lt;-sampleRegular(dem_vic, size = 5e5, asRaster = TRUE) %&gt;% as.data.frame(xy = TRUE, na.rm = TRUE)%&gt;% setNames(c(&#39;x&#39;, &#39;y&#39;, &#39;layer&#39;)) Make sure to always use xy = TRUE, so that the coordinates are included in the new data.frame. We now have a format that ggplot can work with using geom_raster() ## x y layer ## 902 140.9675 -33.9925 19.73859 ## 1803 140.9675 -34.0025 48.89558 ## 1804 140.9775 -34.0025 25.70812 ## 2704 140.9675 -34.0125 53.28782 ## 2705 140.9775 -34.0125 27.96507 ## 2706 140.9875 -34.0125 20.13142 ggplot(data = sample_raster, aes ( x = x, y = y, fill = layer))+ geom_raster() Let’s make it look a bit prettier. The package viridis offers a few great color palettes for gradients of continuous variables: ggplot(data = sample_raster, aes ( x = x, y = y, fill = layer))+ geom_raster()+ scale_fill_viridis(option = &#39;A&#39;)+ labs(x = &#39;Longitude&#39;, y = &#39;Latitude&#39;, fill = &#39;Elevation&#39;)+ coord_sf(crs = 4326)+ theme_bw() We can plot additional layers on top of the raster by adding more geom_’s. That means we can plot rasters and spatial vectors together to make great maps for your papers and reports. I’m sure you can figure it out yourself for the next and final challenge! 3.6.1 Challenge 3 Add koala observations, color points by a categorical variable of your choice and change the color scheme. hint: to add different types of data, each data layer needs its own data source!: (geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+…) ggplot()+ geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+ geom_sf(data=koala_vic, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ scale_fill_viridis(option = &quot;D&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, fill = &quot;Elevation&quot;) + coord_sf(crs = 4326) + theme_bw() 3.7 Solutions to Challenges 3.7.1 Challenge 1 states_proj &lt;- st_transform(states, crs = st_crs(koala_sf)) plot(st_geometry(states_proj), axes = TRUE) plot(st_geometry(koala_sf), col = koala_sf$state, add = T) 3.7.2 Challenge 2 ggplot(data = australia_koala_area)+ geom_sf(size = 0.3, fill = &#39;white&#39;)+ geom_sf(data = koala_sf, size = 2, aes(color = sex))+ theme_minimal()+ coord_sf(crs = 4326)+ theme(panel.background = element_rect(fill = &#39;steelblue2&#39;)) 3.7.3 Challenge 3 ggplot()+ geom_raster(data = sample_raster, aes(x = x, y = y, fill = layer))+ geom_sf(data=koala_vic, size = 2, aes(color = sex), show.legend = &quot;point&quot;)+ scale_fill_viridis(option = &quot;D&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;, fill = &quot;Elevation&quot;) + coord_sf(crs = 4326) + theme_bw() "],
["calculating-class-area-from-raster.html", "Chapter 4 Calculating class area from raster 4.1 Simple area and area fraction calculations on a classified raster", " Chapter 4 Calculating class area from raster 4.1 Simple area and area fraction calculations on a classified raster For this tutorial you will need to load the following packages: library(raster) library(sf) library(tidyverse) You will often come across the task of extracting the area each of your raster classes occupies within your entire raster area. There is a simple way of doing this using raster and dplyr. The output will be a comprehensive table. First let’s create some dummy data to work with: 4.1.1 Creating dummy data x &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x)&lt;-100 # we are using a resolution of 100 x 100 so that each pixel is 1ha in size #populate the raster with values values(x)&lt;-base::sample(5, ncell(x), replace = T, prob = c(10,30,20,5,35)) plot(x) Our raster now contains 5 classes that could e.g. be land-use types such as forest, infrastructure or pasture. We additionally gave each class a probability of occurrence so that we can double check of our calculated areas are correct. In total, our raster x has 410^{4} cells. 4.1.2 Preparing the raster for area calculation To get area metrics, we need to transform the raster into a data frame: rast_df&lt;-x%&gt;%as.data.frame(xy = T, na.rm = T) To count each pixel, we can assign an extra column to this dataframe with an ID 1 to be able to tally all cells. Additionally, we are extracting the resolution of our raster as a variable to our environment to later calculate the area. rast_df$ID&lt;-1 reso&lt;-res(x)[1] head(rast_df) ## x y layer ID ## 1 -9950 9950 5 1 ## 2 -9850 9950 3 1 ## 3 -9750 9950 1 1 ## 4 -9650 9950 3 1 ## 5 -9550 9950 5 1 ## 6 -9450 9950 5 1 The layer column contains the class each pixel was assigned to. This will be universal for any raster you put in. ID is the same for each row, this is only needed in the next step. 4.1.3 Compiling a comprehensive table area&lt;-rast_df%&gt;%group_by(layer)%&gt;% summarise(pixelsum = sum(ID), area_ha = (pixelsum*reso^2)/10000)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;% rename(class = layer) area ## # A tibble: 5 x 5 ## class pixelsum area_ha sumA per ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4057 4057 40000 10.1 ## 2 2 12088 12088 40000 30.2 ## 3 3 7854 7854 40000 19.6 ## 4 4 2063 2063 40000 5.16 ## 5 5 13938 13938 40000 34.8 And there we go. Each class has it’s pixelsum calculated, then using the sum we can calculate the area in ha (or else, here you can alternate the code). In this case the pixel sum matches our area_ha because one pixel is already of size 1 ha. We can change the code to e.g. calculate area_km2. area_km2&lt;-rast_df%&gt;%group_by(layer)%&gt;% summarise(pixelsum = sum(ID), area_km2 = pixelsum/100)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;% rename(class = layer) area_km2 ## # A tibble: 5 x 5 ## class pixelsum area_km2 sumA per ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4057 40.6 40000 10.1 ## 2 2 12088 121. 40000 30.2 ## 3 3 7854 78.5 40000 19.6 ## 4 4 2063 20.6 40000 5.16 ## 5 5 13938 139. 40000 34.8 Next we calculate the sum of all pixels (sumA) using mutate() to get the total raster area. This should in this case be the same as ncell(x) (40000). To derive the percentage of the entire each class occupies, we just need to divide the pixelsum of each class by the total sum and multiply by 100. This should match our probabilities we assigned for each class when filling the raster with values: area_km2$per == summary(as.factor(values(x)))/400 ## 1 2 3 4 5 ## TRUE TRUE TRUE TRUE TRUE Enjoy trying it out on your own raster with some actual class ares! "],
["extracting-raster-fractions-and-area-using-a-polygon-mask.html", "Chapter 5 Extracting raster fractions and area using a polygon mask 5.1 Problem statement 5.2 Creating some dummy raster data 5.3 Create some reserves 5.4 Extracting data 5.5 Plotting the results", " Chapter 5 Extracting raster fractions and area using a polygon mask 5.1 Problem statement For this tutorial you will need to load the following packages. Please install if you don’t have them yet. library(raster) library(sf) library(exactextractr) library(tidyverse) Say you have a raster of habitat suitability for a certain species. You could now be interested, how much suitable habitat lies within certain defined areas. For example we could check weather the species is adequately protected by extracting how much suitable area lies within National Parks or other reserves Additionally we can test, how much suitable (and unsuitable) area lies outside the areas you want to test to have an idea of the potential for further protection, or the danger to the species from e.g. land clearing outside of parks. In the end we additionally want to know which fraction of the total area both suitable and unsuitable habitat occupies with our parks, reserves and outside ares. 5.2 Creating some dummy raster data First let’s get a raster to play around with: x &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x)&lt;-100 #make the resolution 100 x 100 meter, so one cell is 1 ha in size values(x)&lt;-runif(ncell(x)) #populate the raster with random values between 0 - 1 plot(x) This raster now contains makeshift values of habitat suitability on a percentage scale. Let’s say we found our cutoff point for suitable habitat at 0.5. Any values below this cutoff are then defined as unsuitable for our target species. We can reclassify our raster accordingly: x_re&lt;-reclassify(x, c(-Inf,0.5,0, 0.5,Inf,1)) plot(x_re) 0 now means non suitable and 1 suitable habitat. Since we randomized our values using runif() we have a equal distribution, meaning that 50% of the area is suitable and 50% of the area is unsuitable. This will help us in double-checking our results later but will probably never occur in real life examples. hist(x) barplot(x_re, col = c(&#39;darkred&#39;,&#39;darkgreen&#39;)) If we wanted to get different probabilities, we could also create our classified raster like this: x_re2 &lt;- raster(ncol=100, nrow=100, xmn=-10000, xmx=10000, ymn=-10000, ymx=10000) res(x_re2)&lt;-100 values(x_re2)&lt;-sample(0:1, ncell(x_re), replace = T, prob = c(80,20)) plot(x_re2) Using sample() with prob = c(80,20), assigns 80% of the area as unsuitable, but still on a normal pattern. 5.3 Create some reserves We also needs some reserve areas to extract values from. extent(x_re) #have a look at the extent of our raster to decide for extent of dummy areas ## class : Extent ## xmin : -10000 ## xmax : 10000 ## ymin : -10000 ## ymax : 10000 #create some extents a&lt;-extent(c(-1000, 1000, -5233, -2355)) b&lt;-extent(c(-7000, 6530, -400, 4223)) c&lt;-extent(c(-10000, -5427, -10000, -4785)) #transform them into polygons a_sf&lt;-as(a, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res1&#39;) b_sf&lt;-as(b, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res2&#39;) c_sf&lt;-as(c, &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;res3&#39;) #get the total area to create our outside areas all&lt;-as(extent(x_re), &#39;SpatialPolygons&#39;)%&gt;%st_as_sf()%&gt;%mutate(name = &#39;outside&#39;) plot(x_re) plot(a_sf$geometry, add = T, lwd = 2, border = &#39;red&#39;) plot(b_sf$geometry, add = T, lwd = 2, border = &#39;blue&#39;) plot(c_sf$geometry, add = T, lwd = 2, border = &#39;orange&#39;) Now we have three areas that we assume to be our reserves. We still need a polygon for all other areas, also we need to combine our polygons into one. #bind reserves reserves&lt;-rbind(a_sf, b_sf, c_sf) #get the outside areas only outside&lt;-st_difference(all, st_combine(reserves)) #bind all for extraction all_areas&lt;-rbind(reserves, outside) plot(x_re) plot(st_geometry(outside$geometry), border = &#39;blue&#39;, lwd = 4, add = T) plot(st_geometry(reserves), border = as.factor(reserves$name), lwd = 3, add =T) The blue area is all raster area outside of our reserves. Our three reserves are colored in red, black and green. 5.4 Extracting data To extract the cell data from each polygon, we use exact_extract from the package exactextractr, which is a quicker alternative to raster::extract. It extracts data from each feature separately in parallel and saves the extracted values into a list. extract&lt;-exact_extract(x_re,all_areas, fun = NULL) ## | | | 0% | |================ | 25% | |================================ | 50% | |================================================= | 75% | |=================================================================| 100% for( i in seq_along(extract)){ extract[[i]]$ID &lt;- seq_along(extract)[i] } In the second step, we assign an ID column to each table in the created list. This will help us in matching the extracted values to the name of the area polygon we extracted from. We convert out list into a table and then add the lc column to assign each value the name of the polygon it belongs to: extract_table&lt;-dplyr::bind_rows(extract)%&gt;%dplyr::select(-2) extract_table$lc &lt;- as.factor(all_areas$name[extract_table$ID]) head(extract_table) ## value ID lc ## 1 0 1 res1 ## 2 0 1 res1 ## 3 1 1 res1 ## 4 0 1 res1 ## 5 1 1 res1 ## 6 1 1 res1 We can now compile a summary table using dplyr syntax. To calculate the area, we are working with the resolution of the raster and will need to save it as a variable fist. reso&lt;-res(x_re)[1] area_habitat&lt;-extract_table%&gt;%group_by(lc, value)%&gt;% summarise(pixelsum = sum(ID), areaha = (pixelsum*reso^2)/10000)%&gt;% mutate(sumA = sum(pixelsum), per = 100*pixelsum/sumA)%&gt;%ungroup()%&gt;% mutate(sum_all = sum(pixelsum), pertotal = 100*pixelsum/sum_all) area_habitat ## # A tibble: 8 x 8 ## lc value pixelsum areaha sumA per sum_all pertotal ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 outside 0 61684 61684 123560 49.9 144258 42.8 ## 2 outside 1 61876 61876 123560 50.1 144258 42.9 ## 3 res1 0 315 315 600 52.5 144258 0.218 ## 4 res1 1 285 285 600 47.5 144258 0.198 ## 5 res2 0 6388 6388 12784 50.0 144258 4.43 ## 6 res2 1 6396 6396 12784 50.0 144258 4.43 ## 7 res3 0 3759 3759 7314 51.4 144258 2.61 ## 8 res3 1 3555 3555 7314 48.6 144258 2.46 What happens here is, we group by class (lc which refers to our polygon names) as well as value (0 for unsuitable and 1 for suitable habitat). We then tally all pixels using the ID column and calculate the area in ha from the pixelsum. In our case these match because the resolution is already 1 ha (each cell is 100x100 meters). Furthermore we can then tally all pixels to get the total raster area and from that calculate first the percentage of suitable and unsuitable habitat in each polygon and then (by using ungroup()) the fraction of this area compared to the total raster area. 5.5 Plotting the results Using ggplot2 we can then plot these results to better visualize them. The code below will work for any table that is in the format of our area_habitat table above, if you follow the steps of this guide. bar&lt;-ggplot(data = area_habitat, aes(x = reorder(as.factor(lc), -pixelsum), y = per, fill = as.factor(value)))+ geom_bar(stat = &#39;identity&#39;, color = &#39;black&#39;, position=position_fill(), show.legend = FALSE)+ scale_y_continuous(labels = scales::percent)+ geom_text(aes(label=round(per, digits = 1)), check_overlap = TRUE, position=position_fill(), vjust = 1.2, color = &#39;black&#39;)+ theme_classic()+ theme(axis.title.y = element_text(size = 15, face = &#39;bold&#39;), axis.title.x = element_text(size = 15, face = &#39;bold&#39;), axis.text.x = element_text(size = 15, face = &#39;bold&#39;), axis.text.y = element_text(size = 15, face = &#39;bold&#39;))+ ggtitle(&#39;Whatever this area is&#39;)+ xlab(&#39;&#39;)+ ylab(&#39;area percentage&#39;)+ scale_x_discrete(labels=c(&#39;Outside&#39;, &#39;Reserve 2&#39;, &#39;Reserve 3&#39;, &#39;Reserve 1&#39;))+ scale_fill_manual(values = c(&#39;lightgrey&#39;, &#39;darkgreen&#39;), labels = c(&#39;unsuitable&#39;, &#39;suitable&#39;), name = &#39;&#39;) bar Our plot shows for each area we wanted to test, which fraction is suitable and which unsuitable. As expected, in our example we have about a 50:50 distribution, but this may look quite different in a real-world example. Hope you can find a useful application for this code! :) "],
["functions-loops-and-apply.html", "Chapter 6 Functions, loops, and apply 6.1 Introduction 6.2 Writing basic functions 6.3 Using apply functions 6.4 Loops", " Chapter 6 Functions, loops, and apply 6.1 Introduction Most data analysis in R requires executing a particular task on multiple data observations, groups of observations, or separate data sets. Many novice users begin with the strategy of assigning a new variable for each input: x1 &lt;- dataset1 x2 &lt;- dataset2 meanx1 &lt;- mean(x1) meanx2 &lt;- mean(x2) While this certainly works, it can be very time consuming and introduces substantial risk of user error. There are several alternative strategies that can be used to repeat mathematical operations or sequences of R functions for multiple variables or data sets that avoid this risk. The foundation for any of these strategies is a function to be repeated. Once you have a function that you would like to repeat on multiple inputs, there are a few approaches for iterating through each repetition. In this tutorial, we will focus on the apply functions, which are a set of base R functions that can be used to execute a particular chunk of code on multiple inputs and generate appropriate outputs. The apply functions are simple to use and create very neat code, but one limitation is that they can only be used in operations for which the order of iteration is not important. We will also introduce for and while loops, which can be used when the order of operations does matter. 6.2 Writing basic functions Functions in R take an input value, apply a process, and generate an output. R packages include numerous functions that can do nearly anything you desire, but research often requires specific processes or sequences of commands designed specifically to work with your data structure. You can write your own functions in R to perform these tasks. Here is a very simple function: fun1 &lt;- function (x, y){ x + y } Here, x and y are identified as the required input variables. The code written within the brackets describes the mathematical operation to be applied to the input variables. fun1(x = 7, y = 12) ## [1] 19 A function can be applied to a vector of inputs. In this scenario, the function will be applied to each pair of values in the vector sequence: vals_x &lt;- c(4, 7, 1) vals_y &lt;- c(2, 1, 3) fun1(x = vals_x, y = vals_y) ## [1] 6 8 4 6.2.0.1 Nesting functions Functions can also include other functions, which is particularly useful in the context of repeating complex data analyses. Here is a basic example: fun2 &lt;- function (x, y){ mean(x) + y } fun2(x = vals_x, y = vals_y) ## [1] 6 5 7 It is important to note that in this situation, the internal function (mean()) takes a vector of input values. This function will calculate the mean of the vals_x vector and then add this to each separate value of vals_y. This can be very useful, but it’s critical that you know exactly how your function processes input values or you may run into major problems! 6.2.0.2 Assigning variables within functions and return() In some more complex analyses, the nested functions may generate data that needs to be stored as variables so they can be used as input for the next process. Here is a function that calculates the mean of our x values, adds y, and then generates 10 numbers from a normal distribution with a mean of the value calculated in the first step. fun3 &lt;- function (x, y){ val &lt;- mean(x) + y dist &lt;- rnorm(1:10, mean = val, sd = 1) } output &lt;- fun3(x = vals_x, y = 3) output ## [1] 7.006171 8.065973 6.424325 7.342045 6.372207 5.439520 6.166673 ## [8] 7.498010 7.504085 7.408942 If your function assigns multiple variables, you can use return() to determine the data that is output from the function. This can be useful if your function includes writing a data set, or includes another process that is not intended to return anything to R as well as some process intended to return data to the R environment. #Let&#39;s see what happens without using the return() function fun4 &lt;- function (x, y){ val &lt;- mean(x) + y dist &lt;- rnorm(1:10, mean = x, sd = 1) write.csv(dist, &quot;filename.csv&quot;) } output &lt;- fun4(x = vals_x, y = 3) output ## NULL Our output here is NULL because the default output for a function will come from the last command within the function. Here, write.csv() does not produce data to be returned to R. If we add return(), our function will write the .csv and also return the data we want into the environment: #Now let&#39;s add return() fun4 &lt;- function (x, y){ val &lt;- mean(x) + y dist &lt;- rnorm(1:10, mean = x, sd = 1) write.csv(dist, &quot;filename.csv&quot;) return(dist) } output &lt;- fun4(x = vals_x, y = 3) output ## [1] 4.880992 8.928873 2.073899 2.047614 6.753400 1.719961 3.929610 ## [8] 6.441054 1.992635 3.842700 Much better! 6.3 Using apply functions Once you have written a function, you will likely need to execute that function on multiple data sets or variables. This is where the apply functions come in handy. The apply functions are a set of functions that can be used to, well, apply functions to lists, dataframes, and matrices. apply functions are similar to loops, but have a much simpler syntax. There are several different functions that can be used depending upon the inputs and desired outputs. We will work through the following apply functions: apply - used with arrays, including matrices, and returns a vector, array, or list. sapply - (simplified apply) applies the function to each element of a vector, list, or matrix, and returns the outputs as a vector or matrix. lapply - (list apply) is similar to sapply, but returns a list of objects, rather than a vector. mapply - (multivariate apply) applies a function to the first elements of two or more vectors or lists, and then the second pair of elements, etc.. If this doesn’t make much sense, don’t stress! We will walk though an example of each function and situations for which they might be useful. 6.3.1 apply Let’s generate a theoretical transition matrix for a “before” and “after” data set that shows the number of transitions from one type to another. before &lt;- rpois(1000, lambda = 1) after &lt;- rpois(1000, lambda = 1) tab &lt;- table(before, after) tab ## after ## before 0 1 2 3 4 5 ## 0 166 123 60 19 6 0 ## 1 143 115 73 22 10 2 ## 2 59 73 38 10 1 0 ## 3 22 24 10 4 0 1 ## 4 4 6 6 1 0 0 ## 5 0 0 0 1 0 0 ## 8 0 1 0 0 0 0 We would like to convert this table to the relative probability for each transition- that is, what is the probability that an object of type “0” in the “before” dataset transitions to a 1? To a 2? 3? To calculate the probability, we will need to get the sum of observations in each row, and then divide each transition in that particular row by the row sum. We can do this using the apply function. apply has three arguments: X is the array to which the function will be applied. MARGIN is used to identify whether the function should be applied to the rows, columns, or both. ‘1’ is for rows, ‘2’ is for columns, and c(1,2) can be used for both. FUN is the function to be applied. In this scenario, we set MARGIN = 1 so it will perform the function row wise. Up to this point, we have defined our functions outside of other commands, but in this scenario, it is a very simple function and so we can define the function within the command itself. *Note: The t() function surrounding the command transposes the data set so it returns the matrix in the original orientation (before as rows and after as columns) rel_prob &lt;- t(apply(tab, MARGIN = 1, FUN = function (i) {i/sum(i)} )) rel_prob ## after ## before 0 1 2 3 4 5 ## 0 0.4438503 0.3288770 0.1604278 0.05080214 0.016042781 0.000000000 ## 1 0.3917808 0.3150685 0.2000000 0.06027397 0.027397260 0.005479452 ## 2 0.3259669 0.4033149 0.2099448 0.05524862 0.005524862 0.000000000 ## 3 0.3606557 0.3934426 0.1639344 0.06557377 0.000000000 0.016393443 ## 4 0.2352941 0.3529412 0.3529412 0.05882353 0.000000000 0.000000000 ## 5 0.0000000 0.0000000 0.0000000 1.00000000 0.000000000 0.000000000 ## 8 0.0000000 1.0000000 0.0000000 0.00000000 0.000000000 0.000000000 Hurray! Please not that there are other ways to perform the same task, but this one is a conceptually a simple example :) 6.3.2 sapply Let’s check out the Loblolly dataset. It contains the height, age, and seed source of a sample of Loblolly pines. library(datasets) data(Loblolly) head(Loblolly) ## height age Seed ## 1 4.51 3 301 ## 15 10.89 5 301 ## 29 28.72 10 301 ## 43 41.74 15 301 ## 57 52.70 20 301 ## 71 60.92 25 301 The data set includes a range of ages. Let’s see if we can write a function that calculates the mean height for each age group. #Create a list with each unique age group ages &lt;- unique(Loblolly$age) ages ## [1] 3 5 10 15 20 25 #Create a function that calculates the mean height for each age group fun5 &lt;- function (x){ agegroup &lt;- subset(Loblolly, age == x) mean_height &lt;- mean(agegroup$height) return(mean_height) } fun5(ages) ## [1] 32.3644 Hmmm, this returns one value rather than a separate mean for each group. Why is that? In this function, x is a vector, and so the subset function selects the cases where age is equal to any of the values included in the ages vector. Because we have selected every value of age, we will get the mean height for the entire dataset. If we would like a vector of the means for each height, we will can use sapply. #Create a function that calculates the mean height for each age group mean_heights &lt;- sapply(X = ages, FUN = fun5) mean_heights ## [1] 4.237857 10.205000 27.442143 40.543571 51.468571 60.289286 Excellent! We now have a vector with the mean height for each age group! 6.3.3 lapply You can use the lapply function to get the same information, but in a list format. heights &lt;- lapply(X = ages, FUN = fun5) heights ## [[1]] ## [1] 4.237857 ## ## [[2]] ## [1] 10.205 ## ## [[3]] ## [1] 27.44214 ## ## [[4]] ## [1] 40.54357 ## ## [[5]] ## [1] 51.46857 ## ## [[6]] ## [1] 60.28929 In this scenario, sapply might be a better fit. lapply is generally more useful if your output is a an object like a data.frame or produces objects of different types. Let’s write a function that creates a subsetted data.frame for each value of tree age. ages &lt;- unique(Loblolly$age) func6 &lt;- function (x){ subset(Loblolly, age == x) } sub_df &lt;- lapply(ages, func6) #Let&#39;s look at items 1 and 2 in the list sub_df[1:2] ## [[1]] ## height age Seed ## 1 4.51 3 301 ## 2 4.55 3 303 ## 3 4.79 3 305 ## 4 3.91 3 307 ## 5 4.81 3 309 ## 6 3.88 3 311 ## 7 4.32 3 315 ## 8 4.57 3 319 ## 9 3.77 3 321 ## 10 4.33 3 323 ## 11 4.38 3 325 ## 12 4.12 3 327 ## 13 3.93 3 329 ## 14 3.46 3 331 ## ## [[2]] ## height age Seed ## 15 10.89 5 301 ## 16 10.92 5 303 ## 17 11.37 5 305 ## 18 9.48 5 307 ## 19 11.20 5 309 ## 20 9.40 5 311 ## 21 10.43 5 315 ## 22 10.57 5 319 ## 23 9.03 5 321 ## 24 10.79 5 323 ## 25 10.48 5 325 ## 26 9.92 5 327 ## 27 9.34 5 329 ## 28 9.05 5 331 6.3.4 mapply mapply is an extremely useful function that is a great way to avoid using for loops! With mapply, you can use multiple input variables and it will iterate through each pair (or trio or more) of variables as inputs for the function. Let’s say we would like to generate a theoretical normal distribution of tree height for each age group based on the mean and standard deviation of height. We will need a vector of means and a vector of heights to use as input values. We have already genereated a vector of means in our sapply example, so we will use a similar equation to generate a vector of sd. fun6 &lt;- function (x){ agegroup &lt;- subset(Loblolly, age == x) sd(agegroup$height) } sd_height &lt;- sapply(ages, fun6) sd_height ## [1] 0.4036026 0.8155767 1.5378866 1.9508444 2.2118278 2.2688339 Now, we can write a function that takes the two variables and applied it to each pair of input values to generate 6 six distributions. fun7 &lt;- function (x, y){ rnorm(100, mean = x, sd = y) } height_dists &lt;- mapply(FUN = fun7, x = mean_heights, y = sd_height) colnames(height_dists) &lt;- ages #This names the columns by the tree age summary(height_dists) ## 3 5 10 15 ## Min. :3.157 Min. : 7.647 Min. :22.45 Min. :37.34 ## 1st Qu.:3.999 1st Qu.: 9.685 1st Qu.:26.67 1st Qu.:38.99 ## Median :4.252 Median :10.199 Median :27.41 Median :40.23 ## Mean :4.264 Mean :10.178 Mean :27.46 Mean :40.34 ## 3rd Qu.:4.524 3rd Qu.:10.680 3rd Qu.:28.60 3rd Qu.:41.51 ## Max. :5.300 Max. :11.991 Max. :30.81 Max. :44.76 ## 20 25 ## Min. :46.62 Min. :54.92 ## 1st Qu.:50.30 1st Qu.:58.81 ## Median :51.66 Median :60.18 ## Mean :51.86 Mean :60.22 ## 3rd Qu.:53.42 3rd Qu.:61.37 ## Max. :57.42 Max. :65.47 6.4 Loops I haven’t done this part yet! "]
]
