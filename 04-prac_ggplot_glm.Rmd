# Introduction to data visualization in `ggplot2`

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F)
```


```{r setup, include=FALSE}
library(tidyverse)
library(performance)
library(corrplot)
library(emojifont)
koala<-read.csv(file="data/koala.csv")
```

Structure used by **ggplot** is basic.Identify data, specify a **mapping**, and then choose an appropriate **geometry** to display data.
+ is the key to constructing sophisticated ggplot2 graphics. It allows you to start simple, then get more and more complex, checking your work at each step.

aesthetic = variable describing which variables in the layer data should be mapped to which aesthetics used by the paired geom/stat. The expression variable is evaluated within the layer data, so there is no need to refer to the original dataset (i.e., use ggplot(df,aes(variable)) instead of ggplot(df,aes(df$variable))). The names for x and y aesthetics are typically omitted because they are so common; all other aesthetics must be
named.



```{r cars}
ggplot(data = koala)
p <- ggplot(data = koala, mapping = aes(x = weight, y = size))

```

To see the individual points, specify the geometry that you would like to use. For **X,Y** data, X, Y, ... List of name value pairs. Elements must be either quoted calls, strings, onesided formulas or constants.
we can use **geom_point()**.

```{r}
p + geom_point()
p + geom_smooth()
```
Add other features of the geom. Try playing around with shape=, size=, alpha=. 
*For shape*, use integer values from 0 to 20 (although there are many others to choose from). 
*For size*, use positive non-zero values (non-integers are OK). 
*For alpha*, use values from 0 to 1. You can use more than one of these at a time. 
Just separate them with commas in the geom statement.
We can also add nice (or more detailed) labelling. To do this we just need to add the labs component to the overall statement (like adding the geom_point() or geom_smooth). 

```{r pressure, echo=FALSE}
p + geom_point(colour="darkblue", size=0.75, shape=5) + geom_smooth(method='lm')
p + geom_point() + geom_smooth() + scale_x_log10() + scale_y_log10()

p + geom_point(colour = 'darkblue', size = 0.75, shape = 3) +
  labs(x = "Weight", y = "Size", 
       title = "Weight and size of drop BEAR ")
```

The boxplot compactly displays the distribution of a continuous variable. 
and ```coord_flip``` flips the x axis to y and reverse

```{r}
r<-ggplot(data=koala, mapping=aes(x=state, y=tail))
r+geom_boxplot()
r+geom_boxplot()+coord_flip()
```

```Reorder``` is a generic function. The "default" method treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric.

```{r}
r<-ggplot(data=koala, aes(x=reorder(state, tail), y=tail))
r+geom_boxplot()
```

```stat_summary``` operates on unique x; ```stat_summary_bin``` operates on binned x. They are more flexible versions of ```stat_bin()```: instead of just counting, they can compute any aggregate.


```{r}
ggplot(koala, aes(x=sex,y=tail))+geom_boxplot()+
  stat_summary(fun.y = mean,
               geom="point",
               size=3,color="blue")
```

Reverse the condition logic
Its actually very simple with R and dplyr. 
Its **!(exclamation mark)**. And, it goes like this.

```{r}
koala%>% 
  filter(!state=="New South Wales")%>%
ggplot(aes(state,weight))+
  geom_boxplot(aes(x=state, y=weight)) + 
  stat_summary(fun.y = mean,
               geom="point",
               size=3,color="blue")

```

# Introduction to linear models in R

linear regression model is represented as:


$$Y  = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon $$


Each of the model assumptions concerns the error term of the regression model. These are:

1. Individual observations are independent
2. Response data are normally distributed
3. Variance is homogeneous across range of predictor
4. Data are linear


```{r}
qplot(koala$weight)
qplot(log(koala$weight))
qplot(koala$fur)
linearmodel<-lm(weight~fur, data=koala)
summary(linearmodel)
anova(linearmodel)
check_model(linearmodel, check=c("qq", "normality", "ncv", "outliers")) 
```

```{r}
lm<-lm(size~fur, data=koala)
summary(lm)
anova(lm)
check_model(lm, check=c("qq", "normality", "ncv", "outliers"))
```
**Correlation** is a statistical technique that can show whether and how strongly pairs of variables are related.The main result of a correlation is called the correlation coefficient (or "r"). It ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related.


```{r}

corkoala<-koala %>% 
  select_if(is.numeric)%>%
  select(-c(1,2))

corMat<-cor(corkoala, use="complete.obs", method = "pearson")

corrplot(corMat, 
         method="shade",
         type="lower",
         diag = FALSE,
         addCoef.col = "black")
```
Determine if two or more samples are from the same population
-H0: 
  -Sample means are all equal(i.e.,  ??A= ) 
  -There is no effect of the factor on the response variablel
-If reject H0
  -Suggests that at least one sample mean is different from the others
-If don't reject H0
  -No evidence that any of the sample means are different from the overall population mean
  
```{r}
anova<-aov(weight~age, koala)
summary(anova)

anova1<-aov(tail~age, koala)
summary(anova1)

anova2<-aov(tail~age+sex, koala)
summary(anova)

anova(anova, anova1, anova2, test="chi")
```

Making a new variable to start with presence absence data

```ifelse``` returns a value with the same shape as test which is filled with elements selected from either yes or no depending on whether the element of test is TRUE or FALSE.

```mutate``` Mutate adds new variables and preserves existing; transmute drops existing variables.

```{r}
koala<-koala %>%
  mutate(presabs=ifelse(joey == "Yes","1","0"))

summary(as.factor(koala$presabs))
```

A series of test/training partitions are created using ```createDataPartition``` 
Any supervised machine learning task require to split the data between a train set and a test set. You can create in the other supervised learning tutorials to create a train/test set.

```{r}
library(caret)
set.seed(1234)
train <- createDataPartition(koala$presabs, 
                             time=1,#the number of partitions to create 
                             p = 0.7,#he percentage of data that goes to training
                             list=F)
data_train <- koala[train,]
data_test<- koala[-train,]
```

**Build the model **


```{r}
formula <- as.numeric(presabs)~fur+size+weight
pre <- glm(formula, data = data_train, family = 'binomial')
summary(pre)
```

**Assess the performance of the model**
**Confusion Matrix**
The confusion matrix is a better choice to evaluate the classification performance compared with the different metrics you saw before. The general idea is to count the number of times True instances are classified are False.
To compute the confusion matrix, you first need to have a set of predictions so that they can be compared to the actual targets.
```{r}
predict <- predict(pre, data_test, type = 'response')
# confusion matrix
table_mat <- table(data_test$presabs, predict > 0.5)
table_mat

```
Each row in a confusion matrix represents an actual target, while each column represents a predicted target.
You can calculate the model accuracy by summing the true positive + true negative over the total observation

```{r}
accuracy_Test <- sum(diag(table_mat))/sum(table_mat)
accuracy_Test
```
`r emoji("smile_cat")`

